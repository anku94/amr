From f02a8de96fc3d248576243b20f5bf63591743c4e Mon Sep 17 00:00:00 2001
From: Ankush Jain <anku.94@gmail.com>
Date: Thu, 4 Apr 2024 16:33:07 -0400
Subject: [PATCH] bunch of plotting code

---
 scripts/tau_analysis/20230825_athpk.py        |  144 +++
 scripts/tau_analysis/20230831_athmsgs.py      |  261 +++++
 scripts/tau_analysis/20230911_perfprobes.py   |  150 +++
 scripts/tau_analysis/20230919_getslot.py      |  255 +++++
 .../tau_analysis/20231016_analyze_pprof.py    |  722 ++++++++++++
 .../tau_analysis/20231023_pprof_blastwave.py  |  190 +++
 .../tau_analysis/20231101_analyze_pprof.py    |  667 +++++++++++
 scripts/tau_analysis/20231220_blkmat_plots.py |   93 ++
 scripts/tau_analysis/20240109_analyze_hdf5.py |   70 ++
 scripts/tau_analysis/20240109_plot_time.py    |   51 +
 .../tau_analysis/20240126_analyze_pprof.py    |  741 ++++++++++++
 scripts/tau_analysis/20240202_analyze_cdpp.py |   38 +
 .../tau_analysis/20240312_parse_preload.py    |  233 ++++
 .../tau_analysis/20240326_blastwave_hist.py   |  281 +++++
 .../tau_analysis/analyze_dynamic_blocks.py    |  324 ++++++
 scripts/tau_analysis/analyze_msgs.py          |  476 +++++++-
 scripts/tau_analysis/analyze_pprof.py         |  964 ++++++++++++++++
 .../tau_analysis/analyze_pprof_comm_matrix.py |  257 +++++
 scripts/tau_analysis/analyze_prof.py          |  927 +++++++++++++--
 scripts/tau_analysis/analyze_prof_hist.py     |   44 +
 scripts/tau_analysis/analyze_recurrence.py    |  161 +++
 scripts/tau_analysis/analyze_taskflow.py      |    7 +-
 scripts/tau_analysis/analyze_taskflow_2.py    |  117 ++
 scripts/tau_analysis/common.py                |  158 +++
 scripts/tau_analysis/plot_analyses_v2.py      |  109 ++
 scripts/tau_analysis/plot_policy_eval.py      |  851 ++++++++++++++
 scripts/tau_analysis/plot_prof.py             |   93 ++
 scripts/tau_analysis/plot_prof_bid.py         |  143 +++
 scripts/tau_analysis/plot_taskflow.py         |   95 +-
 .../tau_analysis/plot_taskflow_printable.py   | 1020 +++++++++++++++++
 30 files changed, 9483 insertions(+), 159 deletions(-)
 create mode 100644 scripts/tau_analysis/20230825_athpk.py
 create mode 100644 scripts/tau_analysis/20230831_athmsgs.py
 create mode 100644 scripts/tau_analysis/20230911_perfprobes.py
 create mode 100644 scripts/tau_analysis/20230919_getslot.py
 create mode 100644 scripts/tau_analysis/20231016_analyze_pprof.py
 create mode 100644 scripts/tau_analysis/20231023_pprof_blastwave.py
 create mode 100644 scripts/tau_analysis/20231101_analyze_pprof.py
 create mode 100644 scripts/tau_analysis/20231220_blkmat_plots.py
 create mode 100644 scripts/tau_analysis/20240109_analyze_hdf5.py
 create mode 100644 scripts/tau_analysis/20240109_plot_time.py
 create mode 100644 scripts/tau_analysis/20240126_analyze_pprof.py
 create mode 100644 scripts/tau_analysis/20240202_analyze_cdpp.py
 create mode 100644 scripts/tau_analysis/20240312_parse_preload.py
 create mode 100644 scripts/tau_analysis/20240326_blastwave_hist.py
 create mode 100644 scripts/tau_analysis/analyze_dynamic_blocks.py
 create mode 100644 scripts/tau_analysis/analyze_pprof.py
 create mode 100644 scripts/tau_analysis/analyze_pprof_comm_matrix.py
 create mode 100644 scripts/tau_analysis/analyze_prof_hist.py
 create mode 100644 scripts/tau_analysis/analyze_recurrence.py
 create mode 100644 scripts/tau_analysis/analyze_taskflow_2.py
 create mode 100644 scripts/tau_analysis/common.py
 create mode 100644 scripts/tau_analysis/plot_analyses_v2.py
 create mode 100644 scripts/tau_analysis/plot_policy_eval.py
 create mode 100644 scripts/tau_analysis/plot_prof.py
 create mode 100644 scripts/tau_analysis/plot_prof_bid.py
 create mode 100644 scripts/tau_analysis/plot_taskflow_printable.py

diff --git a/scripts/tau_analysis/20230825_athpk.py b/scripts/tau_analysis/20230825_athpk.py
new file mode 100644
index 0000000..03664a6
--- /dev/null
+++ b/scripts/tau_analysis/20230825_athpk.py
@@ -0,0 +1,144 @@
+import matplotlib.pyplot as plt
+import matplotlib.colors as colors
+import matplotlib as mpl
+
+from matplotlib.ticker import FuncFormatter, MultipleLocator, LogLocator
+import multiprocessing
+import numpy as np
+import pandas as pd
+import glob
+import re
+
+from common import plot_init_big, PlotSaver, label_map, get_label
+from analyze_msgs import aggr_msgs_some, join_msgs
+
+"""
+Analyze messages from athenapk5?
+"""
+
+
+def plot_msg_timestamps(trace_name: str):
+    chan_df, send_df = aggr_msgs_some(trace_name, 0, 15)
+    print(send_df)
+    msg_df = join_msgs(chan_df, send_df)
+    # print(msg_df)
+
+    dy = send_df["timestamp"].to_numpy()
+    dy = dy - min(dy)
+    dx = range(len(dy))
+
+    for ts in range(2, 10):
+        ts_df = send_df[send_df["ts"] == ts]
+        dy = ts_df["timestamp"].to_numpy()
+        dy = dy - min(dy)
+
+        hist, bins = np.histogram(dy)
+        fig, ax = plt.subplots(1, 1, figsize=(10, 8))
+        ax.clear()
+        ax.stairs(hist, edges=bins, zorder=2)
+        ax.set_title(f"TS: {ts}")
+        ax.grid(visible=True, which="major", axis="both", color="#bbb")
+        ax.grid(visible=True, which="minor", axis="both", color="#ddd")
+        ax.xaxis.set_major_formatter(
+            FuncFormatter(lambda x, pos: "{:.0f} ms".format(x / 1e3))
+        )
+        ax.xaxis.set_minor_locator(MultipleLocator(20 * 1e3))
+        ax.xaxis.set_major_locator(MultipleLocator(100 * 1e3))
+        fig.tight_layout()
+        fig.show()
+    pass
+
+
+def get_times_trace(trace_name: str) -> np.ndarray:
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_name)
+    log_fpath = f"{trace_dir}/run/log.txt"
+    log_data = open(log_fpath, "r").readlines()
+    log_data = [l for l in log_data if "wsec" in l]
+
+    times = []
+
+    for l in log_data:
+        mobj = re.search(r"\ wsec_step=([0-9\.e\+\-]+)", l)
+        n = float(mobj.group(1))
+        times.append(n)
+
+    return np.array(times)
+
+
+def compare_perf_traces(traces, names):
+    times = [ get_times_trace(t) for t in traces ]
+
+    min_len = min([ len(t) for t in times ])
+    times = [ t[:min_len][1:] for t in times ]
+
+    dx = np.arange(len(times[0]))
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+
+    for t, name in zip(times, names):
+        #  ax.plot(dx, t, label=name)
+        ax.plot(dx, t.cumsum(), label=name)
+
+    ymaj, ymin = 0.2, 0.04
+    ymaj, ymin = 1000, 100
+
+    ax.yaxis.set_major_locator(MultipleLocator(ymaj))
+    ax.yaxis.set_minor_locator(MultipleLocator(ymin))
+    ax.set_ylim(bottom=0)
+    ax.yaxis.grid(visible=True, which="major", color="#bbb")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+
+    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, pos: "{:.1f} s".format(x)))
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (s)")
+    ax.set_title("Runs - Comparison - 10K")
+
+    #  ax2 = ax.twinx()
+    #  ax2.clear()
+    #  ty2 = list(zip(times[:-1], times[1:]))
+    #  cmap = plt.colormaps["Dark2"]
+    #  for idx, (t_old, t_new) in enumerate(ty2):
+        #  dy = -(t_new / t_old) + 1
+        #  N = 20
+        #  dy = np.convolve(dy, np.ones((N,)) / N, mode="valid")
+        #  dx = np.arange(len(dy))
+        #  label = "improv_pct: {} -> {}".format(idx, idx + 1)
+        #  ax2.plot(dx, dy, label=label, color=cmap(idx), linestyle="--")
+
+    #  ax2.yaxis.set_major_formatter(
+        #  FuncFormatter(lambda x, pos: "{:.0f}%".format(x * 100))
+    #  )
+    #  ax2.set_ylim([0, 0.3])
+    fig.tight_layout()
+
+    fig.legend(loc="lower center", fontsize=12)
+
+    plot_fname = "ts_comp_" + "_".join(traces)
+    PlotSaver.save(fig, "", None, plot_fname)
+
+    pass
+
+
+def run():
+    #  trace_name = "athenapk5"
+    #  plot_msg_timestamps(trace_name)
+
+    traces = [
+        "athenapk14",
+        "athenapk15",
+        "athenapk16"
+    ]
+
+    names = [ "Baseline", "LPT", "Contig++" ]
+
+    compare_perf_traces(traces, names)
+
+    pass
+
+
+if __name__ == "__main__":
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    plot_init_big()
+    run()
diff --git a/scripts/tau_analysis/20230831_athmsgs.py b/scripts/tau_analysis/20230831_athmsgs.py
new file mode 100644
index 0000000..c4ebd38
--- /dev/null
+++ b/scripts/tau_analysis/20230831_athmsgs.py
@@ -0,0 +1,261 @@
+import matplotlib
+import multiprocessing
+import numpy as np
+import os
+import pandas as pd
+import pickle
+import re
+
+import matplotlib.colors as colors
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+from analyze_msgs import aggr_msgs_all, join_msgs, get_relevant_pprof_data
+from common import plot_init_big as plot_init, PlotSaver
+from typing import Tuple, List
+
+
+def set_interactive():
+    matplotlib.use("GTK3Agg")
+    plt.ion()
+
+
+def setup_ax_default(ax):
+    ax.xaxis.grid(visible=True, which="major", color="#999")
+    ax.xaxis.grid(visible=True, which="minor", color="#ddd")
+    ax.xaxis.set_major_locator(ticker.MultipleLocator(32))
+    ax.xaxis.set_minor_locator(ticker.MultipleLocator(8))
+
+    ax.yaxis.grid(visible=True, which="major", color="#aaa")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+
+    ax.set_ylim(bottom=0)
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    )
+    pass
+
+
+def get_msg_dfs(trace_name: str) -> Tuple:
+    trace_path = f"/mnt/ltio/parthenon-topo/{trace_name}/trace"
+    chan_df_path = f"{trace_path}/msgs.aggr.chan.csv"
+    send_df_clipped_path = f"{trace_path}/msgs.aggr.send.clipped.csv"
+    send_df_path = f"{trace_path}/msgs.aggr.send.csv"
+
+    if os.path.exists(send_df_clipped_path):
+        print("Clipped send_df found - using that.")
+        send_df_path = send_df_clipped_path
+    else:
+        print("ALERT! send_df_clipped does not exist. Reading the full thing.")
+
+    if os.path.exists(send_df_path) and os.path.exists(chan_df_path):
+        print("Reading chan_df and send_df from aggr files")
+        chan_df = pd.read_csv(chan_df_path)
+        send_df = pd.read_csv(send_df_path)
+    else:
+        print("ALERT! Aggr files for send/chan don't exist. Reading bins")
+        chan_df, send_df = aggr_msgs_all(trace_name)
+
+    max_ts = send_df["ts"].max()
+    max_rank = send_df["rank"].max()
+    print(f"Send_df. Max_ts: {max_ts}, Max_rank: {max_rank}")
+
+    return chan_df, send_df
+
+
+def get_mats(trace_name: str, ts: int):
+    chan_df, send_df = get_msg_dfs(trace_name)
+    send_df = send_df[send_df["ts"] == ts]
+    msg_df = join_msgs(send_df, chan_df)
+
+    blk_mat_df = msg_df.groupby(["blk_id", "nbr_id"], as_index=False).agg(
+        {"msgsz": "count"}
+    )
+    blk_mat = (
+        blk_mat_df.pivot(index="blk_id", columns="nbr_id", values="msgsz")
+        .fillna(0)
+        .astype(int)
+    )
+
+    rank_mat_df = msg_df.groupby(["blk_rank", "nbr_rank"], as_index=False).agg(
+        {"msgsz": "count"}
+    )
+
+    rank_mat = (
+        rank_mat_df.pivot(index="blk_rank", columns="nbr_rank", values="msgsz")
+        .fillna(0)
+        .astype(int)
+    )
+
+    sym_test = sum(blk_mat.sum(axis=1) - blk_mat.sum(axis=0))
+    if sym_test != 0:
+        print("message exchange is not symmetric!!")
+
+    return blk_mat, rank_mat
+
+
+def max_non_zero_column_per_row(matrix):
+    non_zero_mask = matrix != 0
+    max_columns = np.max(
+        np.where(non_zero_mask, np.arange(matrix.shape[1]), -1), axis=1
+    )
+    return max_columns
+
+
+def plot_1():
+    fig, ax = plt.subplots(1, 1, figsize=(11, 8))
+
+    dyt = kfls + kflr + kbcs + kbcr
+    dx = range(len(dyt))
+
+    ax.plot(dx, dyt)
+    setup_ax_default(ax)
+
+    ax.set_ylabel("Time (s)")
+    ax.set_xlabel("Rank ID")
+    ax.set_title("Msg Time vs Comm Time (FL_S + FL_R + BC_S + BC_R)")
+
+    fig.tight_layout()
+    plot_fname = f"{trace_name}_flbc_sndrcv"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+def plot_2():
+    fig, ax = plt.subplots(1, 1, figsize=(11, 8))
+
+    dyt = kfls + kbcs
+    dx = range(len(dyt))
+
+    ax.plot(dx, dyt)
+    setup_ax_default(ax)
+
+    ax.set_ylabel("Time (s) (blue)")
+    ax.set_xlabel("Rank ID")
+    ax.set_title("Msg Time vs Comm Time (Send)")
+
+    ax2 = ax.twinx()
+    ax2.plot(dx, dyn, color="red")
+    ax2.yaxis.set_label_position("right")
+    ax2.set_ylabel("Msgs Sent (red)")
+    ax2.set_ylim(bottom=0)
+
+    # to align ax1 and ax2
+    ax.set_ylim([35 * 1e6, 115 * 1e6])
+
+    fig.tight_layout()
+    plot_fname = f"{trace_name}_flbc_snd.vs.msgcnt"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+def plot_msg_time_corr():
+    trace_name = "athenapk13"
+    pprof_data = get_relevant_pprof_data(trace_name)
+
+    getslot_df = pd.read_csv(f"/mnt/ltio/parthenon-topo/{trace_name}/getslot.csv")
+    getslot_df
+    dygs = getslot_df["usec_sum"]
+
+    blk_mat, rank_mat = get_mats(trace_name, 4)
+
+    rank_msgs = rank_mat.sum(axis=0).to_numpy()
+    rank_msgs.size
+
+    kbcs = pprof_data["kbcs"]
+    kbcr = pprof_data["kbcr"]
+    kfls = pprof_data["kfls"]
+    kflr = pprof_data["kflr"]
+
+    plot_1()
+    plot_2()
+
+    send_time = pprof_data["kbcs"]
+    recv_time = pprof_data["kbcr"]
+    mip_time = pprof_data["mip"]
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+
+    ax.set_xlim([192, 512])
+    ax.set_xlim([0, 512])
+    ax.clear()
+    ax2.clear()
+
+    dyt = send_time
+    dyt = mip_time
+    dyt = pprof_data["kbcs"] + pprof_data["kfls"]
+    dyt = kfls + kflr
+    dyt = kfls + kflr + kbcs + kbcr
+    dyn = rank_msgs
+    dx = range(len(dyt))
+
+    ax.plot(dx, dyt)
+    ax2 = ax.twinx()
+    ax2.plot(dx, dyn, color="red")
+    ax2.yaxis.set_label_position("right")
+    ax2.set_ylabel("Time (us)")
+    ax.set_ylabel("Msgs Sent")
+
+    setup_ax_default(ax)
+
+    ax.set_ylim([35 * 1e6, 115 * 1e6])
+    ax2.set_ylim(bottom=0)
+
+    ax.set_title("Msg Exchange Count vs Comm Time (Send)")
+
+    fig.tight_layout()
+
+    pass
+
+
+def idk():
+    trace_name = "athenapk10"
+    pprof_data = get_relevant_pprof_data(trace_name)
+    blk_mat, rank_mat = get_mats("athenapk6", 3)
+    plt.ion()
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 8))
+    ax.imshow(blk_mat)
+
+    fig, ax = plt.subplots(1, 1, figsize=(16, 16))
+    ax.imshow(rank_mat)
+
+    max_nz_blk = max_non_zero_column_per_row(blk_mat)
+    max_nz_rank = max_non_zero_column_per_row(rank_mat)
+
+    dx = np.arange(512)
+    fig, ax = plt.subplots(1, 1, figsize=(14, 8))
+    ax.plot(dx, max_nz_rank)
+
+    ax2 = ax.twinx()
+    ax2.clear()
+    dy = pprof_data["kbcs"] + pprof_data["kbcr"]
+    dy = pprof_data["kbcs"]
+    ax2.plot(dx, dy, color="red")
+
+    msg_dy = rank_mat.sum(axis=1)
+    ax.clear()
+    ax.plot(dx, msg_dy)
+    ax.set_ylim([0, 600])
+    ax2.set_ylim([0, 6e8])
+
+    ax.xaxis.grid(visible=True, which="major", color="#bbb")
+    ax.xaxis.grid(visible=True, which="minor", color="#ddd")
+    ax.xaxis.set_major_locator(ticker.MultipleLocator(32))
+    ax.xaxis.set_minor_locator(ticker.MultipleLocator(8))
+
+    ax2.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    )
+
+
+def run():
+    idk()
+    pass
+
+
+if __name__ == "__main__":
+    plot_init()
+    run()
diff --git a/scripts/tau_analysis/20230911_perfprobes.py b/scripts/tau_analysis/20230911_perfprobes.py
new file mode 100644
index 0000000..5cc088b
--- /dev/null
+++ b/scripts/tau_analysis/20230911_perfprobes.py
@@ -0,0 +1,150 @@
+import multiprocessing
+import numpy as np
+import os
+import pandas as pd
+import pickle
+import re
+
+import matplotlib.colors as colors
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+from analyze_msgs import aggr_msgs_all, join_msgs, get_relevant_pprof_data
+
+from common import plot_init_big as plot_init, PlotSaver
+
+global tdir
+
+
+def read_perf_df(rank: int) -> pd.DataFrame:
+    global tdir
+    fpath = f"{tdir}/profile/data.{rank}.csv"
+    print(fpath)
+    #  fpath = "/mnt/ltio/parthenon-topo/athenapk11/profile/data.{}.csv".format(rank)
+    df = pd.read_csv(fpath, names=["usec"])
+    df["rank"] = rank
+    return df
+
+
+def read_perf_data(trace_dir: str) -> None:
+    global tdir
+    tdir = trace_dir
+
+    ranks = np.arange(0, 512)
+    with multiprocessing.Pool(16) as p:
+        aggr_df = p.map(read_perf_df, ranks)
+
+    all_dfs = aggr_df
+    aggr_df = pd.concat(all_dfs)
+    aggr_df.sort_values("usec")
+    sum_df = aggr_df.groupby(["rank"], as_index=False).agg(
+        {"usec": ["sum", "max", "mean"]}
+    )
+
+    sum_df_out = f"{trace_dir}/getslot.csv"
+    sum_df.to_csv(sum_df_out, index=None)
+
+    return sum_df
+
+
+def plot_perf_total(sum_df, trace_dir):
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    ranks = np.arange(0, 512)
+    ax.plot(ranks, sum_df["usec"]["sum"], label="getslot_sum_us", color="red")
+
+    ax.set_title("getslot(): total_time and max_time")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Avg Time (ms)")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} us".format(x))
+    )
+
+    ax.yaxis.grid(visible=True, which="major", color="#bbb")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+
+    ax.set_ylim(bottom=0)
+
+    fig.legend()
+    fig.tight_layout()
+
+    trace_name = os.path.basename(trace_dir)
+    plot_fname = f"{trace_name}_perf_getslot_sum"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def plot_perf_avg_max(sum_df, trace_dir):
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+    ax2 = ax.twinx()
+
+    ranks = np.arange(0, 512)
+    ax.plot(ranks, sum_df["usec"]["mean"], label="getslot_sum_us", color="red")
+    ax2.plot(ranks, sum_df["usec"]["max"], label="getslot_max_us", color="blue")
+
+    ax.set_title("getslot(): total_time and max_time")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Avg Time (ms)")
+    ax2.set_ylabel("Max Time (ms")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} us".format(x))
+    )
+    ax2.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} ms".format(x / 1e3))
+    )
+
+    ax.yaxis.grid(visible=True, which="major", color="#bbb")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+
+    ax.set_ylim(bottom=0)
+    ax2.set_ylim(bottom=0)
+
+    fig.legend()
+    fig.tight_layout()
+
+    trace_name = os.path.basename(trace_dir)
+    plot_fname = f"{trace_name}_perf_getslot_sum_max"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def plot_perf_max(sum_df_a, sum_df_b):
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    ranks = np.arange(512)
+
+    ax.plot(ranks, sum_df_a["usec"]["max"], label="getslot_max_us: before")
+    ax.plot(ranks, sum_df_b["usec"]["max"], label="getslot_max_us: after")
+
+    ax.set_title("getslot(): max_time before and after")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Max Time (ms")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} ms".format(x / 1e3))
+    )
+
+    ax.yaxis.grid(visible=True, which="major", color="#bbb")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+
+    ax.set_ylim(bottom=0)
+
+    fig.legend()
+    fig.tight_layout()
+
+    trace_name = os.path.basename(trace_dir)
+    plot_fname = f"{trace_name}_perf_getslot_max_comp"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def run():
+    trace_dir = "/mnt/ltio/parthenon-topo/athenapk11"
+    sum_df = read_perf_data(trace_dir)
+    #  plot_perf_avg_max(sum_df, trace_dir)
+    plot_perf_total(sum_df, trace_dir)
+    pass
+
+
+if __name__ == "__main__":
+    plot_init()
+    run()
diff --git a/scripts/tau_analysis/20230919_getslot.py b/scripts/tau_analysis/20230919_getslot.py
new file mode 100644
index 0000000..6e19504
--- /dev/null
+++ b/scripts/tau_analysis/20230919_getslot.py
@@ -0,0 +1,255 @@
+import multiprocessing
+import numpy as np
+import os
+import pandas as pd
+import pickle
+import re
+
+import matplotlib.colors as colors
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+from analyze_msgs import aggr_msgs_all, join_msgs, get_relevant_pprof_data
+
+from common import plot_init_big as plot_init, PlotSaver
+
+
+def read_rank_df(rank: int) -> pd.DataFrame:
+    trace_dir = "/mnt/ltio/parthenon-topo/athenapk12"
+    df_path = f"{trace_dir}/profile/perf.parsed.{rank}"
+
+    cols = [ "time_us", "parth", "psm_1", "psm_2" ]
+    df = pd.read_csv(df_path, names=cols, on_bad_lines="skip")
+
+    df_agg = df.groupby(["parth", "psm_1", "psm_2"], as_index=False).agg(
+        {"time_us": ["sum", "count"]}
+    )
+
+    df_agg["rank"] = rank
+
+    cols_new = ["rank", "parth", "psm_1", "psm_2", "time_us"]
+    cols_flat = cols_new[:-1] + ["time_us_sum", "time_us_count"]
+    df_agg = df_agg[cols_new]
+    df_agg.columns = cols_flat
+
+    return df_agg
+
+
+def plot_getslot_total(aggr_df):
+    rank_df = aggr_df.groupby(["rank"], as_index=False).agg(
+        {"time_us_sum": "sum", "time_us_count": "sum"}
+    )
+
+    fig, ax = plt.subplots(1, 1)
+    dx = rank_df["rank"]
+    dy = rank_df["time_us_sum"]
+    dy2 = rank_df["time_us_count"]
+
+    ax.plot(dx, dy, label="Time", color="C0", zorder=4)
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Time (ms)")
+    ax.set_title("Getslot() Time By Rank")
+
+    ax.set_ylim(bottom=0)
+    # 1e5 = 1e3 for us to ms, 100 for 100 timesteps
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} ms".format(x / 1e5))
+    )
+    ax.yaxis.grid(visible=True, which="major", color="#bbb")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+
+    ax2 = ax.twinx()
+    ax2.plot(dx, dy2, color="C1", label="Count", zorder=2)
+    ax2.set_ylim(bottom=0)
+    ax2.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f}K".format(x / 1e5))
+    )
+    ax2.set_ylabel("Invoke Count")
+
+    fig.tight_layout()
+    fig.legend()
+
+    ax.set_zorder(ax2.get_zorder() + 1)
+    ax.set_frame_on(False)
+
+    plot_fname = "getslot_total"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+def plot_getslot_by_psm():
+    aggr_df["psm_1"].unique()
+    aggr_df["psm_2"].unique()
+
+    tmp_df = aggr_df.groupby([ "psm_1", "psm_2" ], as_index=False).agg(
+        {"time_us_sum": "sum", "time_us_count": "sum"}
+    ).sort_values("time_us_count")
+
+    func_key = {
+        "am_ctl_getslot_long": "getslot_long",
+        "am_ctl_getslot_med": "getslot_med",
+        "am_ctl_getslot_pkt": "getslot_pkt",
+        "psmi_amsh_long_reply": "amsh_long_reply",
+        "psmi_amsh_short_request": "amsh_short_req",
+        "am_send_pkt_short": "send_pkt_short",
+        "ips_spio_transfer_frame": "spio_xfer_frame",
+        "ips_proto_flow_flush_pio": "ips_proto_xxx",
+        "ips_proto_send_ctrl_message": "ips_proto_xxx",
+        "ips_proto_timer_ctrlq_callback": "ips_proto_xxx",
+    }
+
+    aggr_df["psm_1"] = aggr_df["psm_1"].map(lambda x: func_key[x])
+    aggr_df["psm_2"] = aggr_df["psm_2"].map(lambda x: func_key[x])
+    aggr_df["psm"] = aggr_df["psm_2"] + "->" + aggr_df["psm_1"]
+    aggr_df
+    psm_df = aggr_df.groupby([ "psm", "rank" ], as_index=False).agg(
+        {"time_us_sum": "sum", "time_us_count": "sum"}
+    )
+
+    psm_pvtdf = psm_df.pivot(index='psm', columns='rank', values='time_us_sum')
+    psm_keys = psm_pvtdf.index.to_list()
+    psm_keys
+    psm_mat = psm_pvtdf.to_numpy()
+    psm_mat
+    psm_mat = psm_mat / 1e2 # per_ts
+    psm_mat = psm_mat.astype(int)
+    psm_mat
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+    ax.clear()
+    # normalize by timestep
+    ax.stackplot(np.arange(0, 512), psm_mat / 1000, labels=psm_keys, zorder=2)
+    ax.set_title("spin_lock time by psm-caller")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Time (ms)")
+    ax.set_ylim([0, 30])
+
+    ax.yaxis.grid(visible=True, which="major", color="#bbb")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} ms".format(x))
+    )
+
+    ax.legend()
+
+    fig.tight_layout()
+
+    plot_fname = "getslot.stackbypsm"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+def simplify_parth_stack(callchain: str):
+    distinct_keys = [
+        "IOWrapper",
+        "LoadFromFile",
+        "LoadBalancingAndAdaptiveMeshRefinement",
+        "ReceiveBoundaryBuffers",
+        "SendBoundBufs",
+        "ReceiveBoundBufs",
+        "StartReceiveBoundBufs",
+        "ReceiveFluxCorrections",
+        "LoadAndSendFluxCorrections",
+        "CommBuffer::Send",
+        "CommBuffer::TryStartReceive",
+        "CommBuffer::IsAvailable"
+    ]
+
+    for key in distinct_keys:
+        if key in callchain:
+            return key
+            pass
+
+    return "Uncategorized"
+
+def map_parth_stack(df, col):
+    parth_stack_map = {
+        "IOWrapper": "IO",
+        "LoadFromFile": "IO",
+        "LoadBalancingAndAdaptiveMeshRefinement": "LB",
+        "ReceiveBoundaryBuffers": "Recv_BB",
+        "SendBoundBufs": "Send_BB",
+        "ReceiveBoundBufs": "Recv_BB",
+        "StartReceiveBoundBufs": "Recv_BB",
+        "ReceiveFluxCorrections": "Recv_FL",
+        "LoadAndSendFluxCorrections": "Send_FL",
+        "CommBuffer::Send": "Comm",
+        "CommBuffer::TryStartReceive": "Comm",
+        "CommBuffer::IsAvailable": "Comm",
+        "Uncategorized": "Uncategorized"
+    }
+
+    df[col] = df[col].apply(simplify_parth_stack)
+    df[col] = df[col].apply(lambda x: parth_stack_map[x])
+    return df
+
+def simplify_list(l):
+    print(len(l))
+    l = list(set(map(simplify_parth_stack, l)))
+    print(len(l))
+    for k in l[:20]:
+        print(k)
+
+    return l
+
+
+def plot_getslot_by_parth():
+    parth_df = map_parth_stack(aggr_df, "parth")
+    parth_df = parth_df.groupby([ "parth", "rank" ], as_index=False).agg(
+        {"time_us_sum": "sum", "time_us_count": "sum"}
+    )
+    parth_pvtdf = parth_df.pivot(index='parth', columns='rank', values='time_us_sum')
+    parth_pvtdf = parth_pvtdf.fillna(0)
+
+    parth_keys = list(parth_pvtdf.index)
+    parth_keys
+    parth_mat = parth_pvtdf.to_numpy()
+    parth_mat
+    parth_mat = parth_mat.astype(int)
+    parth_mat
+    
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+    ax.clear()
+    # normalize by timestep
+    ax.stackplot(np.arange(0, 512), parth_mat / 1e5, labels=parth_keys, zorder=2)
+    ax.set_title("spin_lock time by parthenon-caller")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Time (ms)")
+    ax.set_ylim([0, 30])
+
+    ax.yaxis.grid(visible=True, which="major", color="#bbb")
+    ax.yaxis.grid(visible=True, which="minor", color="#ddd")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} ms".format(x))
+    )
+
+    ax.legend(ncol=4)
+
+    fig.tight_layout()
+
+    plot_fname = "getslot.stackbyparth"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def aggr_dfs():
+    all_ranks = np.arange(0, 512)
+    all_ranks
+
+    aggr_df = None
+    with multiprocessing.Pool(16) as p:
+        all_dfs = p.map(read_rank_df, all_ranks)
+        aggr_df = pd.concat(all_dfs)
+
+    aggr_df
+    aggr_df_out = "/mnt/ltio/parthenon-topo/athenapk12/getslot_perf.csv"
+    aggr_df.to_csv(aggr_df_out, index=False)
+
+    plot_getslot_total(aggr_df)
+    pass
+
+
+if __name__ == "__main__":
+    plot_init()
+    run()
diff --git a/scripts/tau_analysis/20231016_analyze_pprof.py b/scripts/tau_analysis/20231016_analyze_pprof.py
new file mode 100644
index 0000000..ac1da40
--- /dev/null
+++ b/scripts/tau_analysis/20231016_analyze_pprof.py
@@ -0,0 +1,722 @@
+import os
+import glob
+import multiprocessing
+import numpy as np
+import pandas as pd
+import io
+import ipdb
+import pickle
+import subprocess
+import string
+import sys
+import time
+
+#  import ray
+import re
+import traceback
+from common import plot_init, plot_init_big, PlotSaver, profile_label_map
+from typing import Tuple
+
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+from pathlib import Path
+
+#  from task import Task
+from trace_reader import TraceOps
+from typing import List, Dict
+
+global trace_dir_fmt
+trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+
+"""
+Returns all unique leaf events in the set
+Discards intermediate events if leaf events present
+"""
+
+
+def get_top_events(df: pd.DataFrame, cutoff: float = 0.05):
+    total_runtime = df[df["name"] == ".TAU application"]["incl_usec"].iloc[0]
+
+    top_df = df[df["incl_usec"] >= total_runtime * cutoff]
+    top_names = top_df["name"].unique()
+
+    return trim_and_filter_events(top_names)
+    top_names
+
+    cand_names = top_names
+    filt_suffixes = [
+        "taupreload_main",
+        "MPI_Isend()",
+        "MPI_Iprobe()",
+        "MPI Collective Sync",
+    ]
+    cand_names = remove_events_suffix(cand_names, filt_suffixes)
+
+    filt_suffixes = [
+        ".TAU application => Driver_Main",
+        "Driver_Main => MultiStage_Step",
+    ]
+    cand_names = remove_events_suffix(cand_names, filt_suffixes)
+    cand_names = dedup_events(cand_names)
+
+    # remove super long events
+    #  all_rel_evts = [ e for e in cand_names if len(e) < 160 ]
+    #  print(f"Dropped long events. Orig: {len(cand_names)}, New: {len(all_rel_evts)}")
+    all_rel_evts = cand_names
+
+    """ Old athenapk events - may be relevant """
+    #  prim_evt = ".TAU application"
+    #  prim_df = df[df["name"].str.contains(prim_evt)]
+    #  top_df = prim_df[prim_df["incl_usec"] >= total_runtime * cutoff]
+    #  all_evts = top_df["name"].to_list()
+
+    #  all_evts = sorted(all_evts, key=lambda x: len(x), reverse=True)
+    #  all_rel_evts = []
+    #  has_prefix = lambda ls, x: any([lsx.startswith(x) for lsx in ls])
+    #  for evt in all_evts:
+    #  if not has_prefix(all_rel_evts, evt):
+    #  all_rel_evts.append(evt)
+
+    return all_rel_evts
+
+
+def fold_cam_case(name, cpref=1, csuf=4, sufidx=-2):
+    splitted = re.sub("([A-Z][a-z]+)", r" \1", re.sub("([A-Z]+)", r" \1", name)).split()
+
+    pref = "".join([s[0:cpref] for s in splitted[:sufidx]])
+    suf = "".join([s[0:csuf] for s in splitted[sufidx:]])
+    folded_str = pref + suf
+    return folded_str
+
+
+def abbrev_evt(evt: str):
+    evt_ls = evt.split("=>")
+    evt_ls = evt_ls[-2:]
+    evt_clean = []
+
+    for evt_idx, evt in enumerate(evt_ls):
+        evt = re.sub(r"Kokkos::[^ ]*", "", evt)
+        evt = re.sub(r"Task_", "", evt)
+        evt = re.sub(r".*?::", "", evt)
+        evt = re.sub(r"\[.*?\]", "", evt)
+        evt = evt.strip()
+        evt = re.sub(r" ", "_", evt)
+        if evt_idx == len(evt_ls) - 1:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=4, sufidx=-2)
+        else:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=1, sufidx=-2)
+            #  evt_try = "".join([c for c in evt if c in string.ascii_uppercase])
+        if len(evt_try) > 1:
+            evt_clean.append(evt_try)
+        else:
+            evt_clean.append(evt)
+
+    abbrev = "_".join(evt_clean)
+
+    if "MPIA" in abbrev:
+        abbrev = "MPI-AllGath"
+
+    return abbrev
+
+
+def get_event_array(concat_df: pd.DataFrame, event: str) -> List:
+    nranks = 512
+
+    ev1 = event
+    ev2 = f"{ev1} [THROTTLED]"
+
+    ev1_mask = concat_df["name"] == ev1
+    ev2_mask = concat_df["name"] == ev2
+    temp_df = concat_df[ev1_mask | ev2_mask]
+
+    #  temp_df = concat_df[concat_df["name"] == event]
+    if len(temp_df) != nranks:
+        print(
+            f"WARN: {event} missing some ranks (nranks={nranks}), found {len(temp_df)}"
+        )
+    else:
+        return temp_df["incl_usec"].to_numpy()
+        pass
+
+    all_rank_data = []
+    all_ranks_present = temp_df["rank"].to_list()
+
+    temp_df = temp_df[["incl_usec", "rank"]].copy()
+    join_df = pd.DataFrame()
+    join_df["rank"] = range(nranks)
+    join_df = join_df.merge(temp_df, how="left").fillna(0).astype({"incl_usec": int})
+    data = join_df["incl_usec"].to_numpy()
+    return data
+
+
+def read_pprof(fpath: str):
+    f = open(fpath).readlines()
+    lines = [l.strip("\n") for l in f if l[0] != "#"]
+
+    nfuncs = int(re.findall("(\d+)", lines[0])[0])
+    rel_lines = lines[1:nfuncs]
+    prof_cols = [
+        "name",
+        "ncalls",
+        "nsubr",
+        "excl_usec",
+        "incl_usec",
+        "unknown",
+        "group",
+    ]
+    df = pd.read_csv(
+        io.StringIO("\n".join(rel_lines)), delim_whitespace=True, names=prof_cols
+    )
+
+    rank = re.findall(r"profile\.(\d+)\.0.0$", fpath)[0]
+    df["rank"] = rank
+    return df
+
+
+def read_all_pprof_simple(trace_dir: str):
+    pprof_glob = f"{trace_dir}/profile/profile.*"
+    #  pprof_files = list(map(lambda x: f"{trace_dir}/profile/profile.{x}.0.0", range(32)))
+    all_files = glob.glob(pprof_glob)
+    #  all_files = pprof_files
+
+    print(f"Trace dir: {trace_dir}, reading {len(all_files)} files")
+
+    with multiprocessing.Pool(16) as pool:
+        all_dfs = pool.map(read_pprof, all_files)
+
+    concat_df = pd.concat(all_dfs)
+    #  del all_dfs
+
+    concat_df["rank"] = concat_df["rank"].astype(int)
+    concat_df.sort_values(["rank"], inplace=True)
+
+    concat_df["name"] = concat_df["name"].str.strip()
+    return concat_df
+
+
+def filter_relevant_events(concat_df: pd.DataFrame, events: List[str]):
+    temp_df = concat_df[concat_df["rank"] == 0].copy()
+    temp_df.sort_values(["incl_usec"], inplace=True, ascending=False)
+
+    all_data = {}
+
+    for event in events:
+        all_data[event] = get_event_array(concat_df, event)
+
+    return all_data
+
+
+def read_pprof_cached(trace_dir, stack_keys):
+    pickle_path = f"{trace_dir}/pprof.cache.pickle"
+    if os.path.exists(pickle_path):
+        return pickle.loads(open(pickle_path, "rb").read())
+        pass
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    pprof_data = filter_relevant_events(concat_df, stack_keys)
+    with open(pickle_path, "wb+") as f:
+        f.write(pickle.dumps(pprof_data))
+
+    return pprof_data
+
+
+def preprocess_pprof_data(trace_dirs):
+    stack_keys, _, _, _, _ = setup_plot_stacked_generic(traces[0])
+    stack_keys
+    #  stack_keys += [
+        #  "RedistributeAndRefineMeshBlocks",
+    #  ]
+
+    data = read_pprof_cached(trace_dir, stack_keys)
+
+    read_pprof_cached(trace_dirs[0], stack_keys)
+    time.sleep(5)
+    read_pprof_cached(trace_dirs[1], stack_keys)
+    time.sleep(5)
+    read_pprof_cached(trace_dirs[2], stack_keys)
+
+
+def setup_plot_stacked_generic(trace_name):
+    global trace_dir_fmt
+    trace_name = "stochsg10"
+    trace_dir = trace_dir_fmt.format(trace_name)
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    stack_keys = get_top_events(concat_df, 0.02)
+    stack_labels = list(map(abbrev_evt, stack_keys))
+
+    key_tot = ".TAU application"
+    stack_labels[stack_keys.index(key_tot)] = ""
+
+    stack_keys
+    stack_labels
+
+    drop_idxes = [
+        i
+        for i in range(len(stack_labels))
+        if len(stack_labels[i]) > 20 or "/" in stack_labels[i]
+    ]
+    stack_keys = [stack_keys[i] for i in range(len(stack_keys)) if i not in drop_idxes]
+    stack_labels = [
+        stack_labels[i] for i in range(len(stack_labels)) if i not in drop_idxes
+    ]
+
+    ylim = 18000 / 5
+    ymaj = 2000 / 5
+    ymin = 500 / 5
+
+    #  ylim, ymaj, ymin = 12000, 1000, 200
+
+    ylim, ymaj, ymin = 3000, 500, 100
+    #  ylim, ymaj, ymin = 1500, 250, 50
+    #  ylim, ymaj, ymin = 1000, 200, 40
+
+    return stack_keys, stack_labels, ylim, ymaj, ymin
+
+
+def trim_and_filter_events(events: List):
+    trimmed = []
+
+    for e in events:
+        e = re.sub(r"(=> )?\[CONTEXT\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[UNWIND\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[SAMPLE\].*?(?==>|$)", "", e)
+        e = e.strip()
+
+        trimmed.append(e)
+
+    trimmed_uniq = list(set(trimmed))
+    trimmed_uniq = [e for e in trimmed_uniq if e != ""]
+
+    events_mss = sorted(
+        [e for e in trimmed_uniq if e.startswith("Multi") and "=>" in e]
+    )
+
+    events_driver = [
+        e
+        for e in trimmed_uniq
+        if e.startswith("Driver_Main") and "=>" in e and "Multi" not in e
+    ]
+
+    events_lb = [
+        e
+        for e in events
+        if e.startswith("LoadBalancingAndAdaptiveMeshRefinement =>")
+    ]
+
+    all_events = events_mss + events_driver + events_lb
+    all_events = [e for e in all_events if "Sync" not in e]
+
+    all_events += [".TAU application"]
+
+    print(
+        f"Events timmed and dedup'ed. Before: {len(events)}. After: {len(all_events)}"
+    )
+
+    print("Events retained: ")
+    for e in all_events:
+        print(f"\t- {e}")
+
+    #  input("Press ENTER to plot.")
+
+    return all_events
+
+def get_key_classification(keys: list[str]) -> dict[str, str]:
+    key_map = {}
+
+    for k in keys:
+        print(k)
+        if "send" in k.lower():
+            print("\t- Classified SEND")
+            key_map[k] = "send"
+        elif "receive" in k.lower():
+            print("\t- Classified RECV")
+            key_map[k] = "recv"
+        elif "redistributeandrefine" in k.lower():
+            print("\t- Classified LB")
+            key_map[k] = "lb"
+        elif ".tau application" in k.lower():
+            print("\t- Classified APP")
+            key_map[k] = "app"
+        elif "mpi_allred" in k.lower():
+            print("\t- Classified SYNC")
+            key_map[k] = "sync"
+        else:
+            print("\t- Classified Compute")
+            key_map[k] = "comp"
+
+    return key_map
+
+
+def get_rankhour_comparison_offline(trace_dirs: List[str]):
+    all_pprof_summ = []
+
+    for trace_dir in trace_dirs:
+        #  concat_df = read_all_pprof_simple(trace_dir)
+        #  pprof_data = filter_relevant_events(concat_df, stack_keys)
+        #  del concat_df
+        pprof_data = read_pprof_cached(trace_dir, [])
+        key_map = get_key_classification(pprof_data.keys())
+
+        pprof_summ = {}
+        for k in pprof_data.keys():
+            k_map = key_map[k]
+            k_sum = pprof_data[k].sum()
+            if k_map in pprof_summ:
+                pprof_summ[k_map].append(k_sum)
+            else:
+                pprof_summ[k_map] = [k_sum]
+        all_pprof_summ.append(pprof_summ)
+
+    all_pprof_summ
+    all_pprof_summ[0]
+
+    phase_data = []
+    nranks = 512
+    for idx, t in enumerate(all_pprof_summ):
+        norm_phase_times = {}
+        for p in t:
+            norm_t = (np.array(t[p]) / (nranks * 1e6)).astype(int)
+            norm_phase_times[p] = norm_t
+        phase_data.append(norm_phase_times)
+
+    phases = ["app", "comp", "send", "recv", "sync", "lb"]
+    phase_events = {}
+    for phase in phases:
+        cur_phase_events = list([p for p in pprof_data.keys() if key_map[p] == phase])
+        phase_events[phase] = cur_phase_events
+
+    phase_data
+    return phase_data
+
+
+
+def get_rankhour_comparison_new(trace_dirs: List[str]):
+    trace_name = traces[0]
+    trace_dir = trace_dirs[0]
+    setup_tuple = setup_plot_stacked_generic(trace_name)
+    stack_keys, stack_labels, ylim, ymaj, ymin = setup_tuple
+
+    rnr_key = [ k for k in stack_keys if "RedistributeAndRefine" in k ]
+    if len(rnr_key) == 0:
+        stack_keys += [
+            "RedistributeAndRefineMeshBlocks",
+        ]
+
+    key_del = "LoadBalancingAndAdaptiveMeshRefinement => UpdateMeshBlockTree"
+    key_del = "Driver_Main => MPI_Allreduce()"
+    stack_keys.remove(key_del)
+
+    key_del = "Driver_Main => LoadBalancingAndAdaptiveMeshRefinement"
+    stack_keys.remove(key_del)
+
+    stack_key_map = {}
+
+    for k in stack_keys:
+        print(k)
+        if "send" in k.lower():
+            print("\t- Classified SEND")
+            stack_key_map[k] = "send"
+        elif "receive" in k.lower():
+            print("\t- Classified RECV")
+            stack_key_map[k] = "recv"
+        elif "redistributeandrefine" in k.lower():
+            print("\t- Classified LB")
+            stack_key_map[k] = "lb"
+        elif ".tau application" in k.lower():
+            print("\t- Classified APP")
+            stack_key_map[k] = "app"
+        elif "mpi_allred" in k.lower():
+            print("\t- Classified SYNC")
+            stack_key_map[k] = "sync"
+        elif "updatemeshblocktree" in k.lower():
+            print("\t- Classified SYNC")
+            stack_key_map[k] = "sync"
+        else:
+            print("\t- Classified Compute")
+            stack_key_map[k] = "comp"
+
+    all_pprof_summ = []
+
+    for trace_dir in trace_dirs:
+        #  concat_df = read_all_pprof_simple(trace_dir)
+        #  pprof_data = filter_relevant_events(concat_df, stack_keys)
+        #  del concat_df
+        pprof_data = read_pprof_cached(trace_dir, [])
+
+        pprof_summ = {}
+        for k in stack_keys:
+            k_map = stack_key_map[k]
+            k_sum = pprof_data[k].sum()
+            if k_map in pprof_summ:
+                pprof_summ[k_map].append(k_sum)
+            else:
+                pprof_summ[k_map] = [k_sum]
+        all_pprof_summ.append(pprof_summ)
+
+    phase_data = []
+    nranks = 512
+    for idx, t in enumerate(all_pprof_summ):
+        norm_phase_times = {}
+        for p in t:
+            norm_t = (np.array(t[p]) / (nranks * 1e6)).astype(int)
+            norm_phase_times[p] = norm_t
+        phase_data.append(norm_phase_times)
+
+    phases = ["app", "comp", "send", "recv", "sync", "lb"]
+    phase_events = {}
+    for phase in phases:
+        cur_phase_events = list([p for p in stack_keys if stack_key_map[p] == phase])
+        phase_events[phase] = cur_phase_events
+
+    phases
+    phase_events
+    phase_data
+    return (phases, phase_events, phase_data)
+
+
+def get_fname(lst):
+    if not lst:
+        return "", []
+        pass
+
+    # Find the shortest string
+    shortest_str = min(lst, key=len)
+
+    # Find common prefix
+    common_prefix = ""
+    for i in range(len(shortest_str)):
+        if all(string[i] == shortest_str[i] for string in lst):
+            common_prefix += shortest_str[i]
+        else:
+            break
+
+    # Find residuals
+    residuals = [string[len(common_prefix) :] for string in lst]
+
+    res_str = "_".join(residuals)
+    fname = f"{common_prefix}_{res_str}"
+    print(f"File Name: {fname}")
+    return fname
+
+
+def plot_rankhour_comparison_simple(trace_names: List[str]) -> None:
+    n_traces = len(traces)
+    width = 0.45
+
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    keys_to_plot = ["comp", "sync", "send", "recv", "lb"]
+    key_labels = ["Compute", "Global Barrier", "MPI Send", "MPI Recv", "LoadBalancing"]
+    bottom = np.zeros(n_traces)
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 6))
+
+    for idx, k in enumerate(keys_to_plot):
+        data_x = np.arange(n_traces)
+        data_y = phase_data_summ[k]
+
+        label = k[0].upper() + k[1:]
+        label = key_labels[idx]
+        ax.bar(data_x, data_y, bottom=bottom, zorder=2, width=width, label=label)
+        bottom += data_y
+
+    p = ax.bar(
+        data_x,
+        phase_data_summ["other"],
+        bottom=bottom,
+        zorder=2,
+        width=width,
+        label="Other",
+        color="#999",
+    )
+
+    ax.bar_label(
+        p,
+        fmt="{:.0f} s",
+        rotation="horizontal",
+        label=phase_data_summ["app"],
+        fontsize=14,
+        padding=4,
+    )
+
+    ax.set_title("Runtime Evolution in Galaxy Sim (10k timesteps)", fontsize=18)
+    ax.set_xticks(data_x)
+    ax.set_xticklabels(trace_names)
+    ax.set_ylabel("Runtime (s)")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} s".format(x))
+    )
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(2000))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(400))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 16000])
+
+    ax.legend(loc="upper right", ncol=3, fontsize=13)
+
+    fig.tight_layout()
+
+    plot_fname = "pprof_rh_simple"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+#  plot_rankhour_comparison_2(trace_names)
+
+
+def plot_rankhour_comparison(trace_names: List[str]) -> None:
+    hatches = ["/", "\\", "|", "-", "+", "x", "o", "O", ".", "*"]
+
+    width = 0.2
+
+    phases, phase_events, phase_data = all_phase_data
+
+    n_phases = np.arange(len(phases))
+    n_traces = len(traces)
+
+    # Reformatting the code with 4-space indentation
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    for i, trace in enumerate(trace_names):
+        for j, phase in enumerate(phases):
+            values = phase_data[i][phase]
+            bottom_value = 0  # Initialize bottom_value for stacking
+            for k, value in enumerate(values):
+                label = trace if j == 0 and k == 0 else ""
+                ax.bar(
+                    n_phases[j] + i * width,
+                    value,
+                    width,
+                    label=label,
+                    color=f"C{i}",
+                    hatch=hatches[k % len(hatches)],
+                    bottom=bottom_value,
+                    edgecolor="black",
+                    zorder=2,
+                )
+                bottom_value += value  # Update bottom_value for next bar in stack
+
+    # Labeling and layout
+    ax.set_xticks(n_phases + width * (n_phases - 1) / 2)
+    ax.set_xticklabels(phases)
+    ax.legend(title="Traces")
+
+    label_fname = get_fname(trace_names)
+    fname = f"pprof_rh_{label_fname}"
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(200))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(50))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    #  ax.set_ylim([0, 12000])
+    ax.set_ylim([0, 1000])
+
+    ax.set_xlabel("App Phase")
+    ax.set_ylabel("Phase Time (s)")
+    ax.set_title("Phase-Wise Perf Breakdown")
+    fig.tight_layout()
+
+    PlotSaver.save(fig, "", None, fname)
+
+plot_rankhour_comparison(trace_names)
+
+
+def prep_data_simple(trace_dirs: list[str]):
+    phase_data = get_rankhour_comparison_offline(trace_dirs)
+    # athenapk5, 2000 timesteps, x5 it
+    for k in phase_data[0]:
+        phase_data[0][k] = (phase_data[0][k] * 5).astype(int)
+        print(phase_data[0][k])
+
+    phase_data
+    trace_names = [
+        "Baseline",
+        "Tuned",
+        "Longest\nProcessing\nTime First",
+        "Contiguous-DP",
+    ]
+
+    trace_names = [
+        "Baseline",
+        "Tuned",
+        "LPT",
+        "Contiguous-DP++",
+    ]
+
+    trace_names = [
+        "Baseline",
+        "LPT",
+        "Contiguous-DP"
+    ]
+
+    phase_data_summ = {}
+    for d in phase_data:
+        for k in d:
+            vsum = np.sum(d[k])
+            if k in phase_data_summ:
+                phase_data_summ[k].append(vsum)
+            else:
+                phase_data_summ[k] = [vsum]
+
+    phase_data_summ
+
+    keys_sum = np.zeros(len(traces), dtype=int)
+    keys_sum
+
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    for k in keys_to_plot:
+        keys_sum += phase_data_summ[k]
+
+    v_other = phase_data_summ["app"] - keys_sum
+    v_other
+    phase_data_summ["other"] = v_other
+
+    print(phase_data_summ)
+
+
+def run_plot_bar_simple():
+    global trace_dir_fmt
+
+    #  traces = ["profile40", "profile41"]
+    traces = ["stochsg10", "stochsg11", "stochsg12"]
+    traces = ["stochsg7", "stochsg8", "stochsg9"]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    all_phase_data = get_rankhour_comparison_new(trace_dirs)
+    pass
+
+
+def run_plot_bar():
+    global trace_dir_fmt
+
+    traces = ["athenapk5", "athenapk14", "athenapk15", "athenapk16"]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    trace_dirs
+    #  plot_rankhour_comparison(traces)
+    for idx in [0, 1]:
+        concat_df = read_all_pprof_simple(trace_dirs[idx])
+        concat_df_out = f"{trace_dirs[idx]}/pprof_concat.csv"
+        concat_df.to_csv(concat_df_out, index=False)
+
+    for k in phase_data[0]:
+        phase_data[0][k] = (phase_data[0][k] * 5).astype(int)
+        print(phase_data[0][k])
+
+    plot_rankhour_comparison(trace_names)
+    plot_rankhour_comparison_2(trace_names)
+
+
+def run():
+    run_plot_bar()
+
+
+if __name__ == "__main__":
+    #  global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    #  plot_init()
+    plot_init_big()
+    run()
diff --git a/scripts/tau_analysis/20231023_pprof_blastwave.py b/scripts/tau_analysis/20231023_pprof_blastwave.py
new file mode 100644
index 0000000..55c08fc
--- /dev/null
+++ b/scripts/tau_analysis/20231023_pprof_blastwave.py
@@ -0,0 +1,190 @@
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+import numpy as np
+
+from common import plot_init, plot_init_big, PlotSaver, profile_label_map
+
+
+global trace_dir_fmt
+trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+
+
+# hardcoded for profile40 to profile43
+def get_rankhour_comparison_blastwave():
+    comp_fd = np.array([2001, 2010, 2001, 2004])
+    comp_cf = np.array([1442, 1442, 1442, 1444])
+    phase_data = {
+        "app": np.array([9093, 8125, 8117, 7979]),
+        "comp": comp_fd + comp_cf,
+        "sync": np.array([646, 237, 334, 282]),
+        "lb": np.array([73, 79, 72, 76]),
+        "send": np.array([468, 635, 476, 478]),
+        "recv": np.array([384, 315, 300, 286]),
+    }
+
+    keys_all = ["comp", "sync", "lb", "send", "recv"]
+    time_def_phases = np.sum([phase_data[k] for k in keys_all], axis=0)
+
+    phase_data["other"] = phase_data["app"] - time_def_phases
+    phase_data
+
+    return phase_data
+
+
+def plot_rankhour_comparison_simple(trace_names: list[str]) -> None:
+    n_traces = len(trace_names)
+    width = 0.45
+
+    phase_data_summ = get_rankhour_comparison_blastwave()
+
+    trace_labels = ["Baseline", "LPT", "Contiguous-DP", "Contiguous-DP++"]
+
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    keys_to_plot = ["comp", "sync", "send", "recv", "lb"]
+    key_labels = ["Compute", "Global Barrier", "MPI Send", "MPI Recv", "LoadBalancing"]
+    bottom = np.zeros(n_traces)
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 6))
+
+    for idx, k in enumerate(keys_to_plot):
+        data_x = np.arange(n_traces)
+        data_y = phase_data_summ[k]
+
+        label = k[0].upper() + k[1:]
+        label = key_labels[idx]
+        ax.bar(data_x, data_y, bottom=bottom, zorder=2, width=width, label=label)
+        bottom += data_y
+
+    p = ax.bar(
+        data_x,
+        phase_data_summ["other"],
+        bottom=bottom,
+        zorder=2,
+        width=width,
+        label="Other",
+        color="#999",
+    )
+
+    ax.bar_label(
+        p,
+        fmt="{:.0f} s",
+        rotation="horizontal",
+        label=phase_data_summ["app"],
+        fontsize=14,
+        padding=4,
+    )
+
+    ax.set_xticks(data_x)
+    ax.set_xticklabels(trace_labels)
+    ax.set_ylabel("Runtime (s)")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} s".format(x))
+    )
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(2000))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(400))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 12000])
+
+    ax.legend(loc="upper right", ncol=3, fontsize=13)
+
+    fig.tight_layout()
+
+    plot_fname = "pprof_rh_simple_blastwave"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+def plot_rankhour_comparison_simple_build(trace_names: list[str]) -> None:
+    n_traces = len(trace_names)
+    width = 0.45
+
+    phase_data_summ = get_rankhour_comparison_blastwave()
+
+    trace_labels = ["Baseline", "LPT", "Contiguous-DP", "Contiguous-DP++"]
+
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    keys_to_plot = ["comp", "sync", "send", "recv", "lb"]
+    key_labels = ["Compute", "Global Barrier", "MPI Send", "MPI Recv", "LoadBalancing"]
+    bottom = np.zeros(n_traces)
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 6))
+    data_x = np.arange(n_traces)
+
+    ax.set_xlim(-0.3975, 3.3975)
+
+    ax.set_title("Runtime Evolution in Blast Wave (30k timesteps)", fontsize=18)
+    ax.set_xticks(data_x[:n_traces])
+    ax.set_xticklabels(trace_labels[:n_traces])
+    ax.set_ylabel("Runtime (s)")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} s".format(x))
+    )
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(2000))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(400))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 12000])
+
+    fig.tight_layout()
+
+    fig.subplots_adjust(right=0.93)
+
+    for idx, k in enumerate(keys_to_plot):
+        data_y = phase_data_summ[k][:n_traces]
+
+        label = k[0].upper() + k[1:]
+        label = key_labels[idx]
+        ax.bar(data_x, data_y, bottom=bottom, zorder=2, width=width, label=label)
+        bottom += data_y
+
+    p = ax.bar(
+        data_x,
+        phase_data_summ["other"][:n_traces],
+        bottom=bottom,
+        zorder=2,
+        width=width,
+        label="Other",
+        color="#999",
+    )
+
+    ax.bar_label(
+        p,
+        fmt="{:.0f} s",
+        rotation="horizontal",
+        label=phase_data_summ["app"],
+        fontsize=14,
+        padding=4,
+    )
+
+    ax.legend(loc="upper right", ncol=3, fontsize=13)
+
+    plot_fname = f"pprof_rh_simple_blastwave_{len(trace_names)}"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def run():
+    global trace_dir_fmt
+    traces = ["profile40", "profile41", "profile42", "profile43"]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    trace_names = traces
+    print(trace_dirs)
+    plot_rankhour_comparison_simple(traces)
+    plot_rankhour_comparison_simple_build(traces[:1])
+    plot_rankhour_comparison_simple_build(traces[:2])
+    plot_rankhour_comparison_simple_build(traces[:3])
+    plot_rankhour_comparison_simple_build(traces[:4])
+
+
+if __name__ == "__main__":
+    #  global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    #  plot_init()
+    plot_init_big()
+    run()
diff --git a/scripts/tau_analysis/20231101_analyze_pprof.py b/scripts/tau_analysis/20231101_analyze_pprof.py
new file mode 100644
index 0000000..274f255
--- /dev/null
+++ b/scripts/tau_analysis/20231101_analyze_pprof.py
@@ -0,0 +1,667 @@
+import glob
+import multiprocessing
+import numpy as np
+import pandas as pd
+import io
+import ipdb
+import pickle
+import subprocess
+import string
+import os
+import sys
+import time
+
+#  import ray
+import re
+import traceback
+from common import plot_init, plot_init_big, PlotSaver, profile_label_map
+
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+import itertools
+
+from pathlib import Path
+
+#  from task import task
+from trace_reader import TraceOps
+
+global trace_dir_fmt
+trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+
+
+def flip(items: list, ncols: int) -> list:
+    flipped = itertools.chain(*[items[i::ncols] for i in range(ncols)])
+    return list(flipped)
+
+
+def abbrev_evt(evt: str):
+    evt_ls = evt.split("=>")
+    evt_ls = evt_ls[-2:]
+    evt_clean = []
+
+    for evt_idx, evt in enumerate(evt_ls):
+        evt = re.sub(r"Kokkos::[^ ]*", "", evt)
+        evt = re.sub(r"Task_", "", evt)
+        evt = re.sub(r".*?::", "", evt)
+        evt = re.sub(r"\[.*?\]", "", evt)
+        evt = evt.strip()
+        evt = re.sub(r" ", "_", evt)
+        if evt_idx == len(evt_ls) - 1:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=4, sufidx=-2)
+        else:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=1, sufidx=-2)
+            #  evt_try = "".join([c for c in evt if c in string.ascii_uppercase])
+        if len(evt_try) > 1:
+            evt_clean.append(evt_try)
+        else:
+            evt_clean.append(evt)
+
+    abbrev = "_".join(evt_clean)
+
+    if "MPIA" in abbrev:
+        abbrev = "MPI-AllGath"
+
+    return abbrev
+
+
+def fold_cam_case(name, cpref=1, csuf=4, sufidx=-2):
+    splitted = re.sub("([A-Z][a-z]+)", r" \1", re.sub("([A-Z]+)", r" \1", name)).split()
+
+    pref = "".join([s[0:cpref] for s in splitted[:sufidx]])
+    suf = "".join([s[0:csuf] for s in splitted[sufidx:]])
+    folded_str = pref + suf
+    return folded_str
+
+
+def read_pprof(fpath: str):
+    f = open(fpath).readlines()
+    lines = [l.strip("\n") for l in f if l[0] != "#"]
+
+    nfuncs = int(re.findall("(\d+)", lines[0])[0])
+    rel_lines = lines[1:nfuncs]
+    prof_cols = [
+        "name",
+        "ncalls",
+        "nsubr",
+        "excl_usec",
+        "incl_usec",
+        "unknown",
+        "group",
+    ]
+    df = pd.read_csv(
+        io.StringIO("\n".join(rel_lines)), delim_whitespace=True, names=prof_cols
+    )
+
+    rank = re.findall(r"profile\.(\d+)\.0.0$", fpath)[0]
+    df["rank"] = rank
+    return df
+
+
+def read_pprof_cached(trace_dir, stack_keys):
+    pickle_path = f"{trace_dir}/pprof.cache.pickle"
+    if os.path.exists(pickle_path):
+        return pickle.loads(open(pickle_path, "rb").read())
+        pass
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    pprof_data = filter_relevant_events(concat_df, stack_keys)
+    with open(pickle_path, "wb+") as f:
+        f.write(pickle.dumps(pprof_data))
+
+    return pprof_data
+
+
+def get_event_array(concat_df: pd.DataFrame, event: str) -> list:
+    nranks = 512
+
+    ev1 = event
+    ev2 = f"{ev1} [THROTTLED]"
+
+    ev1_mask = concat_df["name"] == ev1
+    ev2_mask = concat_df["name"] == ev2
+    temp_df = concat_df[ev1_mask | ev2_mask]
+
+    #  temp_df = concat_df[concat_df["name"] == event]
+    if len(temp_df) != nranks:
+        print(
+            f"WARN: {event} missing some ranks (nranks={nranks}), found {len(temp_df)}"
+        )
+    else:
+        return temp_df["incl_usec"].to_numpy()
+        pass
+
+    all_rank_data = []
+    all_ranks_present = temp_df["rank"].to_list()
+
+    temp_df = temp_df[["incl_usec", "rank"]].copy()
+    join_df = pd.DataFrame()
+    join_df["rank"] = range(nranks)
+    join_df = join_df.merge(temp_df, how="left").fillna(0).astype({"incl_usec": int})
+    data = join_df["incl_usec"].to_numpy()
+    return data
+
+
+def filter_relevant_events(concat_df: pd.DataFrame, events: list[str]):
+    temp_df = concat_df[concat_df["rank"] == 0].copy()
+    temp_df.sort_values(["incl_usec"], inplace=True, ascending=False)
+
+    all_data = {}
+
+    for event in events:
+        all_data[event] = get_event_array(concat_df, event)
+
+    return all_data
+
+
+def trim_and_filter_events(events: list):
+    trimmed = []
+
+    for e in events:
+        e = re.sub(r"(=> )?\[CONTEXT\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[UNWIND\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[SAMPLE\].*?(?==>|$)", "", e)
+        e = e.strip()
+
+        trimmed.append(e)
+
+    trimmed_uniq = list(set(trimmed))
+    trimmed_uniq = [e for e in trimmed_uniq if e != ""]
+    trimmed_uniq
+
+    events_mss = sorted(
+        [e for e in trimmed_uniq if e.startswith("Multi") and "=>" in e]
+    )
+
+    events_driver = [
+        e
+        for e in trimmed_uniq
+        if e.startswith("Driver_Main") and "=>" in e and "Multi" not in e
+    ]
+
+    all_events = events_mss + events_driver
+    all_events = [e for e in all_events if "Sync" not in e]
+
+    [e for e in trimmed_uniq if e not in all_events]
+
+    all_events += [".TAU application"]
+    all_events += ["UpdateMeshBlockTree => MPI_Allgather()"]
+    all_events
+
+    print(
+        f"Events timmed and dedup'ed. Before: {len(events)}. After: {len(all_events)}"
+    )
+
+    print("Events retained: ")
+    for e in all_events:
+        print(f"\t- {e}")
+
+    #  input("Press ENTER to plot.")
+
+    return all_events
+
+
+#  trace_name = "profile40"
+
+
+def setup_plot_stacked_generic(trace_name):
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_name)
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    stack_keys = get_top_events(concat_df, 0.02)
+    stack_labels = list(map(abbrev_evt, stack_keys))
+
+    key_tot = ".TAU application"
+    stack_labels[stack_keys.index(key_tot)] = ""
+
+    stack_keys
+    stack_labels
+
+    drop_idxes = [
+        i
+        for i in range(len(stack_labels))
+        if len(stack_labels[i]) > 20 or "/" in stack_labels[i]
+    ]
+    stack_keys = [stack_keys[i] for i in range(len(stack_keys)) if i not in drop_idxes]
+    stack_labels = [
+        stack_labels[i] for i in range(len(stack_labels)) if i not in drop_idxes
+    ]
+
+    ylim = 18000 / 5
+    ymaj = 2000 / 5
+    ymin = 500 / 5
+
+    ylim, ymaj, ymin = 12000, 1000, 200
+
+    #  ylim, ymaj, ymin = 3000, 500, 100
+    #  ylim, ymaj, ymin = 1500, 250, 50
+    #  ylim, ymaj, ymin = 1000, 200, 40
+
+    return stack_keys, stack_labels, ylim, ymaj, ymin
+
+
+def read_all_pprof_simple(trace_dir: str):
+    pprof_glob = f"{trace_dir}/profile/profile.*"
+    #  pprof_files = list(map(lambda x: f"{trace_dir}/profile/profile.{x}.0.0", range(32)))
+    all_files = glob.glob(pprof_glob)
+    #  all_files = pprof_files
+
+    print(f"Trace dir: {trace_dir}, reading {len(all_files)} files")
+
+    with multiprocessing.Pool(16) as pool:
+        all_dfs = pool.map(read_pprof, all_files)
+
+    concat_df = pd.concat(all_dfs)
+    #  del all_dfs
+
+    concat_df["rank"] = concat_df["rank"].astype(int)
+    concat_df.sort_values(["rank"], inplace=True)
+
+    concat_df["name"] = concat_df["name"].str.strip()
+    return concat_df
+
+
+def preprocess_pprof_data(traces, trace_dirs):
+    stack_keys, _, _, _, _ = setup_plot_stacked_generic(traces[0])
+    stack_keys
+    #  stack_keys += [
+    #  "RedistributeAndRefineMeshBlocks",
+    #  ]
+
+    #  data = read_pprof_cached(trace_dir, stack_keys)
+
+    #  read_pprof_cached(trace_dirs[0], stack_keys)
+    #  time.sleep(5)
+    #  read_pprof_cached(trace_dirs[1], stack_keys)
+    #  time.sleep(5)
+    #  read_pprof_cached(trace_dirs[2], stack_keys)
+    for trace_dir in trace_dirs:
+        read_pprof_cached(trace_dir, stack_keys)
+        time.sleep(2)
+
+
+def get_top_events(df: pd.DataFrame, cutoff: float = 0.05):
+    total_runtime = df[df["name"] == ".TAU application"]["incl_usec"].iloc[0]
+
+    top_df = df[df["incl_usec"] >= total_runtime * cutoff]
+    top_names = top_df["name"].unique()
+
+    return trim_and_filter_events(top_names)
+
+
+def get_key_classification(keys: list[str]) -> dict[str, str]:
+    key_map = {}
+
+    for k in keys:
+        print(k)
+        if "send" in k.lower():
+            print("\t- Classified SEND")
+            key_map[k] = "send"
+        elif "receive" in k.lower():
+            print("\t- Classified RECV")
+            key_map[k] = "recv"
+        elif "redistributeandrefine" in k.lower():
+            print("\t- Classified LB")
+            key_map[k] = "lb"
+        elif "loadbalancingandadaptive" in k.lower():
+            print("\t- Classified LB+SYNC")
+            key_map[k] = "lb+sync"
+        elif ".tau application" in k.lower():
+            print("\t- Classified APP")
+            key_map[k] = "app"
+        elif "mpi_all" in k.lower():
+            print("\t- Classified SYNC")
+            key_map[k] = "sync"
+        elif "mpi" in k.lower():
+            print("\t- Classified MPIOTHER")
+            key_map[k] = "mpioth"
+        elif "makeout" in k.lower():
+            print("\t- Classified IO")
+            key_map[k] = "io"
+        else:
+            print("\t- Classified Compute")
+            key_map[k] = "comp"
+
+    return key_map
+
+
+def get_rankhour_comparison_offline(trace_dirs: list[str]):
+    all_pprof_summ = []
+
+    for trace_dir in trace_dirs:
+        #  concat_df = read_all_pprof_simple(trace_dir)
+        #  pprof_data = filter_relevant_events(concat_df, stack_keys)
+        #  del concat_df
+        pprof_data = read_pprof_cached(trace_dir, [])
+        key_map = get_key_classification(pprof_data.keys())
+
+        pprof_summ = {}
+        for k in pprof_data.keys():
+            k_map = key_map[k]
+            k_sum = pprof_data[k].sum()
+            if k_map in pprof_summ:
+                pprof_summ[k_map].append(k_sum)
+            else:
+                pprof_summ[k_map] = [k_sum]
+        all_pprof_summ.append(pprof_summ)
+
+    all_pprof_summ
+    all_pprof_summ[0]
+
+    phase_data = []
+    nranks = 512
+    for idx, t in enumerate(all_pprof_summ):
+        norm_phase_times = {}
+        for p in t:
+            norm_t = (np.array(t[p]) / (nranks * 1e6)).astype(int)
+            norm_phase_times[p] = norm_t
+        phase_data.append(norm_phase_times)
+
+    phases = ["app", "comp", "send", "recv", "sync", "lb"]
+    phase_events = {}
+    for phase in phases:
+        cur_phase_events = list([p for p in pprof_data.keys() if key_map[p] == phase])
+        phase_events[phase] = cur_phase_events
+
+    phase_data
+    return phase_data
+
+
+def prep_data_simple(trace_dirs: list[str]):
+    phase_data = get_rankhour_comparison_offline(trace_dirs)
+    phase_data
+
+    print("ALERT - scaling 0 by 5X")
+    for k in phase_data[0]:
+        phase_data[0][k] = (phase_data[0][k] * 5).astype(int)
+        print(phase_data[0][k])
+
+    run_names = [
+        "Baseline",
+        "LPT",
+        "Contiguous-DP",
+        "Contiguous-DP++",
+        "Contiguous-DP++-1K",
+    ]
+
+    #  run_names = [
+    #  "Baseline",
+    #  "Manually\nTuned",
+    #  "LPT",
+    #  "Contiguous-DP++",
+    #  ]
+
+    run_names = run_names[: len(trace_dirs)]
+
+    phase_data_summ = {}
+    for d in phase_data:
+        for k in d:
+            vsum = np.sum(d[k])
+            if k in phase_data_summ:
+                phase_data_summ[k].append(vsum)
+            else:
+                phase_data_summ[k] = [vsum]
+
+    phase_data_summ
+
+    keys_sum = np.zeros(len(trace_dirs), dtype=int)
+    keys_sum
+
+    if "lb" not in phase_data_summ:
+        a = np.array(phase_data_summ["lb+sync"])
+        b = np.array(phase_data_summ["sync"])
+        phase_data_summ["lb"] = a - b
+
+        del phase_data_summ["lb+sync"]
+
+    if "mpioth" in phase_data_summ:
+        phase_data_summ["sync"] += np.array(phase_data_summ["mpioth"])
+        del phase_data_summ["mpioth"]
+
+    phase_data_summ
+    if "io" in phase_data_summ:
+        del phase_data_summ["io"]
+
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb", "io"]
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    for k in keys_to_plot:
+        keys_sum += phase_data_summ[k]
+
+    v_other = phase_data_summ["app"] - keys_sum
+    v_other
+    phase_data_summ["other"] = v_other
+
+    print(phase_data_summ)
+    G_data_x = np.arange(len(run_names))
+    return run_names, phase_data_summ
+
+
+def prep_data_blastwave():
+    global G_plot_title
+    global G_plot_ylim
+    global G_plot_fname
+    global G_data_x
+    global trace_dir_fmt
+
+    G_plot_title = "Runtime Evolution in Blast Wave (30k timesteps)"
+    G_plot_ylim = 12000
+    G_plot_fname = f"pprof_rh_simple_blastwave_new"
+
+    traces = ["profile40", "profile41", "profile42", "profile43"]
+    trace_dirs = [trace_dir_fmt.format(t) for t in traces]
+    phase_data = get_rankhour_comparison_offline(trace_dirs)
+
+    run_names = [
+        "Baseline",
+        "LPT",
+        "Contiguous-DP",
+        "Contiguous-DP++",
+        #  "Contiguous-DP++-1K",
+    ]
+
+    phase_data_summ = {}
+    for d in phase_data:
+        for k in d:
+            vsum = np.sum(d[k])
+            if k in phase_data_summ:
+                phase_data_summ[k].append(vsum)
+            else:
+                phase_data_summ[k] = [vsum]
+
+    #  if "lb" not in phase_data_summ:
+    a = np.array(phase_data_summ["lb+sync"])
+    b = np.array(phase_data_summ["sync"])
+    phase_data_summ["lb"] = a - b
+
+    del phase_data_summ["lb+sync"]
+
+    #  if "mpioth" in phase_data_summ:
+    phase_data_summ["sync"] += np.array(phase_data_summ["mpioth"])
+    del phase_data_summ["mpioth"]
+
+    #  if "io" in phase_data_summ:
+    #  del phase_data_summ["io"]
+
+    keys_sum = np.zeros(len(trace_dirs), dtype=int)
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb", "io"]
+    for k in keys_to_plot:
+        keys_sum += phase_data_summ[k]
+
+    v_other = phase_data_summ["app"] - keys_sum
+    phase_data_summ["other"] = v_other
+
+    phase_data_summ
+
+    G_data_x = np.arange(len(run_names))
+    # Build some distance between 3 and 4
+    if len(G_data_x) >= 4:
+        G_data_x = G_data_x.astype(float)
+        G_data_x[2] -= 0.06
+        G_data_x[3] += 0.06
+
+    return run_names, phase_data_summ
+
+
+def prep_data_athenapk():
+    global G_plot_title
+    global G_plot_ylim
+    global G_plot_fname
+    global G_data_x
+    global trace_dir_fmt
+
+    G_plot_title = "Runtime Evolution in Galaxy Cluster (10k timesteps)"
+    G_plot_ylim = 16000
+    G_plot_fname = f"pprof_rh_simple_glxcul_new"
+
+    traces = ["athenapk5", "athenapk14", "athenapk15", "athenapk16"]
+    trace_dirs = [trace_dir_fmt.format(t) for t in traces]
+
+    phase_data = get_rankhour_comparison_offline(trace_dirs)
+    phase_data
+
+    print("ALERT - scaling 0 by 5X")
+    for k in phase_data[0]:
+        phase_data[0][k] = (phase_data[0][k] * 5).astype(int)
+        print(phase_data[0][k])
+
+    run_names = [
+        "Baseline",
+        "Manually\nTuned",
+        "LPT",
+        "Contiguous-DP++",
+    ]
+
+    phase_data_summ = {}
+    for d in phase_data:
+        for k in d:
+            vsum = np.sum(d[k])
+            if k in phase_data_summ:
+                phase_data_summ[k].append(vsum)
+            else:
+                phase_data_summ[k] = [vsum]
+
+    keys_sum = np.zeros(len(trace_dirs), dtype=int)
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    for k in keys_to_plot:
+        keys_sum += phase_data_summ[k]
+
+    v_other = phase_data_summ["app"] - keys_sum
+    phase_data_summ["other"] = v_other
+
+    G_data_x = np.arange(len(run_names))
+    return run_names, phase_data_summ
+
+
+def plot_rankhour_comparison_simple(run_names: list[str], run_data) -> None:
+    global G_plot_title
+    global G_plot_ylim
+    global G_plot_fname
+    global G_data_x
+
+    n_traces = len(run_names)
+    width = 0.45
+
+    print(f"Plotting for {n_traces} traces.")
+
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    keys_to_plot = ["comp", "sync", "send", "recv", "lb", "io"]
+    keys_to_plot = ["comp", "sync", "send", "recv", "lb", "io"]
+    keys_to_plot = ["comp", "io", "send", "recv", "sync", "other", "lb"]
+
+    key_labels = {
+        "comp": "Compute",
+        "sync": "Global Barrier",
+        "send": "MPI Send",
+        "recv": "MPI Recv",
+        "lb": "LoadBalancing",
+        "io": "IO",
+        "other": "Other Comm/Sync",
+    }
+
+    bottom = np.zeros(n_traces)
+    data_x = G_data_x[:n_traces]
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 6))
+
+    #  ax.set_title(G_plot_title, fontsize=18)
+
+    ax.set_xticks(data_x)
+    ax.set_xticklabels(run_names[:n_traces], fontsize=14)
+    ax.set_ylabel("Runtime (s)\n(lower is better)", fontsize=17)
+    ax.tick_params(axis="both", labelsize=15)
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:,.0f} s".format(x))
+    )
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(2000))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(400))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim([0, G_plot_ylim])
+
+    fig.tight_layout()
+    ax.set_xlim([-0.3975, 3.3975])
+    fig.subplots_adjust(bottom=0.1, right=0.9)
+
+    for idx, k in enumerate(keys_to_plot):
+        data_y = run_data[k][:n_traces]
+
+        label = k[0].upper() + k[1:]
+        #  label = key_labels[idx]
+        label = key_labels[k]
+        p = ax.bar(data_x, data_y, bottom=bottom, zorder=2, width=width, label=label)
+        bottom += data_y
+
+    #  p = ax.bar(
+    #  data_x,
+    #  run_data["other"][:n_traces],
+    #  bottom=bottom,
+    #  zorder=2,
+    #  width=width,
+    #  label="Other Comm/Sync",
+    #  color="#999",
+    #  )
+
+    ax.bar_label(
+        p,
+        fmt="{:.0f} s",
+        rotation="horizontal",
+        label=run_data["app"][:n_traces],
+        fontsize=14,
+        padding=4,
+    )
+
+    handles, labels = ax.get_legend_handles_labels()
+    ax.legend(
+        flip(handles, 3),
+        flip(labels, 3),
+        loc="upper left",
+        ncol=3,
+        fontsize=13,
+        bbox_to_anchor=(0.00, 1.02),
+    )
+
+    #  ax.legend(loc="upper left", ncol=3, fontsize=13, bbox_to_anchor=(0.00, 1.02))
+
+    PlotSaver.save(fig, "", None, f"{G_plot_fname}_{n_traces}")
+
+
+def run():
+    #  run_names, run_data = prep_data_simple(trace_dirs)
+    #  run_names, run_data = prep_data_athenapk()
+    run_names, run_data = prep_data_blastwave()
+    n_traces = list(range(len(run_names), 0, -1))
+    #  n_traces = [4]
+
+    for nt in n_traces:
+        plot_rankhour_comparison_simple(run_names[:nt], run_data)
+
+
+if __name__ == "__main__":
+    #  global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    #  plot_init()
+    plot_init_big()
+    run()
diff --git a/scripts/tau_analysis/20231220_blkmat_plots.py b/scripts/tau_analysis/20231220_blkmat_plots.py
new file mode 100644
index 0000000..8dd5890
--- /dev/null
+++ b/scripts/tau_analysis/20231220_blkmat_plots.py
@@ -0,0 +1,93 @@
+import matplotlib.colors as colors
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+import pickle
+import numpy as np
+
+from common import plot_init_big as plot_init, PlotSaver
+
+""" mat0, mat1 = get_evt_mat(0) and get_evt_mat(1) """
+
+
+def get_evt_mat(evt_code):
+    mat_path = f"{trace_dir}/evt{evt_code}.mat.pickle"
+    mat = pickle.loads(open(mat_path, "rb").read())
+    return mat
+
+
+def plot_nblocks_from_evt_mat(mat):
+    mat = mat0
+    blk_cnt = np.sum(~np.isnan(mat), axis=1)
+    blk_cnt
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 7))
+    ax.plot(range(len(blk_cnt)), blk_cnt)
+    ax.grid()
+
+    ax.set_title("Stochastic Subgrid: Nblocks vs Time")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Nblocks")
+
+    fig.tight_layout()
+    plot_fname = "nblocks"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+    pass
+
+
+def plot_compute_vs_ts_fem(mat):
+    mat = mat0
+    mat
+    mat.shape
+    np.percentile(mat, 99)
+    mat.fillna
+    mat = np.nan_to_num(mat0)
+    mat = mat.T[::-1, :]
+
+    vmin = np.percentile(mat, 28)
+    vmax = np.percentile(mat, 99)
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 7))
+
+    bounds = np.linspace(vmin, vmax, 64)
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="max")
+
+    im = ax.imshow(mat, norm=norm, aspect="auto", cmap="plasma")
+    ax.set_title("Block Mat: Evt 0")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("BlockID")
+
+    ymax = mat.shape[0]
+    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda y, pos: f"{int(ymax - y)}"))
+    fig.tight_layout()
+
+    fig.subplots_adjust(left=0.15, right=0.78)
+    cax = fig.add_axes([0.81, 0.12, 0.08, 0.8])
+    cax_fmt = lambda x, pos: "{:.0f} ms".format(x / 1e3)
+    #  cax.yaxis.set_major_formatter(FuncFormatter(cax_fmt))
+    cbar = fig.colorbar(im, cax=cax, format=ticker.FuncFormatter(cax_fmt))
+    plot_fname = "imshow.evt0"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+def plot_evt_percentiles(mat):
+    pass
+
+
+
+def run_do_something():
+    mat0 = get_evt_mat(0)
+    mat0
+    pass
+
+
+def run():
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    trace_name = "stochsg2"
+    trace_dir = trace_dir_fmt.format(trace_name)
+    print(trace_dir)
+    pass
+
+
+if __name__ == "__main__":
+    plot_init()
+    run()
diff --git a/scripts/tau_analysis/20240109_analyze_hdf5.py b/scripts/tau_analysis/20240109_analyze_hdf5.py
new file mode 100644
index 0000000..06fbad5
--- /dev/null
+++ b/scripts/tau_analysis/20240109_analyze_hdf5.py
@@ -0,0 +1,70 @@
+import glob
+import os
+import yt
+import multiprocessing
+import sys
+
+# fp = "/mnt/ltio/parthenon-topo/blastwave01/run/sedov.out1.00031.phdf"
+# ds = yt.load(fp)
+# print(ds.field_list)
+# ds.print_stats()
+# ds.derived_field_list
+
+#  s = yt.SlicePlot(ds, "z", ("parthenon", "advected_0"))
+#  s.annotate_grids()
+#  s.save()
+
+
+def plot_frame(file_path: str, plt_dir: str, plot_vars: list[tuple]) -> None:
+    if not os.path.exists(file_path):
+        return
+
+    file_name = file_path.split("/")[-1]
+    file_name = file_name.replace(".phdf", "")
+    print(f"Plotting {file_name}")
+
+    dataset = yt.load(file_path)
+    for v in plot_vars:
+        s = yt.SlicePlot(dataset, "z", v)
+        s.annotate_grids()
+        #  s.annotate_cell_edges()
+
+        plt_fname = f"{file_name}_{v[1]}"
+        plt_fpath = f"{plt_dir}/{plt_fname}.png"
+        s.save(plt_fpath)
+
+
+def plot_frame_wrapper(args):
+    plot_frame(*args)
+
+
+def get_all_phdf(dir_path: str) -> list[str]:
+    all_files = glob.glob(dir_path + "/*.phdf")
+    return all_files
+
+
+def run():
+    exp_name = "stochsg6"
+    #  exp_name = "sparse1"
+    dir_path = f"/mnt/ltio/parthenon-topo/{exp_name}/run"
+    all_files = get_all_phdf(dir_path)
+
+    #  v = ("parthenon", "sparse_0")
+    v = ("parthenon", "advected_10")
+    plot_path = f"/users/ankushj/repos/amr/scripts/tau_analysis/figures/20240109/{exp_name}/{v[1]}"
+    os.makedirs(plot_path, exist_ok=True)
+
+    print("Input: ", dir_path)
+    print("Output: ", plot_path)
+
+    #  vv = ("parthenon", "advected_10")
+    #  v = ("parthenon", "a")
+    with multiprocessing.Pool(8) as p:
+        jobs = [[f, plot_path, [v]] for f in all_files]
+        print(jobs)
+        p.map(plot_frame_wrapper, jobs)
+    pass
+
+
+if __name__ == "__main__":
+    run()
diff --git a/scripts/tau_analysis/20240109_plot_time.py b/scripts/tau_analysis/20240109_plot_time.py
new file mode 100644
index 0000000..ffd792f
--- /dev/null
+++ b/scripts/tau_analysis/20240109_plot_time.py
@@ -0,0 +1,51 @@
+import numpy as np
+import re
+from common import PlotSaver
+
+import matplotlib.pyplot as plt
+
+def get_times(run_dir):
+    fpath = f"{run_dir}/run/log.txt"
+    data = open(fpath, 'r').read().split('\n')
+    lines = [l.strip() for l in data if l.startswith('cycle=')]
+
+    cycles = []
+    times = []
+
+    for l in lines:
+        mobj = re.match(r'cycle=(\d+).*wsec_step=([^\ ]+).*$', l)
+        c = int(mobj.group(1))
+        t = float(mobj.group(2))
+        
+        cycles.append(c)
+        times.append(t)
+
+    return (cycles, times)
+
+def plot_times(times):
+    data_x, data_y = times
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+
+    ylim = np.percentile(data_y, 97)
+
+    ax.plot(data_x, data_y)
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (s)")
+    ax.set_title("StochSubgrid - Time per Timestep")
+    ax.grid()
+
+    ax.set_ylim([0, ylim])
+
+    fig.tight_layout()
+    plot_fname = "stochsg_times"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+def run():
+    run_dir = "/mnt/ltio/parthenon-topo/stochsg5"
+    times = get_times(run_dir)
+    times
+    plot_times(times)
+    pass
+
+if __ name__ == "__main__":
+    run()
diff --git a/scripts/tau_analysis/20240126_analyze_pprof.py b/scripts/tau_analysis/20240126_analyze_pprof.py
new file mode 100644
index 0000000..20a0095
--- /dev/null
+++ b/scripts/tau_analysis/20240126_analyze_pprof.py
@@ -0,0 +1,741 @@
+import matplotlib
+
+import os
+import glob
+import multiprocessing
+import numpy as np
+import pandas as pd
+import io
+import ipdb
+import pickle
+import subprocess
+import string
+import sys
+import time
+
+#  import ray
+import re
+import traceback
+from common import plot_init, plot_init_big, PlotSaver, profile_label_map
+from typing import Tuple
+
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+from pathlib import Path
+
+#  from task import Task
+from trace_reader import TraceOps
+from typing import List, Dict
+
+global trace_dir_fmt
+trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+
+"""
+Returns all unique leaf events in the set
+Discards intermediate events if leaf events present
+"""
+
+def fold_cam_case(name, cpref=1, csuf=4, sufidx=-2):
+    splitted = re.sub("([A-Z][a-z]+)", r" \1",
+                      re.sub("([A-Z]+)", r" \1", name)).split()
+
+    pref = "".join([s[0:cpref] for s in splitted[:sufidx]])
+    suf = "".join([s[0:csuf] for s in splitted[sufidx:]])
+    folded_str = pref + suf
+    return folded_str
+
+
+def abbrev_evt(evt: str):
+    evt_ls = evt.split("=>")
+    evt_ls = evt_ls[-2:]
+    evt_clean = []
+
+    for evt_idx, evt in enumerate(evt_ls):
+        evt = re.sub(r"Kokkos::[^ ]*", "", evt)
+        evt = re.sub(r"Task_", "", evt)
+        evt = re.sub(r".*?::", "", evt)
+        evt = re.sub(r"\[.*?\]", "", evt)
+        evt = evt.strip()
+        evt = re.sub(r" ", "_", evt)
+        if evt_idx == len(evt_ls) - 1:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=4, sufidx=-2)
+        else:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=1, sufidx=-2)
+            #  evt_try = "".join([c for c in evt if c in string.ascii_uppercase])
+        if len(evt_try) > 1:
+            evt_clean.append(evt_try)
+        else:
+            evt_clean.append(evt)
+
+    abbrev = "_".join(evt_clean)
+
+    if "MPIA" in abbrev:
+        abbrev = "MPI-AllGath"
+
+    return abbrev
+
+
+def get_event_array(concat_df: pd.DataFrame, event: str, nranks) -> List:
+    ev1 = event
+    ev2 = f"{ev1} [THROTTLED]"
+
+    ev1_mask = concat_df["name"] == ev1
+    ev2_mask = concat_df["name"] == ev2
+    temp_df = concat_df[ev1_mask | ev2_mask]
+
+    #  temp_df = concat_df[concat_df["name"] == event]
+    if len(temp_df) != nranks:
+        print(
+            f"WARN: {event} missing some ranks (nranks={nranks}), found {len(temp_df)}"
+        )
+    else:
+        return temp_df["incl_usec"].to_numpy()
+        pass
+
+    all_rank_data = []
+    all_ranks_present = temp_df["rank"].to_list()
+
+    temp_df = temp_df[["incl_usec", "rank"]].copy()
+    join_df = pd.DataFrame()
+    join_df["rank"] = range(nranks)
+    join_df = join_df.merge(temp_df, how="left").fillna(
+        0).astype({"incl_usec": int})
+    data = join_df["incl_usec"].to_numpy()
+    return data
+
+
+def read_pprof(fpath: str):
+    f = open(fpath).readlines()
+    lines = [l.strip("\n") for l in f if l[0] != "#"]
+
+    nfuncs = int(re.findall("(\d+)", lines[0])[0])
+    rel_lines = lines[1:nfuncs]
+    prof_cols = [
+        "name",
+        "ncalls",
+        "nsubr",
+        "excl_usec",
+        "incl_usec",
+        "unknown",
+        "group",
+    ]
+    df = pd.read_csv(
+        io.StringIO("\n".join(rel_lines)), delim_whitespace=True, names=prof_cols
+    )
+
+    rank = re.findall(r"profile\.(\d+)\.0.0$", fpath)[0]
+    df["rank"] = rank
+    return df
+
+
+def read_all_pprof_simple(trace_dir: str):
+    pprof_glob = f"{trace_dir}/profile/profile.*"
+    all_files = glob.glob(pprof_glob)
+    #  pprof_files = list(map(lambda x: f"{trace_dir}/profile/profile.{x}.0.0", range(32)))
+    #  all_files = pprof_files
+
+    print(f"Trace dir: {trace_dir}, reading {len(all_files)} files")
+
+    with multiprocessing.Pool(16) as pool:
+        all_dfs = pool.map(read_pprof, all_files)
+
+    concat_df = pd.concat(all_dfs)
+    #  del all_dfs
+
+    concat_df["rank"] = concat_df["rank"].astype(int)
+    concat_df.sort_values(["rank"], inplace=True)
+
+    concat_df["name"] = concat_df["name"].str.strip()
+    return concat_df
+
+
+def filter_relevant_events(concat_df: pd.DataFrame, events: List[str], nranks):
+    temp_df = concat_df[concat_df["rank"] == 0].copy()
+    temp_df.sort_values(["incl_usec"], inplace=True, ascending=False)
+
+    all_data = {}
+
+    for event in events:
+        all_data[event] = get_event_array(concat_df, event, nranks)
+
+    return all_data
+
+
+def read_pprof_cached(trace_dir, stack_keys, nranks):
+    pickle_path = f"{trace_dir}/pprof.cache.pickle"
+    if os.path.exists(pickle_path):
+        return pickle.loads(open(pickle_path, "rb").read())
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    pprof_data = filter_relevant_events(concat_df, stack_keys, nranks)
+    with open(pickle_path, "wb+") as f:
+        f.write(pickle.dumps(pprof_data))
+
+    return pprof_data
+
+
+def transform_pprof_cached(pprof_data):
+    keys = list(pprof_data.keys())
+    kmpi = "MPI Collective Sync"
+    klb = "LoadBalancingAndAdaptiveMeshRefinement"
+
+    k1 = kmpi
+    k2 = f"UpdateMeshBlockTree => {k1}"
+    k3 = f"Driver_Main => {klb}"
+    k4 = f"{klb} => UpdateMeshBlockTree"
+    k5 = f"{klb} => RedistributeAndRefineMeshBlocks"
+
+    keys_tmp = [k1, k2, k3, k4, k5]
+
+    for k in keys_tmp:
+        print(f"Checking if {k} in keys")
+        assert k in keys
+
+    keys_final = [k for k in keys if k not in keys_tmp and "MPI" not in k]
+
+    data_final = {k: pprof_data[k] for k in keys_final}
+    data_final[klb] = pprof_data[k4] + pprof_data[k5] - pprof_data[k2]
+    data_final[kmpi] = pprof_data[kmpi]
+
+    return data_final
+
+
+def preprocess_pprof_data(trace_dirs, nranks):
+    trace0_df = read_all_pprof_simple(trace_dirs[0])
+    stack_keys = get_top_events(trace0_df, 0.02)
+    del trace0_df
+
+    time.sleep(4)
+
+    for t in trace_dirs:
+        read_pprof_cached(t, stack_keys, nranks)
+        time.sleep(2)
+
+
+def setup_plot_stacked_generic(trace_name):
+    global trace_dir_fmt
+    trace_name = "stochsg10"
+    trace_dir = trace_dir_fmt.format(trace_name)
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    stack_keys = get_top_events(concat_df, 0.02)
+    stack_labels = list(map(abbrev_evt, stack_keys))
+
+    key_tot = ".TAU application"
+    stack_labels[stack_keys.index(key_tot)] = ""
+
+    stack_keys
+    stack_labels
+
+    drop_idxes = [
+        i
+        for i in range(len(stack_labels))
+        if len(stack_labels[i]) > 20 or "/" in stack_labels[i]
+    ]
+    stack_keys = [stack_keys[i]
+                  for i in range(len(stack_keys)) if i not in drop_idxes]
+    stack_labels = [
+        stack_labels[i] for i in range(len(stack_labels)) if i not in drop_idxes
+    ]
+
+    ylim = 18000 / 5
+    ymaj = 2000 / 5
+    ymin = 500 / 5
+
+    #  ylim, ymaj, ymin = 12000, 1000, 200
+
+    ylim, ymaj, ymin = 3000, 500, 100
+    #  ylim, ymaj, ymin = 1500, 250, 50
+    #  ylim, ymaj, ymin = 1000, 200, 40
+
+    return stack_keys, stack_labels, ylim, ymaj, ymin
+
+
+def plot_stacked_pprof(trace_dir: str, trace_label: str, nranks: int):
+    trace_name = os.path.basename(trace_dir)
+    print(f"Running plot_stacked_pprof for trace: {trace_name}, {trace_label}")
+
+    pprof_data = read_pprof_cached(trace_dir, [], nranks)
+    pprof_data = transform_pprof_cached(pprof_data)
+    stack_keys = pprof_data.keys()
+    stack_keys = [k for k in stack_keys if k != ".TAU application"]
+    skidx = list(range(len(stack_keys)))
+    skidx[0] = 1
+    skidx[1] = 0
+    stack_keys = [stack_keys[i] for i in skidx]
+    stack_labels = list(map(abbrev_evt, stack_keys))
+
+    data_y = [pprof_data[k] for k in stack_keys if k != ".TAU application"]
+    data_x = np.arange(len(data_y[0]))
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+    ax.stackplot(data_x, *data_y, labels=stack_labels, zorder=2)
+
+    data_y_app = pprof_data[".TAU application"]
+    y_mean = int(np.mean(data_y_app) / 1e6)
+    print(f"Total mean: {y_mean} s")
+
+    ymax = max(data_y_app)
+    ylim = int(np.ceil(ymax / 1e9) * 1e9)
+
+    # AGGR PLOT
+    ax.plot(data_x, data_y_app, label="APP", zorder=2, linewidth=3)
+
+    ax.set_title(f"Runtime Breakdown - {trace_label}")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Time (s)")
+
+    ax.set_ylim([0, ylim])
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    ax.yaxis.set_major_locator(ticker.AutoLocator())
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    #  ax.legend(ncol=4)
+    ax.legend(ncol=5, fontsize=8)
+
+    fig.tight_layout()
+    plot_fname = f"runtime.pprof.stacked.rankwise.{trace_name}"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+    return
+
+    # INDVL PLOTS
+    for lab, dy in zip(stack_labels, data_y):
+        fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+        ax.plot(data_x, dy, label=lab, zorder=2)
+        ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+        ax.set_ylim(bottom=0)
+        ax.set_title(f"Pprof Component: {lab}")
+        ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())
+        ax.xaxis.set_major_locator(ticker.MultipleLocator(64))
+        ax.xaxis.set_minor_locator(ticker.MultipleLocator(16))
+        plt.grid(visible=True, which="major", color="#999", zorder=0)
+        plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+        plot_fname = f"runtime.pprof.stacked.com{lab.lower()}.{trace_name}"
+        PlotSaver.save(fig, "", None, plot_fname)
+
+
+def trim_and_filter_events(events: List):
+    trimmed = []
+
+    for e in events:
+        e = re.sub(r"(=> )?\[CONTEXT\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[UNWIND\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[SAMPLE\].*?(?==>|$)", "", e)
+        e = e.strip()
+
+        trimmed.append(e)
+
+    trimmed_uniq = list(set(trimmed))
+    trimmed_uniq = [e for e in trimmed_uniq if e != ""]
+
+    events_mpi = [e for e in trimmed_uniq if "MPI" in e and "=>" not in e]
+    events_mpi = [e for e in trimmed_uniq if "MPI Collective Sync" in e]
+
+    events_mss = [e for e in trimmed_uniq if e.startswith(
+        "Multi") and "=>" in e]
+
+    events_driver = [
+        e
+        for e in trimmed_uniq
+        if e.startswith("Driver_Main") and "=>" in e and "Multi" not in e
+    ]
+
+    events_lb = [
+        e for e in events if e.startswith("LoadBalancingAndAdaptiveMeshRefinement =>")
+    ]
+
+    #  events_driver
+    #  events_lb
+
+    all_events = events_mpi + events_mss + events_driver + events_lb
+    all_events = [e for e in all_events if "MPI_" not in e]
+
+    all_events += [".TAU application"]
+    all_events
+
+    print(
+        f"Events timmed and dedup'ed. Before: {len(events)}. After: {len(all_events)}"
+    )
+
+    print("Events retained: ")
+    for e in all_events:
+        print(f"\t- {e}")
+
+    #  input("Press ENTER to plot.")
+
+    return all_events
+
+
+def get_top_events(df: pd.DataFrame, cutoff: float = 0.05):
+    total_runtime = df[df["name"] == ".TAU application"]["incl_usec"].iloc[0]
+
+    top_df = df[df["incl_usec"] >= total_runtime * cutoff]
+    top_names = top_df["name"].unique()
+
+    return trim_and_filter_events(top_names)
+
+
+def get_key_classification(keys: list[str]) -> dict[str, str]:
+    key_map = {}
+
+    for k in keys:
+        print(k)
+        if "send" in k.lower():
+            print("\t- Classified SEND")
+            key_map[k] = "send"
+        elif "receive" in k.lower():
+            print("\t- Classified RECV")
+            key_map[k] = "recv"
+        elif "loadbalancing" in k.lower():
+            print("\t- Classified LB")
+            key_map[k] = "lb"
+        elif ".tau application" in k.lower():
+            print("\t- Classified APP")
+            key_map[k] = "app"
+        elif "mpi" in k.lower():
+            print("\t- Classified SYNC")
+            key_map[k] = "sync"
+        else:
+            print("\t- Classified Compute")
+            key_map[k] = "comp"
+
+    return key_map
+
+
+def get_rankhour_comparison(trace_dirs: List[str], nranks):
+    # nranks used for normalizing into rank-hours
+    all_pprof_summ = []
+
+    for trace_dir in trace_dirs:
+        #  concat_df = read_all_pprof_simple(trace_dir)
+        #  pprof_data = filter_relevant_events(concat_df, stack_keys)
+        #  del concat_df
+        pprof_data = read_pprof_cached(trace_dir, [], nranks)
+        pprof_data = transform_pprof_cached(pprof_data)
+        key_map = get_key_classification(pprof_data.keys())
+
+        pprof_summ = {}
+        for k in pprof_data.keys():
+            k_map = key_map[k]
+            k_sum = pprof_data[k].sum()
+            if k_map in pprof_summ:
+                pprof_summ[k_map].append(k_sum)
+            else:
+                pprof_summ[k_map] = [k_sum]
+        all_pprof_summ.append(pprof_summ)
+
+    all_pprof_summ
+    all_pprof_summ[0]
+
+    # phase_data = list of {}s, key = phase, val = list of per-trace per-key vals
+    phase_data = []
+    for idx, t in enumerate(all_pprof_summ):
+        norm_phase_times = {}
+        for p in t:
+            norm_t = (np.array(t[p]) / (nranks * 1e6)).astype(int)
+            norm_phase_times[p] = norm_t
+        phase_data.append(norm_phase_times)
+
+    phases = ["app", "comp", "send", "recv", "sync", "lb"]
+    phase_events = {}
+    for phase in phases:
+        cur_phase_events = list(
+            [p for p in pprof_data.keys() if key_map[p] == phase])
+        phase_events[phase] = cur_phase_events
+
+    phase_data
+
+    # aggregate phase_data further
+    keys_aggr = set().union(*phase_data)
+    phase_data_aggr = {}
+    for k in keys_aggr:
+        v = [p[k].sum() for p in phase_data]
+        phase_data_aggr[k] = v
+
+    # compute "other" as "app" - rest
+    sum_vals = np.sum(np.array(list(phase_data_aggr.values())), axis=0)
+    phase_data_aggr["other"] = 2 * np.array(phase_data_aggr["app"]) - sum_vals
+
+    phase_data_aggr
+    phases.append("other")
+
+    phases = [ p for p in phases if p in phase_data_aggr.keys() ]
+    phases = [ p for p in phases if p in phase_events.keys() and len(phase_events[p]) > 0 ]
+
+    keys_to_del = [ k for k in phase_events if len(phase_events[k]) == 0 ]
+    for k in keys_to_del:
+        del phase_events[k]
+
+    return (phases, phase_events, phase_data_aggr)
+
+
+def get_fname(lst):
+    if not lst:
+        return "", []
+        pass
+
+    # Find the shortest string
+    shortest_str = min(lst, key=len)
+
+    # Find common prefix
+    common_prefix = ""
+    for i in range(len(shortest_str)):
+        if all(string[i] == shortest_str[i] for string in lst):
+            if shortest_str[i] not in string.ascii_letters:
+                break
+            common_prefix += shortest_str[i]
+        else:
+            break
+
+    # Find residuals
+    residuals = [string[len(common_prefix):] for string in lst]
+
+    res_str = "_".join(residuals)
+    fname = f"{common_prefix}_{res_str}"
+    print(f"File Name: {fname}")
+    return fname
+
+
+def plot_rankhour_comparison_simple(trace_names: List[str]) -> None:
+    n_traces = len(traces)
+    width = 0.45
+
+    keys_to_plot = ["comp", "send", "recv", "sync", "lb"]
+    keys_to_plot = ["comp", "sync", "send", "recv", "lb", "other"]
+    key_labels = ["Compute", "Global Barrier",
+                  "MPI Send", "MPI Recv", "LoadBalancing", "Other"]
+    bottom = np.zeros(n_traces)
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 6))
+
+    for idx, k in enumerate(keys_to_plot):
+        data_x = np.arange(n_traces)
+        data_y = phase_data_summ[k]
+
+        label = k[0].upper() + k[1:]
+        label = key_labels[idx]
+        ax.bar(data_x, data_y, bottom=bottom,
+               zorder=2, width=width, label=label)
+        bottom += data_y
+
+    p = ax.bar(
+        data_x,
+        phase_data_summ["other"],
+        bottom=bottom,
+        zorder=2,
+        width=width,
+        label="Other",
+        color="#999",
+    )
+
+    ax.bar_label(
+        p,
+        fmt="{:.0f} s",
+        rotation="horizontal",
+        label=phase_data_summ["app"],
+        fontsize=14,
+        padding=4,
+    )
+
+    ax.set_title(
+        "Runtime Evolution in Galaxy Sim (10k timesteps)", fontsize=18)
+    ax.set_xticks(data_x)
+    ax.set_xticklabels(trace_names)
+    ax.set_ylabel("Runtime (s)")
+
+    ax.yaxis.set_major_formatter(
+        ticker.FuncFormatter(lambda x, pos: "{:.0f} s".format(x))
+    )
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(2000))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(400))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 16000])
+
+    ax.legend(loc="upper right", ncol=3, fontsize=13)
+
+    fig.tight_layout()
+
+    plot_fname = "pprof_rh_simple"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+#  plot_rankhour_comparison_2(trace_names)
+
+
+def plot_rankhour_comparison(trace_names: list[str], trace_labels: list[str], all_phase_data) -> None:
+    hatches = ["/", "\\", "|", "-", "+", "x", "o", "O", ".", "*"]
+
+    width = 0.2
+
+    phases, phase_events, phase_data = all_phase_data
+
+    n_phases = np.arange(len(phases))
+    n_traces = len(trace_names)
+
+    # Reformatting the code with 4-space indentation
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    for i, trace in enumerate(trace_labels):
+        keys_trace = phases
+        vals_trace = [phase_data[p][i] for p in phases]
+        print(keys_trace)
+        print(vals_trace)
+
+        data_x = n_phases + i * width
+        print(data_x, vals_trace)
+        ax.bar(data_x, vals_trace, width, label=trace,
+               edgecolor="black", zorder=2)
+
+    #  for i, trace in enumerate(trace_names):
+        #  for j, phase in enumerate(phases):
+        #  values = phase_data[phase]
+        #  bottom_value = 0  # Initialize bottom_value for stacking
+        #  for k, value in enumerate(values):
+        #  label = trace if j == 0 and k == 0 else ""
+        #  ax.bar(
+        #  n_phases[j] + i * width,
+        #  value,
+        #  width,
+        #  label=label,
+        #  color=f"C{i}",
+        #  hatch=hatches[k % len(hatches)],
+        #  bottom=bottom_value,
+        #  edgecolor="black",
+        #  zorder=2,
+        #  )
+        #  bottom_value += value  # Update bottom_value for next bar in stack
+        #  break
+
+    # Labeling and layout
+    ax.set_xticks(n_phases + width * (n_phases - 1) / 2)
+    ax.set_xticklabels(phases)
+    ax.legend(title="Traces")
+
+    label_fname = get_fname(trace_names)
+    fname = f"pprof_rh_{label_fname}"
+
+    #  ax.yaxis.set_major_locator(ticker.MultipleLocator(200))
+    ax.yaxis.set_major_locator(ticker.AutoLocator())
+    #  ax.yaxis.set_minor_locator(ticker.MultipleLocator(50))
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    #  ax.set_ylim([0, 12000])
+
+    ymax = max(phase_data["app"])
+    ylim = int(np.ceil(ymax / 1000) * 1000)
+    ax.set_ylim([0, ylim])
+
+    ax.set_xlabel("App Phase")
+    ax.set_ylabel("Phase Time (s)")
+    ax.set_title("Phase-Wise Perf Breakdown (nranks=1536, ts=20k)")
+    fig.tight_layout()
+
+    PlotSaver.save(fig, "", None, fname)
+
+
+def run_plot_bar_simple():
+    global trace_dir_fmt
+
+    #  traces = ["profile40", "profile41"]
+    traces = ["stochsg10", "stochsg11", "stochsg12", "stochsg13"]
+    traces = ["stochsg10", "stochsg14", "stochsg15", "stochsg16"]
+    traces = ["stochsg17", "stochsg18", "stochsg19", "stochsg20"]
+    traces = ["stochsg21", "stochsg22", "stochsg23", "stochsg26"]
+    #  traces = ["stochsg28", "stochsg29", "stochsg30", "stochsg31"]
+    #  traces = ["stochsg32", "stochsg33", "stochsg34", "stochsg35"]
+    #  traces = ["stochsg36", "stochsg37", "stochsg38", "stochsg39"]
+    traces = ["stochsg40", "stochsg41", "stochsg42", "stochsg43"]
+    #  traces = ["stochsg44", "stochsg45", "stochsg46", "stochsg47"]
+    # traces = ["stochsg26"]
+    #  traces = ["stochsg7", "stochsg8", "stochsg9"]
+    trace_labels = ["Baseline", "LPT", "Contiguous-DP", "CDPP++"]
+    trace_labels = ["Baseline", "LPT", "Contiguous-DP", "CDPP++"]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+
+    nranks = 1536
+
+    # gen .pickle files, use the first time
+    preprocess_pprof_data(trace_dirs, nranks)
+    # return
+    all_phase_data = get_rankhour_comparison(trace_dirs, nranks)
+    all_phase_data
+    phase_mat = np.array([all_phase_data[2][k] for k in all_phase_data[1] if k in all_phase_data[2]])
+    print(phase_mat)
+    #  phase_mat[:, 3] - phase_mat[:, 1]
+    plot_rankhour_comparison(traces, trace_labels, all_phase_data)
+    #  all_phase_data = get_rankhour_comparison_new(trace_dirs)
+    #  pass
+
+
+def run_plot_bar():
+    global trace_dir_fmt
+
+    traces = ["athenapk5", "athenapk14", "athenapk15", "athenapk16"]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    trace_dirs
+    #  plot_rankhour_comparison(traces)
+    for idx in [0, 1]:
+        concat_df = read_all_pprof_simple(trace_dirs[idx])
+        concat_df_out = f"{trace_dirs[idx]}/pprof_concat.csv"
+        concat_df.to_csv(concat_df_out, index=False)
+
+    for k in phase_data[0]:
+        phase_data[0][k] = (phase_data[0][k] * 5).astype(int)
+        print(phase_data[0][k])
+
+    plot_rankhour_comparison(trace_names)
+    plot_rankhour_comparison_2(trace_names)
+
+
+def run_plot_stacked():
+    global trace_dir_fmt
+    nranks = 1536
+
+    trace_names = ["stochsg21", "stochsg22", "stochsg23", "stochsg26"]
+    trace_labels = ["Baseline", "LPT", "Contiguous-DP", "CDPP"]
+
+    trace_names = ["stochsg26"]
+    trace_labels = ["CDPP-NoLog"]
+
+    trace_names = ["stochsg28", "stochsg29", "stochsg30", "stochsg31"]
+    trace_labels = ["Baseline", "LPT", "Contiguous-DP", "CDPP"]
+
+    trace_names = ["stochsg32", "stochsg33", "stochsg34", "stochsg35"]
+    # trace_names = ["stochsg36", "stochsg37", "stochsg38", "stochsg39"]
+    # trace_names = ["stochsg40", "stochsg41", "stochsg42", "stochsg43"]
+    trace_names = ["stochsg44", "stochsg45", "stochsg46", "stochsg47"]
+
+    trace_label_prefix = "StochSG-n1024-"
+    trace_label_prefix = "StochSG-n1536-3k-"
+    # trace_label_prefix = "StochSG-n2048-5k-"
+    trace_label_prefix = "StochSG-n2048-20k-"
+    trace_labels = [f"{trace_label_prefix}{t}" for t in trace_labels]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), trace_names))
+
+    for trace_dir, trace_label in zip(trace_dirs, trace_labels):
+        plot_stacked_pprof(trace_dir, trace_label, nranks)
+    pass
+
+
+def run():
+    run_plot_bar_simple()
+    #  run_plot_stacked()
+
+
+if __name__ == "__main__":
+    #  global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    #  plot_init()
+    plot_init_big()
+    run()
diff --git a/scripts/tau_analysis/20240202_analyze_cdpp.py b/scripts/tau_analysis/20240202_analyze_cdpp.py
new file mode 100644
index 0000000..3b126f9
--- /dev/null
+++ b/scripts/tau_analysis/20240202_analyze_cdpp.py
@@ -0,0 +1,38 @@
+import pandas as pd
+
+from common import PlotSaver, plot_init_big
+from matplotlib import pyplot as plt
+
+
+def get_nblocks_mat(trace_dir: str):
+    df_path = f"{trace_dir}/prof.aggr.evt3.csv"
+    df = pd.read_csv(df_path)
+    aggr_df = df.groupby(["sub_ts", "rank"]).size().reset_index(name="nblocks")
+    mat = aggr_df.pivot(index="sub_ts", columns="rank", values="nblocks").to_numpy()
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 7))
+    im = ax.imshow(mat, aspect="auto", cmap="viridis")
+    ax.set_xlabel("Rank")
+    ax.set_ylabel("Timestep")
+    ax.set_title("Nblocks Per-rank Per-timestep")
+
+    fig.tight_layout()
+
+    fig.subplots_adjust(left=0.15, right=0.8)
+    cax = fig.add_axes([0.81, 0.12, 0.08, 0.8])
+    cbar = fig.colorbar(im, cax=cax)
+
+    plot_fname = "nblocks_mat_byts"
+    PlotSaver.save(fig, trace_dir, None, plot_fname)
+
+
+def run():
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    trace_name = "stochsg21"
+    trace_dir = trace_dir_fmt.format(trace_name)
+    get_nblocks_mat(trace_dir)
+
+
+if __name__ == "__main__":
+    plot_init_big()
+    run()
diff --git a/scripts/tau_analysis/20240312_parse_preload.py b/scripts/tau_analysis/20240312_parse_preload.py
new file mode 100644
index 0000000..9510c09
--- /dev/null
+++ b/scripts/tau_analysis/20240312_parse_preload.py
@@ -0,0 +1,233 @@
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+import numpy as np
+import pandas as pd
+
+from datetime import datetime
+from typing import TypedDict
+
+from io import StringIO
+from common import PlotSaver, plot_init_big
+
+
+class RunSuite(TypedDict):
+    nranks: int
+    trace_names: list[str]
+    log_files: list[str]
+
+
+class ParsedRunSuite(TypedDict):
+    nranks: int
+    trace_names: list[str]
+    section_prof: list[pd.DataFrame]
+    section_comm: list[pd.DataFrame]
+
+
+def get_today() -> str:
+    return datetime.now().strftime("%Y%m%d")
+
+
+def parse_log_section(section: list[str]) -> pd.DataFrame:
+    section = [l.strip('\n') for l in section if l.strip() !=
+               "" and not l.strip().startswith("----")]
+    data = StringIO('\n'.join(section))
+    df = pd.read_fwf(data)
+    df.columns = list(map(lambda x: x.strip(':'), df.columns))
+    for c in df.columns:
+        # if c is a series of type str
+        if df[c].dtype == 'object':
+            df[c] = df[c].str.strip(':')
+    return df
+
+
+def parse_log(log_file: str) -> tuple[pd.DataFrame, pd.DataFrame]:
+    with open(log_file, 'r') as f:
+        lines = f.readlines()
+        pass
+
+    dashes = [lidx for (lidx, l) in enumerate(lines) if l.startswith("----")]
+    section_prof = parse_log_section(lines[dashes[0] - 1:dashes[1] - 1])
+    section_comm = parse_log_section(lines[dashes[-1] - 1:])
+
+    return section_prof, section_comm
+
+
+def parse_suite(suite: RunSuite) -> ParsedRunSuite:
+    parsed_logs = []
+    for log_file in suite["log_files"]:
+        parsed_logs.append(parse_log(log_file))
+
+    section_prof, section_comm = zip(*parsed_logs)
+    parsed_suite = ParsedRunSuite(
+        nranks=suite["nranks"],
+        trace_names=suite["trace_names"],
+        section_prof=section_prof,
+        section_comm=section_comm
+    )
+
+    return parsed_suite
+
+
+def classify_prof_phases(nranks: int, df: pd.DataFrame) -> dict[str, int]:
+    metric_map = {
+        "Driver_Main": "app",
+        "UpdateMeshBlockTree": "sync",
+        "RedistributeAndRefineMeshBlocks": "lb",
+        "Task_LoadAndSendBoundBufs": "comm",
+        "Task_ReceiveBoundBufs": "comm",
+        "Task_DoLotsOfWork": "comp",
+        "Task_FillDerived": "comp"}
+
+    phase_times: dict[str, int] = {}
+
+    df_slice: pd.DataFrame = pd.DataFrame(df[['Metric', 'Count', 'Avg']].copy())
+    df_slice['Time'] = df_slice['Avg'] * df_slice['Count'] / nranks
+    df_slice['Time'] = df_slice['Time'].astype(int)
+
+    for _, row in df_slice.iterrows():
+        metric: str = str(row["Metric"])
+        time: int = int(row["Time"])
+
+        if metric not in metric_map:
+            continue
+
+        phase = metric_map[metric]
+        if phase not in phase_times:
+            phase_times[phase] = 0
+
+        phase_times[phase] += int(time / 1e6)
+
+    return phase_times
+
+
+def plot_prof(parsed_suite: ParsedRunSuite, prof_phases: list[str], prof_data: list[dict[str, int]]):
+    nranks = parsed_suite["nranks"]
+    nphases = len(prof_phases)
+    nphases_x = np.arange(nphases)
+    traces = parsed_suite["trace_names"]
+
+    width = 0.2
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+
+    for i, trace in enumerate(traces):
+        trace_prof_data = prof_data[i]
+        trace_prof_data = [trace_prof_data[p] for p in prof_phases]
+
+        data_x = nphases_x + i * width
+        ax.bar(data_x, trace_prof_data, width,
+               label=trace, edgecolor="black", zorder=2)
+
+    ax.set_xticks(nphases_x + width * (len(traces) - 1) / 2)
+    ax.set_xticklabels(prof_phases)
+    ax.legend(title="Traces")
+
+    ax.yaxis.set_major_locator(ticker.AutoLocator())
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+
+    ymax = max([x["app"] for x in prof_data])
+    ylim = int(np.ceil(ymax / 1000) * 1000)
+    ax.set_ylim(bottom=0, top=ylim)
+
+    ax.set_xlabel("App Phase")
+    ax.set_ylabel("Phase Time (s)")
+    ax.set_title(f"Phase-Wise Perf Breakdown (nranks={nranks})")
+    fig.tight_layout()
+
+    today = get_today()
+    fname = f"{today}_prof_phases_nranks{nranks}"
+    PlotSaver.save(fig, "", None, fname)
+
+
+def convert_size_to_gb(size: str) -> float:
+    size = size.strip()
+    size_bytes = 0
+
+    if size.endswith("GiB"):
+        size_bytes = float(size[:-3]) * 1024 * 1024 * 1024
+    elif size.endswith("MiB"):
+        size_bytes = float(size[:-3]) * 1024 * 1024
+    elif size.endswith("KiB"):
+        size_bytes = float(size[:-3]) * 1024
+    elif size.endswith("B"):
+        size_bytes = float(size[:-1])
+    else:
+        raise ValueError(f"Invalid size format: {size}")
+
+    size_gb = size_bytes / 2**30
+    return size_gb
+
+
+def plot_comm(suite: ParsedRunSuite):
+    comm_dfs = [df for df in suite["section_comm"]]
+    comm_local = [df.iloc[1]['Local'] for df in comm_dfs]
+    comm_total = [df.iloc[1]['Global'] for df in comm_dfs]
+
+    comm_local = np.array([convert_size_to_gb(s) for s in comm_local])
+    comm_total = np.array([convert_size_to_gb(s) for s in comm_total])
+
+    data_x = np.arange(len(comm_local))
+    width = 0.3
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+    ax.bar(data_x, comm_local, width, label="Local",
+           edgecolor="black", zorder=2)
+    ax.bar(data_x + width, comm_total, width,
+           label="Total", edgecolor="black", zorder=2)
+
+    ax.yaxis.set_major_locator(ticker.AutoLocator())
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+
+    ymax = max(comm_total)
+    ylim = int(np.ceil(ymax / 100) * 100)
+    ax.set_ylim(bottom=0, top=ylim)
+
+    ax.set_xticks(data_x + width / 2)
+    ax.set_xticklabels(suite["trace_names"])
+    ax.legend(title="Comm Type")
+
+    ax.set_xlabel("Trace")
+    ax.set_ylabel("Data Exchange (GiB)")
+    ax.set_title(f"P2P Comm Volume (nranks={suite['nranks']})")
+
+    nranks = suite["nranks"]
+
+    today = get_today()
+    fname = f"{today}_comm_size_nranks{nranks}"
+    PlotSaver.save(fig, "", None, fname)
+
+
+def run_parse_suite():
+    nranks = 512
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    traces = ["stochsg52", "stochsg53", "stochsg54", "stochsg55"]
+    policies = ["Baseline", "LPT", "Contiguous-DP", "CDPP"]
+    log_files = [trace_dir_fmt.format(t) + "/run/log.txt" for t in traces]
+
+    suite = RunSuite(nranks=nranks, trace_names=policies, log_files=log_files)
+    parsed_suite = parse_suite(suite)
+
+    prof_phases = ["app", "comp", "comm", "sync", "lb"]
+    prof_data = [classify_prof_phases(nranks, df)
+                 for df in parsed_suite["section_prof"]]
+
+    plot_prof(parsed_suite, prof_phases, prof_data)
+    plot_comm(parsed_suite)
+
+
+def run():
+    plot_init_big()
+    run_parse_suite()
+
+
+if __name__ == '__main__':
+    run()
diff --git a/scripts/tau_analysis/20240326_blastwave_hist.py b/scripts/tau_analysis/20240326_blastwave_hist.py
new file mode 100644
index 0000000..10ad179
--- /dev/null
+++ b/scripts/tau_analysis/20240326_blastwave_hist.py
@@ -0,0 +1,281 @@
+import matplotlib.pyplot as plt 
+from matplotlib import ticker
+
+import re
+import glob
+import os
+import yt
+import numpy as np
+import multiprocessing
+import sys
+
+
+def plot_frame(file_path: str, plt_dir: str, plot_vars: list[tuple]) -> None:
+    if not os.path.exists(file_path):
+        return
+
+    file_name = file_path.split("/")[-1]
+    file_name = file_name.replace(".phdf", "")
+    print(f"Plotting {file_name}")
+
+    dataset = yt.load(file_path)
+    for v in plot_vars:
+        s = yt.SlicePlot(dataset, "z", v)
+        s.annotate_grids()
+        #  s.annotate_cell_edges()
+
+        plt_fname = f"{file_name}_{v[1]}"
+        plt_fpath = f"{plt_dir}/{plt_fname}.png"
+        s.save(plt_fpath)
+
+
+def get_key_id(key: str) -> int:
+    key_id = int(re.findall(r'00\d+', key)[0])
+    return key_id
+
+
+def get_timestep(key: str) -> int:
+    key_id = get_key_id(key)
+    timestep = 300 * key_id
+    return timestep
+
+
+def plot_frame_wrapper(args):
+    plot_frame(*args)
+
+
+def get_all_phdf(dir_path: str) -> list[str]:
+    all_files = glob.glob(dir_path + "/*.phdf")
+    return all_files
+
+
+def get_hist(file_path: str, plot_path: str, var: tuple) -> None:
+    dataset = yt.load(file_path)
+    data = dataset.all_data()
+    data_var = data[var]
+    dmin, dmax = np.min(data_var), np.max(data_var)
+    print(f"Var: {var}, Min: {dmin}, Max: {dmax}")
+
+    # get 1000 bins between 0 and 1
+    # dmin, dmax = 0, 0.001
+    # bins = np.linspace(dmin, dmax, 1000)
+    # hist = np.histogram(data_var, bins=bins)
+    bins, hist = np.histogram(data_var, bins=10000)
+    bins / sum(bins)
+    # get non-zero bin vals
+    norm_bins = bins / sum(bins)
+    norm_bins = norm_bins[norm_bins > 0.01]
+    print(len(norm_bins))
+    print(norm_bins)
+    # lum(bins)
+    file_path
+    bins
+    bins[1:30]
+    hist
+    sum(bins[1:])
+
+
+
+    # plot histogram
+    fig, ax = plt.subplots(1, 1)
+    ax.plot(hist[:-1], bins, label=var[1])
+    ax.set_xscale("log")
+    ax.set_ylim(bottom=0)
+    fig.tight_layout()
+
+    file_name = file_path.split("/")[-1]
+    file_name = file_name.replace(".phdf", "")
+    plt_fname = f"{file_name}_{var[1]}_hist"
+    plt_fpath = f"{plot_path}/{plt_fname}.png"
+
+    fig.savefig(plt_fpath, dpi=300)
+
+
+def plot_hist(data: dict[str, np.ndarray], ptile: float, plot_fpath: str) -> None:
+    fig, ax = plt.subplots(1, 1)
+    for k, v in data.items():
+        # ax.hist(v, bins=1000, density=True, histtype="step", label=k)
+        ax.ecdf(v, label=k)
+
+    ax.set_ylim(bottom=0)
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+    ax.grid(which='major', color='#bbb')
+    ax.grid(which='minor', color='#ddd')
+    ax.legend()
+
+    ax.set_xlabel("Energy")
+    ax.set_ylabel("Percentile")
+    ax.set_title(f"CDF (Percentile: {ptile:.0f})")
+    fig.tight_layout()
+
+    print(f"Writing to {plot_fpath}")
+    fig.savefig(plot_fpath, dpi=300)
+
+
+def plot_hist_2(data: dict[str, np.ndarray], ptile: float, plot_fpath: str) -> None:
+    fig, ax = plt.subplots(1, 1)
+    # bins = np.linspace(0, 3, 30)
+    bins = [0, 0.1, 0.5, 3.0]
+
+    for k, v in data.items():
+        hist, _ = np.histogram(v, bins=bins)
+        # ax.hist(v, bins=25, density=False, histtype="step", label=k)
+        hist += 1
+        print(hist)
+        ax.plot(bins[:-1], hist, label=k)
+
+    # ax.set_ylim(bottom=0)
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+    ax.grid(which='major', color='#bbb')
+    ax.grid(which='minor', color='#ddd')
+    ax.legend()
+
+    ax.set_yscale('log')
+
+    ax.set_xlabel("Energy")
+    ax.set_ylabel("Percentile")
+    ax.set_title(f"CDF (Percentile: {ptile:.0f})")
+    fig.tight_layout()
+
+    print(f"Writing to {plot_fpath}")
+    fig.savefig(plot_fpath, dpi=300)
+
+
+def get_fnames_to_plot(exp_name: str) -> list[str]:
+    dir_path = f"/mnt/ltio/parthenon-topo/{exp_name}/run"
+    all_files = glob.glob(dir_path + "/*.phdf")
+
+    fnames = [f.split("/")[-1] for f in all_files]
+    fids = [int(f.split(".")[-2]) for f in fnames if 'final' not in f]
+
+    fids_to_plot = list(range(0, 100, 15))
+    fnames_to_plot = [(f, fid) for f, fid in zip(all_files, fids) if fid in fids_to_plot]
+    fnames_to_plot = sorted(fnames_to_plot, key=lambda x: x[1])
+    fnames_to_plot = [f[0] for f in fnames_to_plot]
+
+    return fnames_to_plot
+
+def get_all_data(fnames: list[str], var: tuple) -> dict[str, np.ndarray]:
+    all_data = {}
+    for f in fnames:
+        dataset = yt.load(f)
+        data = dataset.all_data()
+        data_var = data[var]
+        fname = os.path.basename(f).replace(".phdf", "")
+        all_data[fname] = data_var
+
+    return all_data
+
+def get_topk_data(all_data: dict[str, np.ndarray], ptile: float) -> dict[str, np.ndarray]:
+    ptile_data = {}
+    for k, v in all_data.items():
+        varray = np.array(v)
+        cutoff = np.percentile(varray, ptile)
+        ptile_data[k] = np.array(varray[varray > cutoff])
+
+    return ptile_data
+
+def write_hist_to_file(all_data: dict[str, np.ndarray], dir_out: str) -> None:
+    os.makedirs(dir_out, exist_ok=True)
+
+    bins = np.linspace(0, 3, 3000)
+    np.savetxt(f"{dir_out}/bins.txt", bins)
+
+    for k, v in all_data.items():
+        hist, _ = np.histogram(v, bins=bins)
+        hist = hist / sum(hist)
+        # hist = hist[hist > 0.01]
+
+        hist_fpath = f"{dir_out}/{k}_hist.txt"
+        np.savetxt(hist_fpath, hist)
+
+
+def plot_wide_bins(data: dict[str, np.ndarray], plot_fpath: str) -> None:
+    keys = list(data.keys())
+    keys = sorted(keys, key=get_key_id)
+    timesteps = list(map(get_timestep, keys))
+
+    bins = [0, 0.1, 0.5, 3.0]
+    all_hists = []
+    for k in keys:
+        hist, _ = np.histogram(data[k], bins=bins)
+        hist = hist / sum(hist)
+        all_hists.append(hist)
+
+    all_hists = np.array(all_hists)
+    all_hists = all_hists.T
+
+    timesteps = list(map(get_timestep, keys))
+
+    fig, ax = plt.subplots(1, 1)
+    ax.plot(timesteps, all_hists[0], label="low energy (0-0.1)")
+    ax.plot(timesteps, all_hists[1], label="mid energy (0.1-0.5)")
+    ax.plot(timesteps, all_hists[2], label="high energy (0.5-3.0)")
+
+    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
+    ax.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1))
+    ax.grid(which='major', color='#bbb')
+    ax.grid(which='minor', color='#ddd')
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("% of cells in energy state")
+    ax.set_title("Energy distribution of Sedov Blast Wave over time")
+
+    ax.set_ylim(bottom=0)
+    ax.legend()
+    fig.tight_layout()
+
+    fig.savefig(plot_fpath, dpi=300)
+
+
+def run_cdf(exp_name):
+    fnames = get_fnames_to_plot(exp_name)
+    print(f"Found {len(fnames)} files to plot")
+
+    var = ("parthenon", "c.energy")
+    all_data = get_all_data(fnames, var)
+
+    plot_dir = f"/users/ankushj/repos/amr/scripts/tau_analysis/figures/20240326"
+    plot_path = f"{plot_dir}/{exp_name}/hists"
+
+    write_hist_to_file(all_data, plot_path)
+
+    plot_fname = f"{plot_path}/{exp_name}_wide_bins.png"
+    plot_wide_bins(all_data, plot_fname)
+    return
+
+    os.makedirs(plot_path, exist_ok=True)
+
+    ptile = 95
+    ptile_data = get_topk_data(all_data, ptile)
+    plot_fname = f"{plot_path}/{exp_name}_cdf_{ptile}.png"
+    plot_hist(ptile_data, ptile, plot_fname)
+
+
+    ptile = 90
+    ptile_data = get_topk_data(all_data, ptile)
+    plot_fname = f"{plot_path}/{exp_name}_cdf_{ptile}.png"
+    plot_hist(ptile_data, ptile, plot_fname)
+
+    ptile = 50
+    ptile_data = get_topk_data(all_data, ptile)
+    plot_fname = f"{plot_path}/{exp_name}_cdf_{ptile}.png"
+    plot_hist(ptile_data, ptile, plot_fname)
+
+    plot_fname = f"{plot_path}/{exp_name}_hist_{ptile}.png"
+    plot_hist_2(ptile_data, ptile, plot_fname)
+
+    ptile = 0
+    plot_fname = f"{plot_path}/{exp_name}_cdf_{ptile}.png"
+    plot_hist(all_data, ptile, plot_fname)
+
+    plot_fname = f"{plot_path}/{exp_name}_hist_{ptile}.png"
+    plot_hist_2(all_data, ptile, plot_fname)
+
+
+def run():
+    exp_name = "blastwave01"
+    run_cdf(exp_name)
+
+if __name__ == "__main__":
+    run()
diff --git a/scripts/tau_analysis/analyze_dynamic_blocks.py b/scripts/tau_analysis/analyze_dynamic_blocks.py
new file mode 100644
index 0000000..d467220
--- /dev/null
+++ b/scripts/tau_analysis/analyze_dynamic_blocks.py
@@ -0,0 +1,324 @@
+import glob
+import multiprocessing
+import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+import matplotlib.colors as colors
+import pickle
+import re
+import subprocess
+import struct
+import sys
+import time
+import os
+
+from common import plot_init, PlotSaver, prof_evt_map
+from matplotlib.ticker import FuncFormatter, MultipleLocator
+from typing import List, Tuple
+from pathlib import Path
+from trace_reader import TraceReader, TraceOps
+
+
+def get_prof_path(trace_dir: str, evt: int) -> str:
+    # replace merged with agg if single self-contained run
+    ppath = f"{trace_dir}/prof.merged.evt{evt}.csv"
+    ppath = f"{trace_dir}/prof.aggr.evt{evt}.csv"
+    return ppath
+
+
+def make_uniform(obj_2d):
+    lens = list(map(lambda x: x.shape[0], obj_2d))
+    max_len = max(lens)
+    padded_1d = [np.pad(arr, (0, max_len - len(arr)), "constant") for arr in obj_2d]
+    mat = np.stack(padded_1d)
+
+    print(f"Mat shape: {mat.shape}")
+
+    return mat
+
+
+def get_evt_mat(evt):
+    clip = False
+
+    global trace_dir
+    df_path = get_prof_path(trace_dir, evt)
+    print(f"Reading dataframe: {df_path}")
+    df = pd.read_csv(df_path)
+
+    df_agg = df.groupby(["sub_ts", "block_id"], as_index=False).agg(
+        {"rank": "min", "time_us": ["sum", "count"]}
+    )
+
+    df_agg.columns = list(map(lambda x: "_".join(x).strip("_"), df_agg.columns))
+
+    df_agg2 = df_agg.groupby("sub_ts", as_index=False).agg(
+        {"time_us_sum": list, "time_us_count": list}
+    )
+
+    if clip:
+        match_low = df_agg2["sub_ts"] >= 1155
+        match_hi = df_agg2["sub_ts"] <= 6270
+        df_clip = df_agg2[match_low & match_hi].copy()
+    else:
+        df_clip = df_agg2
+
+    mat_sum = df_clip["time_us_sum"].apply(np.array).to_numpy()
+    mat_sum = make_uniform(mat_sum)
+
+    mat_count = df_clip["time_us_count"].apply(np.array).to_numpy()
+    mat_count = make_uniform(mat_count)
+
+    return mat_sum, mat_count
+
+
+def get_regr_slopes(mat):
+    mat = mat.T
+    m, n = mat.shape
+    X = np.vstack([np.arange(n)] * m)
+
+    # slope = N sum(xy) - sum(x)sum(y) / nsum(x^2) - sum(x)^2
+    den = n * (X**2).sum(axis=1) - X.sum(axis=1) ** 2
+    num1 = n * np.sum(np.multiply(mat, X), axis=1)
+    num2 = np.multiply(X.sum(axis=1), mat.sum(axis=1))
+
+    slopes = (num1 - num2) / den
+    indices = np.argsort(slopes)[::-1]
+    slopes_sorted = slopes[indices]
+    slopes_tuple = list(zip(slopes_sorted, indices))
+    return slopes_tuple
+
+
+def plot_blocks(ax, mat, block_idxes, key):
+    global trace_dir
+
+    data_x = np.arange(mat.shape[0]) + 1155
+
+    for idx in block_idxes:
+        data_y = mat[:, idx]
+        ax.plot(data_x, data_y, "o", label=f"b{idx}", ms=1)
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (ms)")
+    ax.set_title(f"({key})")
+
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} ms".format(x / 1e3))
+    ax.set_ylim([0, 40000])
+    ax.yaxis.set_major_locator(MultipleLocator(6000))
+    ax.yaxis.set_minor_locator(MultipleLocator(1500))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb")
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd")
+
+    #  ax.legend(ncol=4)
+
+
+def plot_blocks_evt(mat, evt):
+    slopes = get_regr_slopes(mat)
+
+    idxes = list(map(lambda x: x[1], slopes))
+    idxes_top8 = idxes[:8]
+    idxes_top100 = idxes[:100:13]
+    idxes_all = idxes[::121]
+
+    fig = plt.figure(figsize=(9, 5))
+    axes = fig.subplots(1, 3)
+
+    plot_blocks(axes[0], mat, idxes_top8, "Top 8")
+    plot_blocks(axes[1], mat, idxes_top100, "Top 100")
+    plot_blocks(axes[2], mat, idxes_all, "All")
+
+    fig.suptitle(f"Selected Blocks vs Selected Timeslice (Evt {evt})", fontsize=18)
+    fig.tight_layout()
+
+    plot_fname = f"interesting_blocks.evt{evt}"
+    PlotSaver.save(fig, trace_dir, None, plot_fname)
+
+
+def plot_blocks_imshow(mat, evt, evt_label, vmin, vmax):
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    #  norm = colors.LogNorm(vmin=15000, vmax=30000)
+    #  norm = colors.LogNorm(vmin=vmin, vmax=vmax)
+    bounds = np.linspace(vmin, vmax, 256)
+    bounds = np.linspace(vmin, vmax, 64)
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="max")
+
+    im = ax.imshow(mat, norm=norm, aspect="auto", cmap="plasma")
+    #  im = ax.imshow(mat, aspect="auto", cmap="plasma")
+    #  evts = {0: "FillDerived", 1: "CalculateFluxes"}
+    #  evt_label = evts[evt]
+    ax.set_title(f"Block ID vs Time (Evt {evt_label})")
+    ax.set_xlabel("Block ID")
+    ax.set_ylabel("Timestep")
+
+    #  ax.yaxis.set_major_formatter(lambda x, pos: f"{int(x + 1155)}")
+
+    fig.tight_layout()
+
+    fig.subplots_adjust(left=0.15, right=0.78)
+    cax = fig.add_axes([0.81, 0.12, 0.08, 0.8])
+    cax_fmt = lambda x, pos: "{:.0f} ms".format(x / 1e3)
+    #  cax.yaxis.set_major_formatter(FuncFormatter(cax_fmt))
+    #  cax.xaxis.set_major_formatter(FuncFormatter(cax_fmt))
+
+    #  cbar_ticks = np.arange(15, 34, 5) * 1e3
+    #  cbar_labels = list(map(lambda x: cax_fmt(x, None), cbar_ticks))
+    cbar = fig.colorbar(im, cax=cax, format=FuncFormatter(cax_fmt))
+    #  cbar = fig.colorbar(im, cax=cax, format="{x:.0f} ms")
+    #  cbar = fig.colorbar(im, format=FuncFormatter(cax_fmt))
+    #  cbar.ax.ticklabel_format(style='plain')
+    #  cbar.ax.yaxis.set_major_formatter(FuncFormatter(cax_fmt))
+    #  cbar.ax.xaxis.set_major_formatter(FuncFormatter(cax_fmt))
+    #  cax.yaxis.set_major_formatter(FuncFormatter(cax_fmt))
+    #  cax.xaxis.set_major_formatter(FuncFormatter(cax_fmt))
+    #  cbar = fig.colorbar(im, cax=cax)
+    #  cbar.ax.set_yticks(cbar_ticks)
+    #  cbar.ax.set_yticklabels(cbar_labels)
+
+    global trace_dir
+    fname = f"blockmat.sums.evt{evt}"
+    PlotSaver.save(fig, trace_dir, None, fname)
+
+
+def plot_blocks_imshow_v2(mat, evt, evt_label, vmin, vmax):
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    #  bounds = np.linspace(vmin, vmax, 256)
+    bounds = np.linspace(vmin, vmax, 64)
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="max")
+
+    im = ax.imshow(mat, norm=norm, aspect="auto", cmap="plasma")
+    #  ax.set_title(f"Block ID vs Time (Evt {evt_label})")
+    ax.set_xlabel("Timestep", fontsize=15)
+    ax.set_ylabel("Block ID", fontsize=15)
+
+    ax.tick_params(axis="both", labelsize=13)
+
+    tick_max = mat.shape[0]
+    ax.xaxis.set_major_formatter(lambda x, pos: f"{int(x/1e3)}K")
+    ax.yaxis.set_major_formatter(lambda x, pos: f"{int(tick_max - x)}")
+
+    fig.tight_layout()
+
+    left_bound = 0.15
+    right_bound = 0.72
+    fig.subplots_adjust(left=left_bound, right=right_bound)
+    cax = fig.add_axes([right_bound + 0.03, 0.17, 0.07, 0.76])
+    cax.tick_params(axis="both", labelsize=13)
+    cax_fmt = lambda x, pos: "{:.0f} ms".format(x / 1e3)
+    cbar = fig.colorbar(im, cax=cax, format=FuncFormatter(cax_fmt))
+    cax.set_ylabel("Kernel Invocation Time (ms)", fontsize=16)
+    global trace_dir
+    fname = f"blockmat.sums.evt{evt}.v2"
+    PlotSaver.save(fig, trace_dir, None, fname)
+    pass
+
+
+def plot_counts_imshow(mat, evt):
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    norm = colors.LogNorm(vmin=15000, vmax=30000)
+    bounds = np.arange(0, 6)
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="max")
+
+    im = ax.imshow(mat, norm=norm, aspect="auto", cmap="plasma")
+    evts = {0: "FillDerived", 1: "CalculateFluxes"}
+    evt_label = evts[evt]
+    ax.set_title(f"Block ID vs InvokeCount-Per-TS (Evt {evt_label})")
+    ax.set_xlabel("Block ID")
+    ax.set_ylabel("Timestep")
+
+    ax.yaxis.set_major_formatter(lambda x, pos: f"{int(x + 1155)}")
+
+    fig.tight_layout()
+
+    fig.subplots_adjust(left=0.15, right=0.78)
+    cax = fig.add_axes([0.81, 0.12, 0.08, 0.8])
+
+    #  cax_fmt = lambda x, pos: "{:.0f} ms".format(x / 1e3)
+    #  cbar_ticks = np.arange(15, 34, 5) * 1e3
+    #  cbar_labels = list(map(lambda x: cax_fmt(x, None), cbar_ticks))
+    #  cbar = fig.colorbar(im, cax=cax, format=FuncFormatter(cax_fmt))
+    cbar = fig.colorbar(im, cax=cax)
+    #  cbar.ax.set_yticks(cbar_ticks)
+    #  cbar.ax.set_yticklabels(cbar_labels)
+
+    global trace_dir
+    fname = f"blockmat.counts.evt{evt}"
+    PlotSaver.save(fig, trace_dir, None, fname)
+
+
+def plot_mat_experiments():
+    mat0_sum, mat0_count = get_evt_mat(0)
+    ms = mat0_sum.copy()
+    ms = ms.astype(float)
+    ms[ms == 0] = np.nan
+
+    vmin = np.nanpercentile(ms, 2)
+    vmax = np.nanpercentile(ms, 98)
+
+    plot_blocks_imshow(mat0_sum, 0, "FillDerived", 15000, 25000)
+
+    plot_blocks_imshow(ms, 0, "FillDerived", vmin, vmax)
+    ms
+    ms.T.shape
+    rot_ms = ms.T[::-1]
+    plot_blocks_imshow_v2(rot_ms, 0, "FillDerived", vmin, vmax)
+    pass
+
+
+def get_interesting_blocks():
+    mat0_sum, mat0_count = get_evt_mat(0)
+    #  plot_blocks_evt(mat0_sum, 0)
+    plot_blocks_imshow(mat0_sum, 0)
+
+    plot_counts_imshow(mat0_count, 0)
+
+    mat1_sum, mat1_count = get_evt_mat(1)
+    #  plot_blocks_evt(mat1_sum, 1)
+    plot_blocks_imshow(mat1_sum, 1)
+    plot_counts_imshow(mat1_count, 1)
+
+
+def get_interesting_blocks_2():
+    evts = [0, 1, 5, 6]
+    evt_labels = ["FD", "CF", "SNF", "TabCool"]
+
+    evts = evts[:2]
+    evt_labels = evt_labels[:2]
+
+    evt_tuples = list(map(get_evt_mat, evts))
+    mats = list(map(lambda x: x[0], evt_tuples))
+    mat_sum = np.sum(mats, axis=0)
+
+    for mat, evt, label in zip(mats, evts, evt_labels):
+        vmin = np.percentile(mat, 2)
+        vmax = np.percentile(mat, 98)
+        print(f"Evt: {evt}, Vmin: {vmin}, Vmax: {vmax}")
+        plot_blocks_imshow(mat, evt, label, vmin, vmax)
+
+    vmin = np.percentile(mat_sum, 1)
+    vmax = np.percentile(mat_sum, 99)
+    plot_blocks_imshow(mat_sum, "0156", "COMP_ALL", vmin, vmax)
+    pass
+
+
+def run():
+    plot_init()
+    get_interesting_blocks_2()
+
+
+if __name__ == "__main__":
+    global trace_dir_fmt
+    global trace_dir
+
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/profile{}"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile40"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile37"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile39"
+    trace_dir = "/mnt/ltio/parthenon-topo/burgers2"
+    trace_dir = "/mnt/ltio/parthenon-topo/athenapk1"
+    trace_dir = "/mnt/ltio/parthenon-topo/stochsg2"
+    run()
diff --git a/scripts/tau_analysis/analyze_msgs.py b/scripts/tau_analysis/analyze_msgs.py
index ca7e4d7..d2d80af 100644
--- a/scripts/tau_analysis/analyze_msgs.py
+++ b/scripts/tau_analysis/analyze_msgs.py
@@ -1,24 +1,55 @@
 import glob
+import os
+
+import matplotlib
 import matplotlib.pyplot as plt
+import matplotlib.colors as colors
 import multiprocessing
 import numpy as np
 import pandas as pd
 import pickle
+import re
 import subprocess
+import struct
 import sys
 import time
 
-from memory_profiler import profile
+#  from memory_profiler import profile
 
 import ray
 import traceback
-from typing import Tuple
+
+from matplotlib.ticker import FuncFormatter, MultipleLocator
+from pandas import DataFrame
+
+from sklearn import linear_model
+from typing import Dict, Tuple
 
 from trace_reader import TraceOps
 
 from task import Task
 
-ray.init(address="h0:6379")
+from analyze_pprof import (
+    setup_plot_stacked_generic,
+    read_all_pprof_simple,
+    filter_relevant_events,
+)
+
+
+#  ray.init(address="h0:6379")
+trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+
+
+def setup_interactive():
+    matplotlib.use("abc")
+    matplotlib.use("TkAgg")
+    matplotlib.use("WebAgg")
+    matplotlib.use("GTK3Agg")
+    plt.ion()
+    fig, ax = plt.subplots(1, 1)
+    dx = np.arange(5)
+    dy = np.arange(5, 0, -1)
+    ax.plot(dx, dy)
 
 
 class MsgAggrTask(Task):
@@ -72,7 +103,7 @@ def aggr_msgs(fn_args):
     rank = fn_args["rank"]
 
     rank_msgcsv = "{}/trace/msgs.rcv.{}.csv".format(trace_dir, rank)
-    print(rank_msgcsv)
+    print(f"Reading {rank_msgcsv}")
 
     df = pd.read_csv(rank_msgcsv, sep="|")
     #  print(df)
@@ -88,6 +119,223 @@ def aggr_msgs(fn_args):
     df = None
 
 
+def read_msgs(fpath):
+    rank = int(re.search(r"msgs.(\d+).bin$", fpath).groups(0)[0])
+    msgbin_data = open(fpath, "rb").read()
+
+    print(f"Read messages: {rank}: {fpath}")
+
+    # ptr, blk_id, blk_rank, nbr_id, nbr_rank, tag, is_flx
+    chan_sz = 29
+    chan_fmt = "@Piiiiic"
+    chan_fmtc = struct.Struct(chan_fmt)
+    assert chan_sz == struct.calcsize(chan_fmt)
+
+    # tag, dest, sz, ts
+    # ptr, bufsz, recv_rank, tag, timestamp
+    send_sz = 28
+    # can't use P with =, can't use @ because padding issues
+    send_fmt = "=QiiiQ"
+    send_fmtc = struct.Struct(send_fmt)
+    assert send_sz == struct.calcsize(send_fmt)
+
+    all_ts_data = []
+
+    ptr = 0
+    while ptr < len(msgbin_data):
+        (ts,) = struct.unpack("@i", msgbin_data[ptr : ptr + 4])
+        ptr += 4
+
+        (chanbuf_sz,) = struct.unpack("@i", msgbin_data[ptr : ptr + 4])
+        ptr += 4
+
+        chan_recs = list(chan_fmtc.iter_unpack(msgbin_data[ptr : ptr + chanbuf_sz]))
+        ptr += chanbuf_sz
+
+        (sendbuf_sz,) = struct.unpack("@i", msgbin_data[ptr : ptr + 4])
+        ptr += 4
+
+        send_recs = list(send_fmtc.iter_unpack(msgbin_data[ptr : ptr + sendbuf_sz]))
+        ptr += sendbuf_sz
+
+        all_ts_data.append((ts, chan_recs, send_recs))
+
+    chan_cols = ["ptr", "blk_id", "blk_rank", "nbr_id", "nbr_rank", "tag", "isflx"]
+    send_cols = ["ptr", "msgsz", "Dest", "tag", "timestamp"]
+
+    all_chan_df = []
+    all_send_df = []
+
+    for tup in all_ts_data:
+        ts, chan_recs, send_recs = tup
+        chan_df = pd.DataFrame.from_records(chan_recs, columns=chan_cols)
+        chan_df["ts"] = ts
+
+        send_df = pd.DataFrame.from_records(send_recs, columns=send_cols)
+        send_df["ts"] = ts
+
+        all_chan_df.append(chan_df)
+        all_send_df.append(send_df)
+
+    chan_cdf = pd.concat(all_chan_df)
+    chan_cdf["isflx"] = chan_cdf["isflx"].apply(lambda x: int.from_bytes(x, "little"))
+
+    send_cdf = pd.concat(all_send_df)
+
+    chan_cdf["rank"] = rank
+    send_cdf["rank"] = rank
+
+    cols = chan_cdf.columns
+    cols = ["rank", "ts"] + list(cols[:-2])
+    chan_cdf = chan_cdf[cols]
+
+    cols = send_cdf.columns
+    cols = ["rank", "ts"] + list(cols[:-2])
+    send_cdf = send_cdf[cols]
+
+    return (chan_cdf, send_cdf)
+
+
+def aggr_msgs_some(
+    trace_name: str, rank_beg: int, rank_end: int
+) -> Tuple[pd.DataFrame, pd.DataFrame]:
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_name)
+
+    path_fmt = "{0}/trace/msgs/msgs.{1}.bin"
+    all_bins = [path_fmt.format(trace_dir, r) for r in range(rank_beg, rank_end + 1)]
+    print(f"Reading {all_bins} msgs.bin")
+
+    for p in all_bins:
+        assert os.path.exists(p)
+
+    with multiprocessing.Pool(16) as p:
+        all_dfs = p.map(read_msgs, all_bins)
+
+    chan_df: DataFrame = pd.concat(map(lambda x: x[0], all_dfs))
+    send_df = pd.concat(map(lambda x: x[1], all_dfs))
+
+    chan_df.sort_values(["rank", "ts"], inplace=True)
+    send_df.sort_values(["rank", "ts"], inplace=True)
+
+    return chan_df, send_df
+
+
+def aggr_msgs_all(trace_name):
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_name)
+
+    print(f"Searching for msgs.*.bin in {trace_dir}")
+    glob_patt = trace_dir + "/trace/msgs/msgs.*.bin"
+    print(f"Glob path: {glob_patt}")
+    all_bins = glob.glob(glob_patt)
+    print(f"Bins found: {len(all_bins)}")
+
+    #  all_bins = all_bins[:16]
+
+    with multiprocessing.Pool(16) as p:
+        all_dfs = p.map(read_msgs, all_bins)
+
+    chan_df = pd.concat(map(lambda x: x[0], all_dfs))
+    send_df = pd.concat(map(lambda x: x[1], all_dfs))
+
+    chan_df = chan_df.iloc[:, [1, 0] + list(range(2, chan_df.shape[1]))]
+    send_df = send_df.iloc[:, [1, 0] + list(range(2, send_df.shape[1]))]
+
+    chan_df.sort_values(["ts", "rank"], inplace=True)
+    send_df.sort_values(["ts", "rank"], inplace=True)
+
+    chan_out = f"{trace_dir}/trace/msgs.aggr.chan.csv"
+    print(f"Writing to {chan_out}")
+    chan_df.to_csv(chan_out, index=None)
+
+    send_out = f"{trace_dir}/trace/msgs.aggr.send.csv"
+    print(f"Writing to {send_out}")
+    send_df.to_csv(send_out, index=None)
+
+    return chan_df, send_df
+
+
+def join_msgs(send_df, chan_df):
+    x = chan_df["ptr"].unique()
+    y = send_df["ptr"].unique()
+    # import pdb
+
+    # pdb.set_trace()
+    assert len(set(y).difference(set(x))) == 0
+
+    all_chan_ts = chan_df["ts"].unique()
+    all_chan_df = []
+
+    max_ts = send_df["ts"].max()
+
+    for ts in range(max_ts + 1):
+        if ts in all_chan_ts:
+            closest_ts = ts
+        else:
+            closest_ts = max([t for t in all_chan_ts if t < ts])
+
+        print(f"For ts: {ts}, using ts {closest_ts}")
+        df_ts = chan_df[chan_df["ts"] == closest_ts].copy()
+        df_ts["ts"] = ts
+        df_ts.drop_duplicates(subset=["rank", "ts", "ptr"], keep="last", inplace=True)
+        all_chan_df.append(df_ts)
+
+    chan_unroll_df = pd.concat(all_chan_df)
+    joined_df = send_df.merge(chan_unroll_df, how="left", on=["ts", "ptr"])
+
+    send_counts = send_df.groupby("ts", as_index=False).agg({"msgsz": "count"})
+    print("-> Send counts: \n", send_counts)
+
+    join_counts = joined_df.groupby("ts", as_index=False).agg({"msgsz": "count"})
+    print("-> Join counts: \n", join_counts)
+    return joined_df
+
+
+def plot_imshow(mat, stat):
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    vmin, vmax = np.percentile(mat, 1), np.percentile(mat, 99)
+    bounds = np.linspace(vmin, vmax, 16)
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="both")
+    im = ax.imshow(mat, norm=norm, aspect="auto", cmap="plasma")
+
+    fig.subplots_adjust(left=0.15, right=0.78)
+    cax = fig.add_axes([0.81, 0.12, 0.08, 0.8])
+    fig.colorbar(im, cax=cax)
+
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Timestep")
+
+    ax.set_title(f"Rank-Wise Heatmap (Stat: {stat})")
+    plot_fname = f"msgs.aggr.rw.{stat.lower()}"
+
+    trace_dir = ""
+    #  PlotSaver.save(fig, trace_dir, None, plot_fname)
+
+
+def plot_stat_slice(mat, stat, ts):
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    data_y = mat[ts]
+    data_x = range(len(data_y))
+
+    ax.plot(data_x, data_y, zorder=2)
+    ax.set_xlabel("Rank")
+    ax.set_ylabel(f"Stat {stat}")
+    ax.set_title(f"Stat {stat} vs Rank (TS: {ts})")
+
+    ax.set_ylim(bottom=0)
+
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    plot_fname = f"msgs.aggrslice.rw.{stat.lower()}"
+
+    trace_dir = ""
+    PlotSaver.save(fig, trace_dir, None, plot_fname)
+
+
 def gen_phase_map(trace_dir, cached=False):
     pmap_pkl = "/users/ankushj/CRAP/.phase_map"
     if cached:
@@ -162,7 +410,7 @@ def gen_prev_phase_df():
     return prev_phase_df
 
 
-@profile(precision=3)
+#  @profile(precision=3)
 def gen_phase_df(phase_map):
     prev_phase_df = gen_prev_phase_df()
 
@@ -213,7 +461,7 @@ def gen_phase_df(phase_map):
     return phase_df
 
 
-@profile(precision=3)
+#  @profile(precision=3)
 def aggr_msgs_map(rank, phase_df, trace_dir, max_ts):
     def log(msg):
         print("Rank {}: {}".format(rank, msg))
@@ -406,18 +654,16 @@ def plot_msgtl(ts, bins, hist_snd, hist_rcv, plot_dir):
         "Send/Receive Latency Distribution (BoundaryComm, ts:{})".format(ts_cs)
     )
 
-    ax.set_title(
-        "Message Latency Distribution (TS: 5K of 30K)"
-    )
+    ax.set_title("Message Latency Distribution (TS: 5K of 30K)")
 
     ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f}K".format(x / 1e3))
 
     max_idx = np.max(np.nonzero(hist_rcv))
     max_bin = bins[max_idx]
 
-    ax.plot([max_bin, max_bin], [0, 100000], linestyle='--', color='red')
+    ax.plot([max_bin, max_bin], [0, 100000], linestyle="--", color="red")
 
-    xlim_max = int(max_bin/100) * 100 + 100
+    xlim_max = int(max_bin / 100) * 100 + 100
     xlim_max = 500
 
     ax.set_xlim([0, xlim_max])
@@ -461,6 +707,212 @@ def run_plot_msgtl():
     run_plot_msgtl_ts([25000])
 
 
+def plot_stats_senddf(send_df, chan_df):
+    msg_df = join_msgs(send_df, chan_df)
+    msg_df[msg_df["isflx"] == 0].groupby("ts").agg({"msgsz": "count"})
+    msg_df[msg_df["isflx"] == 1].groupby("ts").agg({"msgsz": "count"})
+    msg_df.columns
+    aggr_msgdf = msg_df.groupby(["ts", "blk_id"]).agg(
+        {"msgsz": ["mean", "std", "sum", "count"], "nbr_id": "nunique"}
+    )
+
+    aggr_msgdf = msg_df.groupby(["ts", "rank_x"]).agg(
+        {"msgsz": ["mean", "std", "sum", "count"], "nbr_id": "nunique"}
+    )
+
+    sum_mat = aggr_msgdf["msgsz"]["sum"].unstack().values
+    count_mat = aggr_msgdf["msgsz"]["count"].unstack().values
+    nuniq_mat = aggr_msgdf["nbr_id"]["nunique"].unstack().values
+
+    plot_imshow(sum_mat, "sum")
+    count_mat = count_mat[2:, :]
+    plot_imshow(count_mat, "count")
+    nuniq_mat = nuniq_mat[2:, :]
+    plot_imshow(nuniq_mat, "nuniq")
+
+    t1 = chan_df[chan_df["ts"] == 1]
+    ca = t1.groupby(["blk_id", "tag"]).agg({"nbr_id": ["min", "max", "count"]})
+
+    aggr_df = send_df.groupby(
+        [
+            "rank",
+            "ts",
+        ],
+        as_index=False,
+    ).agg({"msgsz": ["mean", "std", "sum", "count"]})
+
+    aggr_df.set_index(["rank", "ts"], inplace=True)
+
+    all_stats = list(zip(*aggr_df.columns))[1]
+    for stat in all_stats:
+        print(f"Stat: {stat}")
+        stat_mat = aggr_df["msgsz"][stat].unstack().values
+        print(stat_mat)
+        plot_imshow(stat_mat.T, stat)
+        plot_stat_slice(stat_mat.T, stat, 6)
+
+    pass
+
+
+def run_regr_actual(X, y):
+    regr = linear_model.LinearRegression()
+    regr.fit(X, y)
+
+    print(regr.score(X, y))
+    print(np.array(regr.coef_, dtype=int))
+    print(np.array(regr.coef_))
+    print(regr.intercept_)
+
+
+def get_relevant_pprof_data(trace_name: str) -> Dict:
+    kfls = "MultiStage_Step => Task_LoadAndSendFluxCorrections"
+    kflr = "MultiStage_Step => Task_ReceiveFluxCorrections"
+    kbcs = "MultiStage_Step => Task_LoadAndSendBoundBufs"
+    kbcr = "MultiStage_Step => Task_ReceiveBoundBufs"
+    mip = "MPI_Iprobe()"
+
+    setup_tuple = setup_plot_stacked_generic(trace_name)
+    stack_keys, stack_labels, ylim, ymaj, ymin = setup_tuple
+    stack_keys = [kfls, kflr, kbcs, kbcr, mip]
+
+    trace_dir = trace_dir_fmt.format(trace_name)
+    concat_df = read_all_pprof_simple(trace_dir)
+    pprof_data = filter_relevant_events(concat_df, stack_keys)
+
+    data = {
+        "kfls": pprof_data[kfls],
+        "kflr": pprof_data[kflr],
+        "kbcs": pprof_data[kbcs],
+        "kbcr": pprof_data[kbcr],
+        "mip": pprof_data[mip]
+    }
+
+    return data
+
+
+def run_regr_wpprof():
+    trace_name = "athenapk5"
+    setup_tuple = setup_plot_stacked_generic(trace_name)
+    stack_keys, stack_labels, ylim, ymaj, ymin = setup_tuple
+
+    trace_dir = trace_dir_fmt.format(trace_name)
+    concat_df = read_all_pprof_simple(trace_dir)
+    pprof_data = filter_relevant_events(concat_df, stack_keys)
+
+    for k in pprof_data.keys():
+        print(k)
+
+    chan_df, send_df = aggr_msgs_all("athenapk5")
+    send_df = send_df[send_df["ts"] < 100]
+    msg_df = join_msgs(send_df, chan_df)
+
+    msg_df.columns
+    msg_df[["Dest", "nbr_rank"]]
+    msg_df[["blk_rank", "rank_x"]]
+    msg_df = msg_df[msg_df["ts"] > 1]
+    # only ts 0 and 1, load balancing etc Ig
+    wtf_df = msg_df[msg_df["Dest"] != msg_df["nbr_rank"]]
+    wtf_df = msg_df[msg_df["blk_rank"] != msg_df["rank_x"]]
+    wtf_df
+
+    flx_df = msg_df[msg_df["isflx"] == 1]
+    flx_df
+    nflx_df = msg_df[msg_df["isflx"] == 0]
+    nflx_df
+
+    groupby_cols = ["ts", "rank_x"]
+    groupby_cols = ["ts", "nbr_rank"]
+    flx_msgcnt_df = flx_df.groupby(groupby_cols, as_index=False).agg({"msgsz": "count"})
+
+    bc_msgcnt_df = nflx_df.groupby(groupby_cols, as_index=False).agg({"msgsz": "count"})
+
+    flxcnt_mat = (
+        flx_msgcnt_df.pivot(index="ts", columns=groupby_cols[1], values="msgsz")
+        .fillna(0)
+        .to_numpy(dtype=int)
+    )
+
+    fdim = flxcnt_mat.shape
+    flxmat = np.zeros((fdim[0], 512), dtype=int)
+    flxmat[: fdim[0], : fdim[1]] = flxcnt_mat
+
+    bccnt_mat = (
+        bc_msgcnt_df.pivot(index="ts", columns=groupby_cols[1], values="msgsz")
+        .fillna(0)
+        .to_numpy(dtype=int)
+    )
+
+    fdim = bccnt_mat.shape
+    bcmat = np.zeros((fdim[0], 512), dtype=int)
+    bcmat[: fdim[0], : fdim[1]] = bccnt_mat
+
+    kfls = "MultiStage_Step => Task_LoadAndSendFluxCorrections"
+    kflr = "MultiStage_Step => Task_ReceiveFluxCorrections"
+    kbcs = "MultiStage_Step => Task_LoadAndSendBoundBufs"
+    kbcr = "MultiStage_Step => Task_ReceiveBoundBufs"
+
+    # offset by 2 already
+    x = flxmat[2]
+    y = pprof_data[kflr]
+
+    x = bcmat[2]
+    y = pprof_data[kbcs]
+
+    run_regr_actual(x.reshape(-1, 1), y)
+
+    fig, ax = plt.subplots(1, 1)
+    ax2 = ax.twinx()
+    ax.plot(np.arange(512), pprof_data[kbcr], label="BC_Recv_Sec", zorder=2)
+    ax2.plot(np.arange(512), bcmat[2], color="orange", label="BC_Recv_MsgCount")
+    ax.yaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    )
+    ax.xaxis.set_major_locator(MultipleLocator(50))
+    ax.xaxis.set_minor_locator(MultipleLocator(10))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    ax.set_title("Boundary Comm - Recv Time vs Recv Msg Count")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Total Time for Stage")
+    ax2.set_ylabel("Num Messages")
+    ax2.yaxis.set_label_position("right")
+    fig.tight_layout()
+    fig.legend(loc="upper left", bbox_to_anchor=(0.3, 0.06), ncol=2)
+    PlotSaver.save(fig, trace_dir, None, "bc_recv")
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+    ax2 = ax.twinx()
+    ax.plot(np.arange(512), pprof_data[kbcs], label="BC_Send_Sec", zorder=2)
+    ax2.plot(np.arange(512), bcmat[2], color="orange", label="BC_Send_MsgCount")
+    ax.yaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    )
+    ax.xaxis.set_major_locator(MultipleLocator(50))
+    ax.xaxis.set_minor_locator(MultipleLocator(10))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    ax.set_title("Boundary Comm - Send Time vs Send Msg Count")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Total Time for Stage")
+    ax2.set_ylabel("Num Messages")
+    ax2.yaxis.set_label_position("right")
+    fig.tight_layout()
+    fig.legend(loc="upper left", bbox_to_anchor=(0.3, 0.06), ncol=2)
+    PlotSaver.save(fig, trace_dir, None, "bc_send")
+
+    pass
+
+
+def run_aggr_msgs_new():
+    global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    chan_df, send_df = aggr_msgs_all("athenapk13")
+
+
 if __name__ == "__main__":
     #  run_aggr_msgs()
-    run_plot_msgtl()
+    #  run_plot_msgtl()
+    #  plot_init()
+    run_aggr_msgs_new()
diff --git a/scripts/tau_analysis/analyze_pprof.py b/scripts/tau_analysis/analyze_pprof.py
new file mode 100644
index 0000000..1b98c5e
--- /dev/null
+++ b/scripts/tau_analysis/analyze_pprof.py
@@ -0,0 +1,964 @@
+import glob
+import multiprocessing
+import numpy as np
+import pandas as pd
+import io
+import ipdb
+import pickle
+import subprocess
+import string
+import sys
+import time
+
+#  import ray
+import re
+import traceback
+from common import plot_init, plot_init_big, PlotSaver, profile_label_map
+
+import matplotlib.pyplot as plt
+import matplotlib.ticker as ticker
+
+#  from pathlib import path
+
+#  from task import task
+from trace_reader import TraceOps
+
+global trace_dir_fmt
+trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+
+
+def read_pprof(fpath: str):
+    f = open(fpath).readlines()
+    lines = [l.strip("\n") for l in f if l[0] != "#"]
+
+    nfuncs = int(re.findall("(\d+)", lines[0])[0])
+    rel_lines = lines[1:nfuncs]
+    prof_cols = [
+        "name",
+        "ncalls",
+        "nsubr",
+        "excl_usec",
+        "incl_usec",
+        "unknown",
+        "group",
+    ]
+    df = pd.read_csv(
+        io.StringIO("\n".join(rel_lines)), delim_whitespace=True, names=prof_cols
+    )
+
+    rank = re.findall(r"profile\.(\d+)\.0.0$", fpath)[0]
+    df["rank"] = rank
+    return df
+
+
+def dedup_events(all_evts: list):
+    all_evts = sorted(all_evts, key=lambda x: len(x), reverse=True)
+    dedup_evts = []
+
+    has_evt = lambda ls, x: any([x in lsx for lsx in ls])
+    for evt in all_evts:
+        if not has_evt(dedup_evts, evt):
+            dedup_evts.append(evt)
+
+    print(f"[DedupEvts] Orig: {len(all_evts)}, New: {len(dedup_evts)}")
+    return dedup_evts
+
+
+def remove_events_suffix(all_evts: list, all_suffixes: list):
+    filt_evts = []
+    aug_sfxs = [s for s in all_suffixes]
+    aug_sfxs += [s + " [THROTTLED]" for s in all_suffixes]
+
+    for evt in all_evts:
+        if not any([evt.endswith(s) for s in aug_sfxs]):
+            filt_evts.append(evt)
+
+    print(f"[FiltEvts] Orig: {len(all_evts)}, New: {len(filt_evts)}")
+    return filt_evts
+
+
+def trim_and_filter_events(events: list):
+    trimmed = []
+
+    for e in events:
+        e = re.sub(r"(=> )?\[CONTEXT\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[UNWIND\].*?(?==>|$)", "", e)
+        e = re.sub(r"(=> )?\[SAMPLE\].*?(?==>|$)", "", e)
+        e = e.strip()
+
+        trimmed.append(e)
+
+    trimmed_uniq = list(set(trimmed))
+    trimmed_uniq = [e for e in trimmed_uniq if e != ""]
+    trimmed_uniq
+
+    events_mss = sorted(
+        [e for e in trimmed_uniq if e.startswith("Multi") and "=>" in e]
+    )
+
+    events_driver = [
+        e
+        for e in trimmed_uniq
+        if e.startswith("Driver_Main") and "=>" in e and "Multi" not in e
+    ]
+
+    all_events = events_mss + events_driver
+    all_events = [e for e in all_events if "Sync" not in e]
+
+    all_events += [".TAU application"]
+
+    print(
+        f"Events timmed and dedup'ed. Before: {len(events)}. After: {len(all_events)}"
+    )
+
+    print("Events retained: ")
+    for e in all_events:
+        print(f"\t- {e}")
+
+    #  input("Press ENTER to plot.")
+
+    return all_events
+
+
+"""
+Returns all unique leaf events in the set
+Discards intermediate events if leaf events present
+"""
+
+def get_top_events(df: pd.DataFrame, cutoff: float = 0.05):
+    total_runtime = df[df["name"] == ".TAU application"]["incl_usec"].iloc[0]
+
+    top_df = df[df["incl_usec"] >= total_runtime * cutoff]
+    top_names = top_df["name"].unique()
+
+    return trim_and_filter_events(top_names)
+    top_names
+
+    cand_names = top_names
+    filt_suffixes = [
+        "taupreload_main",
+        "MPI_Isend()",
+        "MPI_Iprobe()",
+        "MPI Collective Sync",
+    ]
+    cand_names = remove_events_suffix(cand_names, filt_suffixes)
+
+    filt_suffixes = [
+        ".TAU application => Driver_Main",
+        "Driver_Main => MultiStage_Step",
+    ]
+    cand_names = remove_events_suffix(cand_names, filt_suffixes)
+    cand_names = dedup_events(cand_names)
+
+    # remove super long events
+    #  all_rel_evts = [ e for e in cand_names if len(e) < 160 ]
+    #  print(f"Dropped long events. Orig: {len(cand_names)}, New: {len(all_rel_evts)}")
+    all_rel_evts = cand_names
+
+    """ Old athenapk events - may be relevant """
+    #  prim_evt = ".TAU application"
+    #  prim_df = df[df["name"].str.contains(prim_evt)]
+    #  top_df = prim_df[prim_df["incl_usec"] >= total_runtime * cutoff]
+    #  all_evts = top_df["name"].to_list()
+
+    #  all_evts = sorted(all_evts, key=lambda x: len(x), reverse=True)
+    #  all_rel_evts = []
+    #  has_prefix = lambda ls, x: any([lsx.startswith(x) for lsx in ls])
+    #  for evt in all_evts:
+    #  if not has_prefix(all_rel_evts, evt):
+    #  all_rel_evts.append(evt)
+
+    return all_rel_evts
+
+
+def fold_cam_case(name, cpref=1, csuf=4, sufidx=-2):
+    splitted = re.sub("([A-Z][a-z]+)", r" \1", re.sub("([A-Z]+)", r" \1", name)).split()
+
+    pref = "".join([s[0:cpref] for s in splitted[:sufidx]])
+    suf = "".join([s[0:csuf] for s in splitted[sufidx:]])
+    folded_str = pref + suf
+    return folded_str
+
+
+def abbrev_evt(evt: str):
+    evt_ls = evt.split("=>")
+    evt_ls = evt_ls[-2:]
+    evt_clean = []
+
+    for evt_idx, evt in enumerate(evt_ls):
+        evt = re.sub(r"Kokkos::[^ ]*", "", evt)
+        evt = re.sub(r"Task_", "", evt)
+        evt = re.sub(r".*?::", "", evt)
+        evt = re.sub(r"\[.*?\]", "", evt)
+        evt = evt.strip()
+        evt = re.sub(r" ", "_", evt)
+        if evt_idx == len(evt_ls) - 1:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=4, sufidx=-2)
+        else:
+            evt_try = fold_cam_case(evt, cpref=1, csuf=1, sufidx=-2)
+            #  evt_try = "".join([c for c in evt if c in string.ascii_uppercase])
+        if len(evt_try) > 1:
+            evt_clean.append(evt_try)
+        else:
+            evt_clean.append(evt)
+
+    abbrev = "_".join(evt_clean)
+
+    if "MPIA" in abbrev:
+        abbrev = "MPI-AllGath"
+
+    return abbrev
+
+
+def get_event_array(concat_df: pd.DataFrame, event: str) -> list:
+    nranks = 512
+
+    ev1 = event
+    ev2 = f"{ev1} [THROTTLED]"
+
+    ev1_mask = concat_df["name"] == ev1
+    ev2_mask = concat_df["name"] == ev2
+    temp_df = concat_df[ev1_mask | ev2_mask]
+
+    #  temp_df = concat_df[concat_df["name"] == event]
+    if len(temp_df) != nranks:
+        print(
+            f"WARN: {event} missing some ranks (nranks={nranks}), found {len(temp_df)}"
+        )
+    else:
+        return temp_df["incl_usec"].to_numpy()
+        pass
+
+    all_rank_data = []
+    all_ranks_present = temp_df["rank"].to_list()
+
+    temp_df = temp_df[["incl_usec", "rank"]].copy()
+    join_df = pd.DataFrame()
+    join_df["rank"] = range(nranks)
+    join_df = join_df.merge(temp_df, how="left").fillna(0).astype({"incl_usec": int})
+    data = join_df["incl_usec"].to_numpy()
+    return data
+
+
+def filter_relevant_events(concat_df: pd.DataFrame, events: list[str]):
+    temp_df = concat_df[concat_df["rank"] == 0].copy()
+    temp_df.sort_values(["incl_usec"], inplace=True, ascending=False)
+
+    all_data = {}
+
+    for event in events:
+        all_data[event] = get_event_array(concat_df, event)
+
+    return all_data
+
+
+def read_all_pprof_simple(trace_dir: str):
+    pprof_glob = f"{trace_dir}/profile/profile.*"
+    #  pprof_files = list(map(lambda x: f"{trace_dir}/profile/profile.{x}.0.0", range(32)))
+    all_files = glob.glob(pprof_glob)
+    #  all_files = pprof_files
+
+    print(f"Trace dir: {trace_dir}, reading {len(all_files)} files")
+
+    with multiprocessing.Pool(16) as pool:
+        all_dfs = pool.map(read_pprof, all_files)
+
+    concat_df = pd.concat(all_dfs)
+    concat_df["rank"] = concat_df["rank"].astype(int)
+    concat_df.sort_values(["rank"], inplace=True)
+
+    concat_df["name"] = concat_df["name"].str.strip()
+    return concat_df
+
+
+def read_all_pprof(trace_dir: str, events: list[str]):
+    pprof_glob = f"{trace_dir}/profile/profile.*"
+    all_files = glob.glob(pprof_glob)
+
+    with multiprocessing.Pool(16) as pool:
+        all_dfs = pool.map(read_pprof, all_files)
+
+    concat_df = pd.concat(all_dfs)
+    concat_df["rank"] = concat_df["rank"].astype(int)
+    concat_df.sort_values(["rank"], inplace=True)
+
+    concat_df["name"] = concat_df["name"].str.strip()
+
+    key_tot = ".TAU application"
+
+    pprof_data = filter_relevant_events(concat_df, events + [key_tot])
+    return pprof_data
+
+
+def setup_plot_stacked_ph_old():
+    stack_keys = [
+        "Task_FillDerived",
+        "CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_SendBoundaryBuffers_MeshData",
+        "Task_ReceiveBoundaryBuffers_MeshData",
+        ".TAU application",
+    ]
+
+    stack_labels = [
+        "$FD_{CO}$",
+        "$CF_{CN}$",
+        "$AG_{NO}$",
+        "$RR_{NO}$",
+        "$BC\_SND_{NO}$",
+        "$BC\_RCV_{NO}$",
+    ]
+
+    ylim = 11000
+    ymaj = 1000
+    ymin = 200
+
+
+def setup_plot_stacked_ph_new():
+    stack_keys = [
+        "Task_FillDerived",
+        "CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_LoadAndSendBoundBufs",
+        "Task_ReceiveBoundBufs",
+        ".TAU application",
+    ]
+
+    stack_labels = [
+        "$FD_{CO}$",
+        "$CF_{CN}$",
+        "$AG_{NO}$",
+        "$RR_{NO}$",
+        "$BC\_SND_{NO}$",
+        "$BC\_RCV_{NO}$",
+    ]
+
+    ylim = 11000
+    ymaj = 1000
+    ymin = 200
+
+
+def setup_plot_stacked_par_vibe():
+    stack_keys = [
+        "Task_FillDerived",
+        "Task_burgers_CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_LoadAndSendBoundBufs",
+        "Task_ReceiveBoundBufs",
+    ]
+
+    stack_keys_extra = [
+        "MPI_Allreduce()",
+        "MultiStage_Step => Task_SetInternalBoundaries",
+        "MultiStage_Step => Task_WeightedSumData",
+        "MultiStage_Step => FluxDivergenceMesh",
+    ]
+
+    stack_labels = [
+        "$MSS\_FD_{CO}$",
+        "$MSS\_CF_{CN}$",
+        "$LB\_AG_{NO}$",
+        "$LB\_RR_{NO}$",
+        "$BC\_SND_{NO}$",
+        "$BC\_RCV_{NO}$",
+    ]
+
+    stack_labels_extra = [
+        "$AR_{NO}$",
+        "$MSS\_SIB_{CO}$",
+        "$MSS\_WSD_{CO}$",
+        "$MSS\_FDM_{CO}$",
+    ]
+
+    #  stack_keys += stack_keys_extra
+    #  stack_labels += stack_labels_extra
+
+    ylim = 1000
+    ymaj = ylim / 10
+    ymin = ymaj / 5
+
+
+def setup_plot_stacked_generic(trace_name):
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_name)
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    stack_keys = get_top_events(concat_df, 0.02)
+    stack_labels = list(map(abbrev_evt, stack_keys))
+
+    key_tot = ".TAU application"
+    stack_labels[stack_keys.index(key_tot)] = ""
+
+    stack_keys
+    stack_labels
+
+    drop_idxes = [
+        i
+        for i in range(len(stack_labels))
+        if len(stack_labels[i]) > 20 or "/" in stack_labels[i]
+    ]
+    stack_keys = [stack_keys[i] for i in range(len(stack_keys)) if i not in drop_idxes]
+    stack_labels = [
+        stack_labels[i] for i in range(len(stack_labels)) if i not in drop_idxes
+    ]
+
+    ylim = 18000 / 5
+    ymaj = 2000 / 5
+    ymin = 500 / 5
+
+    ylim, ymaj, ymin = 12000, 1000, 200
+
+    ylim, ymaj, ymin = 3500, 500, 100
+    #  ylim, ymaj, ymin = 1500, 250, 50
+    #  ylim, ymaj, ymin = 1000, 200, 40
+
+    return stack_keys, stack_labels, ylim, ymaj, ymin
+
+
+"""
+Setup stack_keys, stack_labels, ylim, ymin, ymaj before calling this
+"""
+
+
+def plot_stacked_pprof(trace_name: str):
+    print(f"Running plot_stacked_pprof for trace: {trace_name}")
+
+    setup_tuple = setup_plot_stacked_generic(trace_name)
+    stack_keys, stack_labels, ylim, ymaj, ymin = setup_tuple
+
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_name)
+    trace_label = profile_label_map[trace_name]
+
+    concat_df = read_all_pprof_simple(trace_dir)
+    pprof_data = filter_relevant_events(concat_df, stack_keys)
+
+    data_y = [pprof_data[k] for k in stack_keys if k != ".TAU application"]
+    data_x = np.arange(len(data_y[0]))
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+    ax.stackplot(data_x, *data_y, labels=stack_labels, zorder=2)
+
+    data_y_app = pprof_data[".TAU application"]
+    y_mean = int(np.mean(data_y_app) / 1e6)
+    print(f"Total mean: {y_mean} s")
+
+    # AGGR PLOT
+    ax.plot(data_x, data_y_app, label="APP", zorder=2, linewidth=3)
+
+    ax.set_title(f"Runtime Breakdown - {trace_label}")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Time (s)")
+
+    ax.set_ylim([0, ylim * 1e6])
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(ymaj * 1e6))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(ymin * 1e6))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    #  ax.legend(ncol=4)
+    ax.legend(ncol=5, fontsize=8)
+
+    fig.tight_layout()
+    plot_fname = f"runtime.pprof.stacked.rankwise.{trace_name}"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+    # INDVL PLOTS
+    for lab, dy in zip(stack_labels, data_y):
+        fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+        ax.plot(data_x, dy, label=lab, zorder=2)
+        ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+        ax.set_ylim(bottom=0)
+        ax.set_title(f"Pprof Component: {lab}")
+        ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())
+        ax.xaxis.set_major_locator(ticker.MultipleLocator(64))
+        ax.xaxis.set_minor_locator(ticker.MultipleLocator(16))
+        plt.grid(visible=True, which="major", color="#999", zorder=0)
+        plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+        plot_fname = f"runtime.pprof.stacked.com{lab.lower()}.{trace_name}"
+        PlotSaver.save(fig, "", None, plot_fname)
+
+
+def interpolate_data(data: np.array):
+    valid_data = [d for d in data if d != 0]
+    valid_mean = np.mean(valid_data)
+    interpolated_data = [valid_mean if d == 0 else d for d in data]
+    return np.array(interpolated_data)
+
+
+def get_rankhours(trace_dir: str, keys: list[str]) -> dict:
+    keys_to_interpolate = ["Task_ReceiveBoundaryBuffers_MeshData"]
+
+    pprof_data = read_all_pprof(trace_dir, keys)
+
+    rh_data = {}
+    for k in keys:
+        if k not in pprof_data:
+            print(f"WARN - rankhour key ({k}) not in pprof data")
+            continue
+
+        if k in keys_to_interpolate:
+            print(f"Interpolating rankhours for key {k}")
+            data = interpolate_data(pprof_data[k])
+        else:
+            data = pprof_data[k]
+
+        sum_rsec = data.sum() / 1e6
+        sum_rh = sum_rsec / 3600
+        rh_data[k] = sum_rh
+
+    return rh_data
+
+
+def plot_rankhour_comparison(trace_dirs: list[str]):
+    all_keys = [
+        ".TAU application",
+        "Task_FillDerived",
+        "CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_SendBoundaryBuffers_MeshData",
+        "Task_ReceiveBoundaryBuffers_MeshData",
+    ]
+
+    all_keys = [
+        ".TAU application",
+        "Task_FillDerived",
+        "CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_LoadAndSendBoundBufs",
+        "Task_ReceiveBoundBufs",
+    ]
+
+    all_labels = [
+        "APP",
+        "$FD_{CO}$",
+        "$CF_{CN}$",
+        "$AG_{NO}$",
+        "$RR_{NO}$",
+        "$BC\_SND_{NO}$",
+        "$BC\_RCV_{NO}$",
+    ]
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    data_x = np.arange(len(all_keys))
+    labels_x = all_labels
+
+    all_data_x = []
+    all_prof_labels = []
+    all_labels = []
+
+    total_width = 0.7
+    nbins = len(trace_dirs)
+    bin_width = total_width / nbins
+
+    for bidx, trace_dir in enumerate(trace_dirs):
+        times = get_rankhours(trace_dir, all_keys)
+
+        label = trace_dir.split("/")[-1]
+        all_labels.append(label)
+
+        prof_label = profile_label_map[label]
+        all_prof_labels.append(prof_label)
+
+        # assert all keys appear in same order
+        data_xt = times.keys()
+        all_data_x.append(data_xt)
+        for xi in list(zip(*all_data_x)):
+            assert len(set(xi)) == 1
+
+        data_y = [times[k] for k in all_keys]
+        data_x_bin = data_x + bidx * bin_width
+
+        p = ax.bar(data_x_bin, data_y, bin_width, label=prof_label, zorder=2)
+        ax.bar_label(
+            p, fmt=lambda x: int(x * 7.03), rotation="vertical", fontsize=10, padding=4
+        )
+
+    ax.set_xlabel("Phase Name")
+    ax.set_ylabel("Rank-Hours (512 ranks * hrs/rank)")
+
+    prof_title = " vs ".join(all_prof_labels)
+    ax.set_title(f"Phase-wise Time Comparison")
+
+    ax.set_xticks(data_x, labels_x)
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(200))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(50))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 1500])
+
+    ax.legend()
+
+    fig.tight_layout()
+
+    #  label_fname = "_".join(all_labels)
+    label_fname = get_fname(all_labels)
+    fname = f"pprof_rh_{label_fname}"
+    PlotSaver.save(fig, "", None, fname)
+
+
+def plot_rankhour_comparison_new_util(
+    ax, all_keys, data_x_bin, data_y, plot_label: bool
+):
+    bars = [
+        {"name": "Total", "ids": [0]},
+        {"name": "Compute", "ids": [1, 2]},
+        {"name": "LBandAMR", "ids": [3, 4]},
+        {"name": "BoundaryComm", "ids": [5, 6]},
+    ]
+
+    total_width = 0.7
+    nbins = len(bars)
+    bin_width = total_width / nbins
+
+    for bar_id, bspec in enumerate(bars):
+        bname = bspec["name"]
+        bids = bspec["ids"]
+
+        bottom = 0
+        for did in bids:
+            dx = data_x_bin[bar_id]
+            dy = data_y[did]
+            #  dlabel = all_keys[did]
+            dlabel = ""
+
+            #  if not plot_label:
+            #  dlabel = ""
+
+            plot_bars = ax.bar(
+                [dx],
+                [dy],
+                bottom=bottom,
+                width=bin_width * 0.6,
+                label=dlabel,
+                color=f"C{did}",
+                zorder=2,
+            )
+
+            bottom += dy
+
+            if did == bids[-1]:
+                bar_labels = [f"{int(x)} s" for x in [bottom * 7]]
+                ax.bar_label(plot_bars, bar_labels, padding=2, fontsize=14, rotation=90)
+
+    labels_x = [x["name"] for x in bars]
+    ax.set_xticks(np.arange(len(labels_x)), labels_x)
+    pass
+
+
+def plot_rankhour_comparison_new(trace_dirs: list[str]):
+    all_keys = [
+        ".TAU application",
+        "Task_FillDerived",
+        "CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_SendBoundaryBuffers_MeshData",
+        "Task_ReceiveBoundaryBuffers_MeshData",
+    ]
+
+    all_keys = [
+        ".TAU application",
+        "Task_FillDerived",
+        "CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_LoadAndSendBoundBufs",
+        "Task_ReceiveBoundBufs",
+    ]
+
+    all_labels = [
+        "APP",
+        "$FD_{CO}$",
+        "$CF_{CN}$",
+        "$AG_{NO}$",
+        "$RR_{NO}$",
+        "$BC\_SND_{NO}$",
+        "$BC\_RCV_{NO}$",
+    ]
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    data_x = np.arange(len(all_keys))
+    labels_x = all_labels
+
+    all_data_x = []
+    all_prof_labels = []
+    all_labels = []
+
+    total_width = 0.7
+    nbins = len(trace_dirs)
+    bin_width = total_width / nbins
+
+    all_trace_labels = [
+        "1. Baseline",
+        "2. LPT",
+        "3. Contig-Improved",
+        "4. COntig-Improved-Iterative",
+        "5. CI-Iter + ManualLB",
+    ]
+
+    for bidx, trace_dir in enumerate(trace_dirs):
+        times = get_rankhours(trace_dir, all_keys)
+
+        label = trace_dir.split("/")[-1]
+        all_labels.append(label)
+
+        prof_label = profile_label_map[label]
+        all_prof_labels.append(prof_label)
+
+        # assert all keys appear in same order
+        data_xt = times.keys()
+        all_data_x.append(data_xt)
+        for xi in list(zip(*all_data_x)):
+            assert len(set(xi)) == 1
+
+        data_y = [times[k] for k in all_keys]
+        data_x_bin = data_x + bidx * bin_width
+
+        #  p = ax.bar(data_x_bin, data_y, bin_width, label=prof_label, zorder=2)
+        #  ax.bar_label(
+        #  p, fmt=lambda x: int(x * 7.03), rotation="vertical", fontsize=10, padding=4
+        #  )
+
+        plot_label = True if bidx == 0 else False
+
+        plot_rankhour_comparison_new_util(
+            ax, all_keys, data_x_bin, data_y, plot_label=plot_label
+        )
+        ax.plot([], [], label=all_trace_labels[bidx], color="black")
+
+    ax.set_xlabel("Phase Name")
+    ax.set_ylabel("Rank-Hours (512 ranks * hrs/rank)")
+
+    prof_title = " vs ".join(all_prof_labels)
+    ax.set_title(f"Phase-wise Time Comparison")
+
+    #  ax.set_xticks(data_x, labels_x)
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(200))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(50))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 1600])
+
+    ax.legend()
+
+    fig.tight_layout()
+
+    #  label_fname = "_".join(all_labels)
+    label_fname = get_fname(all_labels)
+    fname = f"pprof_rh_new_{label_fname}"
+    PlotSaver.save(fig, "", None, fname)
+
+
+def get_summary_simplified(trace_idx):
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_idx)
+
+    all_keys = [
+        ".TAU application",
+        "Task_FillDerived",
+        "CalculateFluxes",
+        "UpdateMeshBlockTree",
+        "RedistributeAndRefineMeshBlocks",
+        "Task_SendBoundaryBuffers_MeshData",
+        "Task_ReceiveBoundaryBuffers_MeshData",
+    ]
+
+    times = get_rankhours(trace_dir, all_keys)
+    tvals = [times[k] for k in all_keys]
+    t_unaccounted = tvals[0] - sum(tvals[1:])
+
+    t_comp = tvals[1] + tvals[2] + tvals[4] + t_unaccounted
+    t_sync = tvals[3]
+    t_comm = tvals[5] + tvals[6]
+
+    tsumm = {"comp": t_comp, "comm": t_comm, "sync": t_sync}
+
+    return tsumm
+
+
+def get_summary_simplified_mean(traces: list[int]):
+    props = ["comp", "comm", "sync"]
+    prop_vals = []
+
+    for trace_idx in traces:
+        times_t = get_summary_simplified(trace_idx)
+        vals_t = [times_t[p] for p in props]
+        print(f"Trace {trace_idx}: vals {vals_t}")
+        prop_vals.append(vals_t)
+
+    mean_vals = np.array(prop_vals).mean(axis=0)
+    return props, mean_vals
+
+
+def get_fname(lst):
+    if not lst:
+        return "", []
+        pass
+
+    # Find the shortest string
+    shortest_str = min(lst, key=len)
+
+    # Find common prefix
+    common_prefix = ""
+    for i in range(len(shortest_str)):
+        if all(string[i] == shortest_str[i] for string in lst):
+            common_prefix += shortest_str[i]
+        else:
+            break
+
+    # Find residuals
+    residuals = [string[len(common_prefix) :] for string in lst]
+
+    res_str = "_".join(residuals)
+    fname = f"{common_prefix}_{res_str}"
+    print(f"File Name: {fname}")
+    return fname
+
+
+def plot_summary_simplified():
+    props, tsumm_baseline = get_summary_simplified_mean([31, 34])
+    props, tsumm_lpt = get_summary_simplified_mean([32, 35])
+    props, tsumm_con_imp = get_summary_simplified_mean([33, 36])
+
+    data_x = np.arange(3)
+    labels_x = [
+        "Baseline\n(Contiguous Policy)",
+        "Longest Processing\nTime Policy",
+        "Contiguous-Improved\nPolicy",
+    ]
+
+    tsumm = [tsumm_baseline, tsumm_lpt, tsumm_con_imp]
+    tsumm_t = np.array(list(zip(*tsumm)))
+    tsumm_t = tsumm_t * 3600 / 512.0
+
+    phases = ["Computation", "Communication", "Synchronization"]
+    phases = ["Constant Work", "Communication Overhead", "Synchronization Overhead"]
+    order = [0, 1, 2]
+
+    phases = [phases[o] for o in order]
+    all_data = [tsumm_t[o] for o in order]
+
+    fig, ax = plt.subplots(1, 1, figsize=(8, 9))
+    width = 0.5
+
+    bottom = np.zeros(tsumm_t.shape[1])
+    for data_y, phase in zip(all_data, phases):
+        bars = ax.bar(data_x, data_y, width=width, bottom=bottom, label=phase, zorder=2)
+        bottom += data_y
+
+    bar_labels = [f"{int(x)} s" for x in bottom]
+    ax.bar_label(bars, bar_labels, padding=2, fontsize=14)
+
+    ax.legend(loc="upper right", bbox_to_anchor=(1.01, 1.01))
+    ax.set_xticks(data_x, labels_x)
+
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(1000))
+    ax.yaxis.set_minor_locator(ticker.MultipleLocator(200))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 10300])
+
+    ax.set_title("Policy-Wise Breakdown of AMR Perf")
+    ax.set_ylabel("Time (s)")
+
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x))
+    fig.tight_layout()
+
+    plot_fname = "lb.summary.simpl"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+def run_plot_stacked():
+    traces = [10, 22, 29, 30, 31, 32, 33]
+    traces = [37, 38, 39]
+    traces = [40, 41, 42, 43, 44]
+    traces = [44]
+
+    for trace_idx in traces:
+        trace_name = f"profile{trace_idx}"
+        #  plot_stacked_pprof_ph_new(trace_name)
+
+    #  plot_stacked_pprof_par_vibe("burgers1")
+    trace_name = "athenapk5"
+    trace_name = "athenapk14"
+    trace_name = "athenapk15"
+    trace_name = "athenapk16"
+
+    trace_name = "profile40"
+    trace_name = "stochsg2"
+    trace_dir = trace_dir_fmt.format(trace_name)
+    concat_df = read_all_pprof_simple(trace_dir)
+    get_top_events(concat_df)
+    #  setup_plot_stacked_generic(trace_name)
+    plot_stacked_pprof(trace_name)
+
+
+def run_plot_bar():
+    global trace_dir_fmt
+
+    traces = ["athenapk14", "athenapk15"]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    trace_dirs
+
+    traces = [37, 38, 39]
+    traces = [40, 41, 42, 43, 44]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(f"profile{x}"), traces))
+    plot_rankhour_comparison(trace_dirs)
+    return
+
+    traces = [31, 34, 32, 35, 33, 36]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    plot_rankhour_comparison(trace_dirs)
+
+    traces = [34, 35, 36]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    plot_rankhour_comparison(trace_dirs)
+
+    traces = [31, 32, 33]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    plot_rankhour_comparison(trace_dirs)
+
+    traces = [22, 28, 30]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    plot_rankhour_comparison(trace_dirs)
+
+    traces = [10, 22, 31]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    plot_rankhour_comparison(trace_dirs)
+
+    traces = [28, 32]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    plot_rankhour_comparison(trace_dirs)
+
+    traces = [30, 33]
+    trace_dirs = list(map(lambda x: trace_dir_fmt.format(x), traces))
+    plot_rankhour_comparison(trace_dirs)
+
+
+def run():
+    run_plot_stacked()
+    #  run_plot_bar()
+    #  plot_summary_simplified()
+
+
+if __name__ == "__main__":
+    #  global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    #  plot_init()
+    plot_init_big()
+    run()
diff --git a/scripts/tau_analysis/analyze_pprof_comm_matrix.py b/scripts/tau_analysis/analyze_pprof_comm_matrix.py
new file mode 100644
index 0000000..e7e91be
--- /dev/null
+++ b/scripts/tau_analysis/analyze_pprof_comm_matrix.py
@@ -0,0 +1,257 @@
+import os
+import glob
+import pandas as pd
+import re
+import sys
+
+import multiprocessing
+import numpy as np
+
+from common import PlotSaver, plot_init_big
+import matplotlib.pyplot as plt
+
+
+def read_pprof_line(line: str) -> list[str]:
+    mobj = re.match(r'\"(.*)\"(.*)', line)
+    if mobj == None:
+        return []
+    mobj.groups()
+    key, props = mobj.groups()
+    key = key.strip()
+    props = props.strip().split(' ')
+    return [key] + props
+
+
+def read_map(map_path: str) -> dict[int, int]:
+    with open(map_path, "r") as f:
+        lines = f.readlines()
+        lines = [line.strip() for line in lines]
+        lines = [line.split(",") for line in lines]
+        lines = [(int(line[0]), int(line[1])) for line in lines]
+        return dict(lines)
+
+
+def read_profile(prof_file: str):
+    # prof_dir = "/users/ankushj/repos/amr/scripts/ember-run"
+    # prof_file = f"{prof_dir}/profile.0.0.0"
+    fdata = open(prof_file).readlines()
+    lines = [line.strip() for line in fdata]
+
+    # find index of line with "cumulative" in import
+    idx = [i for i, s in enumerate(lines) if s.startswith("# eventname")][0]
+    header = lines[idx].strip('# ').split()
+    data = list(map(read_pprof_line, lines[idx + 1:]))
+
+    df = pd.DataFrame.from_records(data, columns=header)
+    # prof_file is of the form profile.<rank>.<thread>.<node>
+    # get rank
+    df["rank"] = prof_file.split('.')[-3]
+    return df
+
+
+def read_all_profiles(prof_dir: str):
+    print(f"Reading profiles from {prof_dir}...")
+    prof_files = glob.glob(f"{prof_dir}/profile.*")
+    print(f"Found {len(prof_files)} profile files.")
+
+    nthreads = 16
+    with multiprocessing.Pool(nthreads) as pool:
+        df_list = pool.map(read_profile, prof_files)
+        df = pd.concat(df_list)
+    # df = pd.concat([read_profile(prof_file) for prof_file in prof_files])
+    return df
+
+
+def construct_send_recv_pairs(df: pd.DataFrame) -> pd.DataFrame:
+    df_match = df[df["eventname"].str.contains("Message size sent to node")]
+    df_match = df_match[~df_match["eventname"].str.contains(" : ")]
+    df_match["dest"] = df_match["eventname"].str.split(" ").str[-1]
+
+    print(f"\ndf_match: \n{df_match}")
+
+    if len(df_match) == 0:
+        raise Exception("No message data found in profile.")
+#
+    # convert cols dest, mean, numevents to int
+    df_match = df_match.astype(
+        {"rank": int, "dest": int, "mean": float, "numevents": int})
+
+    df_match["comm_size"] = df_match["mean"] * df_match["numevents"]
+    df_msg = df_match[['rank', 'dest', 'comm_size']].copy()
+
+    return df_msg
+
+
+def construct_matrix_from_pairs(df: pd.DataFrame) -> np.ndarray:
+    df = df.groupby(['rank', 'dest']).sum().reset_index()
+    df = df.pivot(index='rank', columns='dest', values='comm_size').fillna(0)
+    df_mat = df.astype(float).to_numpy()
+    print(df_mat)
+    return df_mat
+
+
+def plot_matrix(df_mat: np.ndarray, plot_fname: str, plot_label: str) -> None:
+    fig, ax = plt.subplots(1, 1, figsize=(8, 8))
+    ax.imshow(df_mat, cmap='plasma', aspect='auto')
+    ax.set_xlabel("Destination Rank")
+    ax.set_ylabel("Source Rank")
+    ax.set_title(f"Comm. Matrix for {plot_label}")
+    plot_fname = f"comm_matrix_{plot_fname}"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def compute_local_comm(msg_df: pd.DataFrame) -> tuple[float, float]:
+    ranks_per_node = 16
+    def node(x): return int(x / ranks_per_node)
+    local_comm = 0.0
+    all_comm = 0.0
+
+    for _, row in msg_df.iterrows():
+        src, dest, msg_size = row
+        src_node = node(src)
+        dest_node = node(dest)
+
+        all_comm += msg_size
+        if src_node == dest_node:
+            local_comm += msg_size
+
+    return (local_comm, all_comm)
+
+
+def compute_local_comm_fast(msg_df: pd.DataFrame) -> tuple[float, float]:
+    ranks_per_node = 16
+    def node(x): return int(x / ranks_per_node)
+    msg_df["src_node"] = msg_df["rank"].apply(node)
+    msg_df["dest_node"] = msg_df["dest"].apply(node)
+    all_comm = msg_df["comm_size"].sum()
+    local_comm = (msg_df[msg_df["src_node"] == msg_df["dest_node"]])[
+        "comm_size"].sum()
+
+    return (local_comm, all_comm)
+
+
+def plot_prof_dir(prof_dir: str, prof_dirtype: str):
+    prof_dir
+    msg_mat
+    msg_mat_2
+    msg_mat_2 - msg_mat * 400
+    comm_df = read_all_profiles(prof_dir)
+    msg_df = construct_send_recv_pairs(comm_df)
+    msg_mat = construct_matrix_from_pairs(msg_df)
+
+    prof_dirname = os.path.basename(prof_dir)
+    # prof_dirtype = "halo3d"
+    # if "ember-runs-26" in prof_dirname:
+    #     prof_dirtype = "halo3d_26"
+    plot_fname = f"comm_matrix_{prof_dirtype}_{prof_dirname}"
+    plot_matrix(msg_mat, plot_fname)
+
+
+def plot_comm_tuples(traces: list[str], comm_tuples: dict[str, tuple[float, float]], trace_labels: dict[str, str]):
+    fig, ax = plt.subplots(1, 1, figsize=(8, 8))
+
+    labels_x = [trace_labels[t] for t in traces]
+    data_x = np.arange(len(traces))
+    width = 0.35
+
+    data_yloc = np.array([t[0] for t in comm_tuples.values()])
+    data_ytot = np.array([t[1] for t in comm_tuples.values()])
+
+    data_yloc = data_yloc / (2 ** 40)  # to terabytes
+    data_ytot = data_ytot / (2 ** 40)  # to terabytes
+
+    ax.bar(data_x, data_yloc, width, label="Local Comm", color="C0", zorder=2)
+    ax.bar(data_x + width, data_ytot, width, label="All Comm", color="C1", zorder=2)
+    ax.set_xticks(data_x + width / 2)
+    ax.set_xticklabels(labels_x)
+
+    #ax.yaxis.grid(True)
+    ax.yaxis.grid(True, which='both')
+    ax.yaxis.set_major_formatter(plt.FuncFormatter(
+        lambda x, loc: "{:.1f}TB".format(x)))
+
+    ax.set_xlabel("Trace")
+    ax.set_ylabel("Comm. Volume")
+    ax.set_title("Comm. Volume for Different Traces")
+    ax.legend()
+    fig.tight_layout()
+    PlotSaver.save(fig, "", None, "comm_volume")
+
+
+def run_plot():
+    prof_dir = "/mnt/ltio/ember-runs/rval510.run0.iter100.var1"
+    plot_prof_dir(prof_dir, "halo3d")
+
+    prof_dir = "/mnt/ltio/ember-runs/rval384.run0.iter100.var1"
+    plot_prof_dir(prof_dir, "halo3d")
+
+    prof_dir = "/mnt/ltio/ember-runs/rval256.run0.iter100.var1"
+    plot_prof_dir(prof_dir, "halo3d")
+
+    prof_dir = "/mnt/ltio/ember-runs/rval128.run0.iter100.var1"
+    plot_prof_dir(prof_dir, "halo3d")
+
+    prof_dir = "/mnt/ltio/ember-runs/rval0.run0.iter100.var1"
+    plot_prof_dir(prof_dir, "halo3d")
+
+
+def run_plot_v2():
+    prof_dir = "/mnt/ltio/ember-runs-v2/rval0.run0.iter100.var4"
+    plot_prof_dir(prof_dir, "halo3dv2")
+
+    prof_dir = "/mnt/ltio/ember-runs-v2/rval510.run0.iter100.var4"
+    plot_prof_dir(prof_dir, "halo3dv2")
+
+
+def run_stochsg():
+    trace_names = ["stochsg44", "stochsg45", "stochsg46", "stochsg47"]
+    trace_labels = ["Baseline", "LPT", "ContigDP", "ContigDP+Iter"]
+    trace_labels = dict(zip(trace_names, trace_labels))
+
+    all_comm_tuples = {}
+
+    trace_name = "stochsg44"
+
+    for trace_name in trace_names:
+        prof_dir = f"/mnt/ltio/parthenon-topo/{trace_name}/profile"
+        comm_df = read_all_profiles(prof_dir)
+        msg_df = construct_send_recv_pairs(comm_df)
+        comm_mat = construct_matrix_from_pairs(msg_df)
+        comm_mat_mb = (comm_mat / (2 ** 20)).astype(int)
+        comm_mat_mb.shape
+        comm_mat_mb
+        comm_mat_mb.sum(axis=1)
+
+        plot_matrix(comm_mat, trace_name, trace_labels[trace_name])
+        comm_tuple = compute_local_comm_fast(msg_df)
+        all_comm_tuples[trace_name] = comm_tuple
+        del comm_df, msg_df, comm_mat
+
+    all_comm_tuples
+    plot_comm_tuples(trace_names, all_comm_tuples, trace_labels)
+
+
+def run(prof_dir: str):
+    # prof_dir = "/users/ankushj/repos/amr/scripts/ember-run"
+
+    # print(f"Local comm: {local_comm}, All comm: {all_comm}")
+    # print(f"Local comm ratio: {local_comm/all_comm}")
+
+    print("{:.0f},{:.0f},{:.2f}".format(
+        local_comm, all_comm, local_comm/all_comm))
+    # msg_df
+    # len(msg_df["rank"].unique())
+    # len(msg_df["dest"].unique())
+
+    # map_path = "/users/ankushj/repos/amr/scripts/ember_maps/map_h32_c16_r0.txt"
+    # rank_map = read_map(map_path)
+    # print(comm_mat)
+    # print(rank_map)
+    # pass
+
+
+if __name__ == "__main__":
+    plot_init_big()
+    prof_dir = sys.argv[1]
+    run(prof_dir)
+    # run_plot_v2()
diff --git a/scripts/tau_analysis/analyze_prof.py b/scripts/tau_analysis/analyze_prof.py
index 8f83d9a..4dafd46 100644
--- a/scripts/tau_analysis/analyze_prof.py
+++ b/scripts/tau_analysis/analyze_prof.py
@@ -2,120 +2,375 @@ import glob
 import multiprocessing
 import numpy as np
 import pandas as pd
-import ipdb
+import matplotlib
 import matplotlib.pyplot as plt
 import matplotlib.colors as colors
+import pathlib
 import pickle
 import re
 import subprocess
 import struct
 import sys
 import time
-
-import ray
+import os
+
+try:
+    import ipdb
+    import ray
+    from task import Task, ProfOutputReader
+except Exception as e:
+    pass
 import traceback
 
-from common import plot_init, PlotSaver, prof_evt_map
+from common import plot_init_big as plot_init, PlotSaver, prof_evt_map
 from matplotlib.ticker import FuncFormatter, MultipleLocator
 from typing import List, Tuple
 from pathlib import Path
-from task import Task
 from trace_reader import TraceReader, TraceOps
+from time import time
 
+global npernode
+npernode = 16
 
-class ProfOutputReader(Task):
-    def __init__(self, trace_dir: str, evt: int):
-        super().__init__(trace_dir)
-        self.evt = evt
+global nworkers
+nworkers = 16
 
-    @staticmethod
-    def worker(args):
-        trace_dir = args["trace_dir"]
-        r = args["rank"]
-        evt = args["evt"]
 
-        print(f"Running ProfReader for Rank: {r}, Evt: {evt}")
-        trace_reader = TraceReader(trace_dir)
-        return trace_reader.read_rank_prof(r, evt)
+def decorator_timer(some_function):
+    def wrapper(*args, **kwargs):
+        t1 = time()
+        result = some_function(*args, **kwargs)
+        end = time() - t1
+        return result, end
         pass
 
-    def gen_worker_fn_args(self):
-        args = super().gen_worker_fn_args()
-        args["evt"] = self.evt
-        return args
-
-    #  def gen_worker_fn_args_rank(self, rank):
-    #  args = self.gen_worker_fn_args()
-    #  args["rank"] = rank
-    #  return args
+    return wrapper
 
 
-def get_prof_path(evt: int) -> str:
-    global trace_dir
+def get_prof_path(trace_dir: str, evt: int) -> str:
     # replace merged with agg if single self-contained run
     ppath = f"{trace_dir}/prof.merged.evt{evt}.csv"
+    ppath = f"{trace_dir}/prof.aggr.evt{evt}.csv"
     return ppath
 
 
-def run_sep_by_evt_util(evt):
+def get_prof_path_rw(trace_dir: str, evt: int, rbeg: int, rend: int) -> str:
+    path_base = get_prof_path(trace_dir, evt)
+    ps = pathlib.Path(path_base).suffix
+    pp = path_base[: -len(ps)]
+    return f"{pp}.{rbeg}-{rend}{ps}"
+
+
+def read_evt_aggr(evt: int) -> pd.DataFrame:
+    global trace_dir
+    df_path = get_prof_path(trace_dir, evt)
+    df = pd.read_csv(df_path)
+    df.sort_values(["sub_ts", "rank", "block_id"], inplace=True)
+    aggr_df = df.groupby("sub_ts", as_index=False).agg(
+        {"rank": list, "block_id": list, "time_us": list}
+    )
+
+
+def rsbe_group_func(df):
+    df_agg = df.groupby(["sub_ts", "block_id"], as_index=False).agg({"data": list})
+
+    return df_agg
+
+
+def rsbe_add_func(df):
+    df_agg = df.groupby(["sub_ts", "block_id"], as_index=False).agg({"data": "sum"})
+    # df_agg.set_index(["sub_ts", "block_id"], inplace=True)
+    return df_agg
+
+
+"""
+More efficient version of run_sep_by_evt
+"""
+
+
+def rsbe_aggr_by_rank(evt_code: int, rbeg: int, rend: int, nranks: int):
+    global npernode, nworkers
+
+    #  evt_code = 0
+    reader = ProfOutputReader(trace_dir, evt_code, nranks, npernode, nworkers)
+    all_dfs = reader.run_rankwise(rbeg, rend)
+
+    # aggregate in advance for lower mem footprint
+    with multiprocessing.Pool(16) as p:
+        #  agg_dfs = p.map(rsbe_add_func, all_dfs)
+        agg_dfs = p.map(rsbe_add_func, all_dfs)
+
+    aggr_df = pd.concat(agg_dfs)
+
+    del agg_dfs
+    del all_dfs
+    return aggr_df
+
+
+def run_sep_by_evt_eff(evt_code):
+    rbeg, rend = 0, 16
+
     global trace_dir
     tr = TraceReader(trace_dir)
-    reader = ProfOutputReader(trace_dir, evt)
-    all_dfs = reader.run_rankwise(0, 512)
+    reader = ProfOutputReader(trace_dir, evt_code)
+    all_dfs = reader.run_rankwise(rbeg, rend)
+
     aggr_df = pd.concat(all_dfs)
+    del all_dfs
+
+    inter_out = f"{trace_dir}/aggr.inter.{rbeg}-{rend}.evt{evt_code}.csv"
+    inter_out = f"{trace_dir}/aggr.evt{evt_code}.inter.{rbeg}-{rend}.csv"
+    inter_out
+    aggr_df
+    aggr_df.to_csv(inter_out, index=None)
+    pass
+
+
+def run_sep_by_evt_create_mat_eff(evt_code: int, nranks: int):
+    #  allr = np.arange(0, nranks + 1, 256)
+    #  rpairs = list(zip(allr[:-1], allr[1:]))
+
+    #  all_dfs = list(map(lambda x: rsbe_aggr_by_rank(evt_code, *x, nranks), rpairs))
+    #  aggr_df = pd.concat(all_dfs)
+
+    #  del all_dfs
+    aggr_df_fpath = f"{trace_dir}/aggr.tmp.evt{evt_code}.csv"
+    #  aggr_df.to_csv(aggr_df_fpath, index=None)
+    aggr_df = pd.read_csv(aggr_df_fpath)
+
+    pivot_table = aggr_df.pivot_table(
+        values="data", index="sub_ts", columns="block_id", aggfunc=np.sum
+    )
+    #  pivot_table.to_numpy().shape
+    mat = pivot_table.to_numpy()
+    mat_npy = f"{trace_dir}/evt{evt_code}.mat.npy"
+    np.save(mat_npy, mat)
+    return
+
+    mat = [list(filter(lambda x: not np.isnan(x), row)) for row in mat]
+
+    mat_file = f"{trace_dir}/evt{evt_code}.mat.pickle"
+    print(f"Writing to {mat_file}")
+
+    with open(mat_file, "wb") as f:
+        f.write(pickle.dumps(mat))
+
+    mat = pickle.loads(open(mat_file, "rb").read())
+    print(f"Event {evt_code}: matrix verified: {len(mat)}, {len(mat[0])}")
+    pass
+
+
+#  evt_code = 1
+def run_sep_by_evt_create_mat(evt_code, nranks):
+    global trace_dir, npernode, nworkers
+    tr = TraceReader(trace_dir)
+    reader = ProfOutputReader(trace_dir, evt_code, nranks, npernode, nworkers)
+    #  all_dfs = reader.run_rankwise(0, 512)
+    #  nranks = 64
+    # nranks = 1024
+    all_dfs = reader.run_rankwise(0, nranks)
+
+    aggr_df = pd.concat(all_dfs)
+    #  print(evt_code, aggr_df)
+
+    print(f"aggr_df for evt: {evt_code}: {aggr_df}")
+
+    pivot_table = aggr_df.pivot_table(
+        values="data", index="sub_ts", columns="block_id", aggfunc=np.sum
+    )
+    pivot_table.to_numpy().shape
+    mat = pivot_table.to_numpy()
+
+    #  len(pivot_table.iloc[0, :].dropna().to_list())
+    #  len(np.sum(~np.isnan(mat), axis=1))
+    mat = [list(filter(lambda x: not np.isnan(x), row)) for row in mat]
+
+    mat_file = f"{trace_dir}/evt{evt_code}.mat.pickle"
+    print(f"Writing to {mat_file}")
+
+    with open(mat_file, "wb") as f:
+        f.write(pickle.dumps(mat))
+
+    mat = pickle.loads(open(mat_file, "rb").read())
+    print(f"Event {evt_code}: matrix verified: {len(mat)}, {len(mat[0])}")
+    return
+
+    #  aggr_df
+
+    #  with multiprocessing.Pool(16) as p:
+        #  agg_dfs = p.map(rsbe_group_func, all_dfs)
+
+    # aggregate in advance for lower mem footprint
+
+    with multiprocessing.Pool(16) as p:
+        agg_dfs = p.map(rsbe_add_func, all_dfs)
+
+    aggr_df = pd.concat(agg_dfs)
+    aggr_df.pivot
+    aggr_df["dlen"] = aggr_df["data"].apply(lambda x: len(x))
+
+    print(f"Value counts per block-id per ts, for evt {evt_code}:")
+    print(aggr_df["dlen"].value_counts())
+
+    aggr_df.sort_values(["sub_ts", "block_id"], inplace=True)
+    ag2_df = aggr_df.groupby("sub_ts", as_index=False).agg(
+        {"block_id": list, "data": list}
+    )
+
+    mat = ag2_df["data"].to_list()
 
-    aggr_df.sort_values(["ts", "sub_ts", "rank", "block_id"], inplace=True)
+    mat_file = f"{trace_dir}/evt{evt_code}.mat.pickle"
+    print(f"Writing to {mat_file}")
 
-    col_names = ["time_us", "time_us", "refine_flag", "block_idx"]
+    with open(mat_file, "wb") as f:
+        f.write(pickle.dumps(mat))
+
+    mat = pickle.loads(open(mat_file, "rb").read())
+    print(f"Event {evt_code}: matrix verified: {len(mat)}, {len(mat[0])}")
+
+
+def run_sep_by_evt_inner(evt, rbeg, rend, evt_df_path, nranks: int):
+    global trace_dir, npernode, nworkers
+    tr = TraceReader(trace_dir)
+    reader = ProfOutputReader(trace_dir, evt, nranks, npernode, nworkers)
+    all_dfs = reader.run_rankwise(rbeg, rend)
+    aggr_df = pd.concat(all_dfs)
+
+    #  aggr_df.sort_values(["ts", "sub_ts", "rank", "block_id"], inplace=True)
+    # below added later to make sure analysis of actual isn't buggy
+    aggr_df.sort_values(["sub_ts", "block_id"], inplace=True)
+
+    col_names = [
+        "time_us",
+        "time_us",
+        "refine_flag",
+        "block_idx",
+        "cost",
+        "time_us",
+        "time_us",
+    ]
     cols_new = list(aggr_df.columns)[:-1]
     cols_new.append(col_names[evt])
     aggr_df.columns = cols_new
 
-    evt_df_path = get_prof_path(evt)
+    if evt == 4:
+        aggr_df["sub_ts"] -= 1
+        #  aggr_df["cost"] *= 1000
+
     print(f"Writing evt {evt} to {evt_df_path}...")
     print(aggr_df)
     aggr_df.to_csv(evt_df_path, index=None)
 
 
-def run_sep_by_evt():
-    evts = [0, 1]
+def run_sep_by_evt_util(evt, nranks: int):
+    global trace_dir
+    rbase = np.arange(0, nranks + 1, 64)
+    #  rbase = np.arange(0, 65, 64)
+    all_rbeg = rbase[:-1]
+    all_rend = rbase[1:]
+
+    all_pairs = list(zip(all_rbeg, all_rend))
+    all_tmp_dfs = []
+
+    for rbeg, rend in all_pairs:
+        print(f"Extracting rank pair {rbeg}-{rend}")
+        df_out = get_prof_path_rw(trace_dir, evt, rbeg, rend)
+        all_tmp_dfs.append(df_out)
+        run_sep_by_evt_inner(evt, rbeg, rend, df_out, nranks)
+
+    all_dfs = map(pd.read_csv, all_tmp_dfs)
+    df_merged = pd.concat(all_dfs)
+
+    df_merged_out = get_prof_path(trace_dir, evt)
+    df_merged.to_csv(df_merged_out, index=None)
+    pass
+
+
+def run_sep_by_evt(nranks: int):  #  evts = [0, 1, 3, 4, 5, 6] evts = [3]
+    #  evts = [0, 1]
+    #  for evt in evts:
+        #  run_sep_by_evt_create_mat_eff(evt, nranks)
+
+    evts = [2, 3]
     for evt in evts:
-        run_sep_by_evt_util(evt)
+        run_sep_by_evt_util(evt, nranks)
 
 
-def group_by_ts(ref_df):
-    boundaries = ref_df["block_id"] == -1
+def read_aggr_more(evt_code):
+    df_path = f"{trace_dir}/prof.aggrmore.evt{evt_code}.csv"
+    df = pd.read_csv(df_path)
 
-    all_refs = []
+    if evt_code == 2:
+        cols = ["block_id", "refine_flag"]
+    else:
+        cols = ["block_id"]
 
-    prev_bidx = -1
-    for bidx in ref_df[boundaries].index:
-        df_cur = ref_df.loc[prev_bidx + 1 : bidx - 1]
-        uniq_ts = df_cur["ts"].unique()
-        assert len(uniq_ts) <= 1
-        if len(uniq_ts) == 0:
-            prev_bidx = bidx
-            continue
+    for col in cols:
+        df[col] = safe_ls(df[col])
 
-        cur_ref_tuple = (
-            uniq_ts[0],
-            list(df_cur["block_id"]),
-            list(df_cur["refine_flag"]),
-        )
+    return df
+
+
+def run_aggr_costs_evt(evt_code):
+    global trace_dir
+    df_path = f"{trace_dir}/prof.aggr.evt{evt_code}.csv"
+    df = pd.read_csv(df_path)
+    aggr_df = df.groupby(["sub_ts", "block_id"], as_index=False).agg(
+        {"rank": "min", "time_us": "sum"}
+    )
 
-        all_refs.append(cur_ref_tuple)
+    aggr_df.sort_values(["sub_ts", "block_id"], inplace=True)
+
+    #  aggr_df2 = aggr_df.groupby([ "sub_ts" ], as_index=False).agg({
+    #  "block_id": list,
+    #  "rank": "min",
+    #  "time_us": list
+    #  })
+
+    return aggr_df
+
+
+def run_aggr_costs():
+    global trace_dir
+    df0 = run_aggr_costs_evt(0)
+    df1 = run_aggr_costs_evt(1)
+
+    aggr_df = df0.merge(df1, on=["sub_ts", "block_id"], how="outer", sort=True)
+
+    na_vals = {
+        "rank_x": -1,
+        "time_us_x": 0,
+        "rank_y": -1,
+        "time_us_y": 0,
+    }
+
+    ag2_df = aggr_df.fillna(value=na_vals).astype(
+        {
+            "rank_x": int,
+            "time_us_x": int,
+            "rank_y": int,
+            "time_us_y": int,
+        }
+    )
 
-        prev_bidx = bidx
+    ag2_df["time_us"] = ag2_df["time_us_x"] + ag2_df["time_us_y"]
+    ag2_df.sort_values(["sub_ts", "block_id"], inplace=True)
+    ag3_df = ag2_df.groupby(["sub_ts"], as_index=False).agg({"time_us": list})
 
-    return all_refs
+    df_out_path = f"{trace_dir}/prof.aggrmore.evt01.csv"
+    ag3_df.to_csv(df_out_path, index=None)
 
 
-def run_aggr_evt3():
+def run_aggr_assns():
     evt_code = 3
     global trace_dir
     df_path = f"{trace_dir}/prof.aggr.evt{evt_code}.csv"
     df_out_path = f"{trace_dir}/prof.aggrmore.evt{evt_code}.csv"
 
+    #  if os.path.exists(df_out_path):
+    #  print(f"File {df_out_path} exists. Just reading that")
+    #  return read_aggr_more(evt_code)
+
     df = pd.read_csv(df_path)
 
     nranks = df["rank"].max() + 1
@@ -127,6 +382,7 @@ def run_aggr_evt3():
         ref_df["sub_ts"] = ref_df["block_idx"].eq(0).cumsum() - 1
         all_dfs.append(ref_df)
 
+    del df
     concat_df = pd.concat(all_dfs)
 
     aggr_df = concat_df.groupby("sub_ts", as_index=False).agg(
@@ -138,11 +394,18 @@ def run_aggr_evt3():
     return aggr_df
 
 
-def run_aggr_evt4():
+def run_aggr_refs(nranks: int):
+    global npernode, nworkers
+
     evt_code = 2
     global trace_dir
     df_out_path = f"{trace_dir}/prof.aggrmore.evt{evt_code}.csv"
-    reader = ProfOutputReader(trace_dir, evt_code)
+
+    #  if os.path.exists(df_out_path):
+    #  print(f"File {df_out_path} exists. Just reading that")
+    #  return read_aggr_more(evt_code)
+
+    reader = ProfOutputReader(trace_dir, evt_code, nranks, npernode, nworkers)
     df0 = reader.run_rank(0)
     print(df0)
 
@@ -169,34 +432,14 @@ def run_aggr_evt4():
     # in cpp-readable format
     return aggr_df
 
-    #  merged_df = blk_df.merge(aggr_df, how="left", on="sub_ts").fillna(
-    #  0, downcast="infer"
-    #  )
-
-    #  tmp_df = merged_df[["nblocks_before", "nref", "nderef", "nblocks_after"]].copy()
-    #  tmp_df["nblocks_after_computed"] = (
-    #  tmp_df["nblocks_before"] + 7 * tmp_df["nref"] - 7 * tmp_df["nderef"] / 8
-    #  )
-
-    #  try:
-    #  assert (tmp_df["nblocks_after_computed"] == tmp_df["nblocks_after"]).all()
-    #  print("Refinements validated. All block counts as expected!!")
-    #  except AssertionError as e:
-    #  mismatch_df = tmp_df[
-    #  tmp_df["nblocks_after_computed"] != tmp_df["nblocks_after"]
-    #  ]
-    #  print(mismatch_df)
-
-    #  print(deref_df[["ts", "nblocks", "nref", "nderef"]])
 
-    #  for index, data in aggr_df.iterrows():
-    #  print(index)
-    #  print(data)
-    #  break
+def run_aggr_costs():
+    df.sort_values(["sub_ts", "block_id"], inplace=True)
 
-    #  print(df)
-    #  print(aggr_df)
-    #  print(aggr_df[ "ts", "block_id", "refine_flag" ])
+    aggr_df = df.groupby(["sub_ts"], as_index=False).agg(
+        {"ts": "unique", "rank": list, "block_id": list, "cost": list}
+    )
+    pass
 
 
 def validate_refinements(blk_df: pd.DataFrame, ref_df: pd.DataFrame) -> None:
@@ -212,21 +455,134 @@ def validate_refinements(blk_df: pd.DataFrame, ref_df: pd.DataFrame) -> None:
     )
 
     tmp_df = merged_df[["nblocks_cur_ts", "nref", "nderef", "nblocks_next_ts"]].copy()
+
+    # for 3D code
+    #  tmp_df["nblocks_next_ts_computed"] = (
+        #  tmp_df["nblocks_cur_ts"] + 7 * tmp_df["nref"] - 7 * tmp_df["nderef"] / 8
+    #  ).astype(int)
+
+    # for 2D code
     tmp_df["nblocks_next_ts_computed"] = (
-        tmp_df["nblocks_cur_ts"] + 7 * tmp_df["nref"] - 7 * tmp_df["nderef"] / 8
-    )
+        tmp_df["nblocks_cur_ts"] + 3 * tmp_df["nref"] - 3 * tmp_df["nderef"] / 4
+    ).astype(int)
 
     try:
         assert (tmp_df["nblocks_next_ts_computed"] == tmp_df["nblocks_next_ts"]).all()
         print("Refinements validated. All block counts as expected!!")
     except AssertionError as e:
         mismatch_df = tmp_df[
-            tmp_df["nblocks_after_computed"] != tmp_df["nblocks_after"]
+            tmp_df["nblocks_next_ts_computed"] != tmp_df["nblocks_next_ts"]
         ]
         print(mismatch_df)
         return
 
 
+def get_evtmat_by_bid(trace_dir: str, evt_code: int) -> Tuple[np.ndarray, np.ndarray]:
+    nblocks_npy = f"{trace_dir}/nblocks.bid.{evt_code}.npy"
+    evtmat_npy = f"{trace_dir}/evtmat.bid.{evt_code}.npy"
+
+    if os.path.exists(nblocks_npy) and os.path.exists(evtmat_npy):
+        with open(nblocks_npy, "rb") as f:
+            nblocks = np.load(f)
+
+        with open(evtmat_npy, "rb") as f:
+            time_evtmat = np.load(f)
+
+        print(f"Reading {trace_dir} evtmat from cache. Shape: {time_evtmat.shape}")
+        return nblocks, time_evtmat
+
+    df_path = get_prof_path(trace_dir, evt_code)
+    df = pd.read_csv(df_path)
+    df.sort_values(["sub_ts", "block_id"], inplace=True)
+    aggr_df = df.groupby("sub_ts", as_index=False).agg(
+        {"ts": "unique", "rank": "unique", "block_id": list, "time_us": list}
+    )
+
+    nblocks = aggr_df["time_us"].apply(lambda x: len(x)).to_numpy()
+    nbmax = max(nblocks)
+
+    time_us_std = aggr_df["time_us"].apply(
+        lambda x: np.array(x + [0] * (nbmax - len(x)))
+    )
+    time_evtmat = np.stack(time_us_std)
+
+    with open(nblocks_npy, "wb") as f:
+        np.save(f, nblocks)
+
+    with open(evtmat_npy, "wb") as f:
+        np.save(f, time_evtmat)
+
+    return nblocks, time_evtmat
+
+
+"""
+Returns a rank-wise matrix for a kernel
+"""
+
+
+def read_times(evt_code: int) -> pd.DataFrame:
+    global trace_dir
+    df_path = get_prof_path(trace_dir, evt_code)
+    df = pd.read_csv(df_path)
+    df.sort_values(["sub_ts", "block_id"], inplace=True)
+
+    nblocks = df.groupby("sub_ts").agg({"block_id": "nunique"})
+    nblocks = nblocks["block_id"].reset_index()["block_id"]
+
+    prof_df = df.groupby(["sub_ts", "rank"], as_index=False).agg({"time_us": "sum"})
+
+    all_sub_ts = prof_df["sub_ts"].unique()
+    nranks = prof_df["rank"].max() + 1
+
+    all_rank_tuples = []
+
+    for ts in np.arange(-1, all_sub_ts[-1] + 1):
+        if ts not in all_sub_ts:
+            for rank in np.arange(0, nranks):
+                rank_tuple = (ts, rank, 0)
+                all_rank_tuples.append(rank_tuple)
+
+    missing_df = pd.DataFrame(all_rank_tuples, columns=prof_df.columns)
+    final_df = pd.concat([prof_df, missing_df])
+    final_df.sort_values(["sub_ts", "rank"], inplace=True)
+    prof_mat = final_df["time_us"].to_numpy().reshape((-1, nranks))
+
+    return nblocks, prof_mat
+
+
+def validate_nblocks(blk_df: pd.DataFrame, nblocks_prof: pd.Series) -> None:
+    nblocks_blk = blk_df["block_id"].apply(lambda x: len(x))
+    #  nblocks_prof = prof_df["time_us"].apply(lambda x: len(x))
+
+    print(f"Validating times for event against assignments")
+    if nblocks_blk.size == nblocks_prof.size:
+        print("\tSizes match. Promising start")
+        if (nblocks_blk == nblocks_prof).all():
+            print("\tAll nblocks match")
+        else:
+            print("\t ERROR. Nblocks do not match. Uncertain how to proceed.")
+    else:
+        print(f"\t ERROR. Size mismatch. {nblocks_blk.size} vs {nblocks_prof.size}")
+
+
+def compute_prof_mat_stats(prof_mat):
+    print("Computing misc stats on prof_mat")
+
+    total_rankhours = prof_mat.sum() / (1e6 * 3600)
+    min_runtime = prof_mat.mean(axis=1).sum() / 1e6
+    actual_runtime = prof_mat.max(axis=1).sum() / 1e6
+
+    print("\tTotal RankHours: {:.1f}s".format(total_rankhours))
+    print("\tMin Runtime Possible: {:.1f}s".format(min_runtime))
+    print("\tActual Runtime: {:.1f}s".format(actual_runtime))
+
+    pass
+
+
+def analyze_prof_mat(prof_mat):
+    pass
+
+
 def safe_ls(ls):
     if type(ls) == str:
         ls = ls.strip("[]")
@@ -308,9 +664,14 @@ def write_assignments():
             bids = safe_ls(row["block_id"])
 
             assert len(ranks) == len(bids)
-            write_int(f, len(ranks))
 
-            for i in ranks:
+            ranks_bids = list(zip(ranks, bids))
+            ranks_bids = sorted(ranks_bids, key=lambda x: x[1])
+            ranks_ordby_bids = list(map(lambda x: x[0], ranks_bids))
+
+            write_int(f, len(ranks_ordby_bids))
+
+            for i in ranks_ordby_bids:
                 write_int(f, i)
 
     return
@@ -422,7 +783,7 @@ def _aggr_nparr_roundrobin(np_arr, nout):
     return all_isum_nparr
 
 
-def _aggr_nparr_by_rank(np_arr, nranks=512):
+def _aggr_nparr_by_rank(np_arr, nranks):
     aggr_arr = _aggr_nparr_roundrobin(np_arr, nranks)
     sum_a = np_arr.sum(axis=1)
     sum_b = aggr_arr.sum(axis=1)
@@ -441,7 +802,7 @@ Then we'll have to consult some allocation map to aggregate.
 """
 
 
-def aggr_block_nparr(np_arr, nranks=512, nnodes=32) -> Tuple[np.array, np.array]:
+def aggr_block_nparr(np_arr, nranks, nnodes) -> Tuple[np.array, np.array]:
     assert nranks % nnodes == 0
     arr_rankwise = _aggr_nparr_by_rank(np_arr, nranks)
     npernode = int(nranks / nnodes)
@@ -460,11 +821,16 @@ def aggr_block_nparr(np_arr, nranks=512, nnodes=32) -> Tuple[np.array, np.array]
 
 
 def _read_and_reg_evt(evt: str, clip=None):
-    df_path = get_prof_path(evt)
+    global trace_dir
+    df_path = get_prof_path(trace_dir, evt)
     print(f"Reading dataframe: {df_path}")
 
+    df0_agg = df0.groupby("sub_ts", as_index=False).agg({"time_us": list})
+    df1_agg = df1.groupby("sub_ts", as_index=False).agg({"time_us": list})
+
     df = pd.read_csv(df_path)
-    df_agg = df.groupby("ts", as_index=False).agg({"time_us": list})
+    df_agg = df.groupby("sub_ts", as_index=False).agg({"time_us": list})
+    df_agg = df_agg[df_agg["sub_ts"] >= 2]
 
     bw_1d = df["time_us"].to_numpy()
     bw_2d = df_agg["time_us"].to_list()
@@ -546,9 +912,7 @@ def _evt_get_labels(evt: str, proftype: str, diff_mode: bool) -> Tuple[str, str]
     return plot_fname, plot_title
 
 
-def plot_timegrid_blockwise(
-    evt: str, data_blockwise: np.array, data_1d: np.array, diff_mode=False
-):
+def plot_timegrid_blockwise( evt: str, data_blockwise: np.array, data_1d: np.array, diff_mode=False):
     plot_fname, title = _evt_get_labels(evt, "block", diff_mode=diff_mode)
 
     if data_1d is None:
@@ -564,7 +928,10 @@ def plot_timegrid_blockwise(
 
     bounds = np.linspace(range_beg, range_end, 10)  # 10 bins
     norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="both")
-    data_im = data_blockwise[:, :5000]
+    #  data_im = data_blockwise[:, :5000]
+    #  data_im = data_blockwise[1155:6270, :]
+
+    data_im = data_blockwise
 
     # Using the explicit API
     fig = plt.figure()
@@ -653,7 +1020,7 @@ def plot_timegrid_nodewise(evt: str, data_nodewise: np.array, diff_mode=False):
     PlotSaver.save(fig, trace_dir, None, plot_fname)
 
 
-def plot_timegrid_tuple(evt: str, evt_tuple: Tuple, nranks=512, nnodes=32):
+def plot_timegrid_tuple(evt: str, evt_tuple: Tuple, nranks, nnodes):
     bw_1d, bw_2d, b1_1d_rel, bw_2d_rel = evt_tuple
 
     rw_2d, nw_2d = aggr_block_nparr(bw_2d, nranks=nranks, nnodes=nnodes)
@@ -665,9 +1032,6 @@ def plot_timegrid_tuple(evt: str, evt_tuple: Tuple, nranks=512, nnodes=32):
 
 
 def plot_timegrid_all():
-    nranks = 512
-    nnodes = 32
-
     evt0_tuple = _read_and_reg_evt("0", clip=5000)
     evt1_tuple = _read_and_reg_evt("1", clip=5000)
     evt0_1_tuple = _add_tuples([evt0_tuple, evt1_tuple])
@@ -683,19 +1047,329 @@ def plot_timegrid_all():
     """
 
 
-def analyze():
-    #  Create tracedir/prof.aggr.evtN.csv
-    run_sep_by_evt()
-    #  run_group_by_ts()
-    #  blk_df = run_aggr_evt3()
-    #  ref_df = run_aggr_evt4()
-    #  validate_refinements(blk_df, ref_df)
-    #  write_refinements()
-    #  write_assignments()
+def get_evt_mat(evt_code):
+    mat_path = f"{trace_dir}/evt{evt_code}.mat.pickle"
+    mat = pickle.loads(open(mat_path, "rb").read())
+    return mat
+
+
+def filter_evt_mat(mat, evt_idx):
+    all_rows = []
+    for ts in mat:
+        row = []
+        for block in ts:
+            if len(block) > evt_idx:
+                row.append(block[evt_idx])
+            else:
+                row.append(0)
+        all_rows.append(row)
+
+    print(f"Rows: {len(all_rows)}")
+    return all_rows
+
+
+def plot_evt_instance_std():
+    mat0 = get_evt_mat(0)
+    mat1 = get_evt_mat(1)
+
+    mat00 = filter_evt_mat(mat0, 0)
+    std00 = list(map(np.std, mat00))
+
+    mat01 = filter_evt_mat(mat0, 1)
+    std01 = list(map(np.std, mat01))
+
+    mat10 = filter_evt_mat(mat1, 0)
+    std10 = list(map(np.std, mat10))
+
+    mat11 = filter_evt_mat(mat1, 1)
+    std11 = list(map(np.std, mat11))
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+    fname = "prof.evt.std.instancewise"
+
+    # BABA
+    ax.plot(range(len(std10)), std10, label="Evt1, 0 ($B_1$)")
+    ax.plot(range(len(std00)), std00, label="Evt0, 0 ($A_1$)")
+    ax.plot(range(len(std11)), std11, label="Evt1, 1 ($B_2$)")
+    ax.plot(range(len(std01)), std01, label="Evt0, 1 ($A_2$)")
+
+    ax.set_title("np.std for evts 0,1 (invocation-wise)")
+    ax.set_xlabel("timestep")
+    ax.set_ylabel("std-dev (ms)")
+
+    ax.legend()
+
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.1f} ms".format(x / 1e3))
+    ax.set_ylim([0, 3000])
+
+    ax.yaxis.set_major_locator(MultipleLocator(400))
+    ax.yaxis.set_minor_locator(MultipleLocator(100))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb")
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd")
+
+    fig.tight_layout()
+    PlotSaver.save(fig, trace_dir, None, fname)
+
+
+def plot_nblocks_from_assigndf():
+    global trace_dir
+
+    df_path = f"{trace_dir}/prof.aggrmore.evt3.csv"
+    df = pd.read_csv(df_path)
+    df["block_id"] = df["block_id"].apply(safe_ls)
+    df["nblocks"] = df["block_id"].apply(len)
+
+    data_y = df["nblocks"].to_numpy()
+    data_x = df["sub_ts"].to_numpy()
+
+    # in case sub_ts is negative
+    data_x += -data_x[0]
+
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    ax.plot(data_x, data_y)
+
+    trace_name = trace_dir.split("/")[-1]
+    ax.set_title(f"Block Count: Trace {trace_name}")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Block Count")
+    ax.set_ylim(bottom=0)
+
+    ax.yaxis.set_major_locator(MultipleLocator(100))
+    ax.yaxis.set_minor_locator(MultipleLocator(20))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb")
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd")
+
+    fig.tight_layout()
+
+    #  trace_name = os.path.basename(trace_dir)
+    #  plot_fname = f"nblocks_{trace_name}"
+    plot_fname = "nblocks"
+    PlotSaver.save(fig, trace_dir, None, plot_fname)
+    pass
+
+
+"""
+Analysis written on June 2, 2023.
+
+Plot heatmaps and variances for each individual occurence of a kernel.
+"""
+
+
+def run_analyze_prof_times():
+    # The matrix created via pickle keeps multiple invocations of a kernel # within a timestep separate. The numpy mat adds the two kernels.
+    run_sep_by_evt_create_mat_eff(0)
+    run_sep_by_evt_create_mat_eff(1)
+    #  run_sep_by_evt_create_mat_eff(5)
+    #  run_sep_by_evt_create_mat_eff(6)
+
+    matplotlib.use("GTK3Agg")
+    plt.ion()
+
+    all_files = glob.glob(f"{trace_dir}/evt*.mat.pickle")
+    all_files = sorted(all_files, key=lambda x: int(x.split(".")[-3][-1]))
+
+    # 0, 1, 5, 6
+    evt_labels = ["FillDerived", "CalcFluxes", "SNIAFeedback", "TabCooling"]
+
+    all_files
+
+    all_mats = list(map(lambda x: pickle.loads(open(x, "rb").read()), all_files))
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    all_50 = list(map(lambda x: np.percentile(x, 50, axis=1), all_mats))
+    all_90 = list(map(lambda x: np.percentile(x, 90, axis=1), all_mats))
+    all_99 = list(map(lambda x: np.percentile(x, 99, axis=1), all_mats))
+
+    data_x = np.arange(all_50[0].shape[0])
+    ax.clear()
+
+    ax.yaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f} ms".format(x / 1e3))
+    )
+    ax.yaxis.grid(which="major", visible=True, color="#bbb")
+    ax.xaxis.grid(which="major", visible=True, color="#bbb")
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd")
+
+    ax.yaxis.set_major_locator(MultipleLocator(20 * 1e3))
+    ax.yaxis.set_minor_locator(MultipleLocator(4 * 1e3))
+    ax.xaxis.set_minor_locator(MultipleLocator(10 * 1e3))
+    ax.xaxis.set_major_locator(MultipleLocator(20 * 1e3))
+
+    ax.set_title("50, 90, and 99 percentile block kernel times")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (ms)")
+
+    ax.xaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f}K".format(x / 1e3))
+    )
+
+    for idx, dy in enumerate(all_50):
+        color = f"C{idx}"
+        ls = "--"
+        alpha = 0.4
+        ax.plot(
+            data_x,
+            dy,
+            color=color,
+            linestyle=ls,
+            zorder=2,
+            alpha=alpha,
+        )
+
+    for idx, dy in enumerate(all_90):
+        color = f"C{idx}"
+        ls = "-."
+        alpha = 0.7
+        ax.plot(
+            data_x,
+            dy,
+            color=color,
+            linestyle=ls,
+            zorder=2,
+            alpha=alpha,
+        )
+
+    for idx, dy in enumerate(all_99):
+        color = f"C{idx}"
+        ls = "-"
+        alpha = 1
+        ax.plot(
+            data_x,
+            dy,
+            color=color,
+            linestyle=ls,
+            zorder=2,
+            label=evt_labels[idx],
+            alpha=alpha,
+        )
+
+    ax.legend(ncol=4)
+    fig.tight_layout()
+    fname = "blktimes.50to99ptile"
+    PlotSaver.save(fig, trace_dir, None, fname)
+
+    return
+
+
+def analyze(nranks):
+    #  #  Create tracedir/prof.aggr.evtN.csv
+    run_sep_by_evt(nranks)
+    #  return
+
+    #  # create aggrmore.X.csv
+    blk_df = run_aggr_assns()
+    ref_df = run_aggr_refs(nranks)
+
+    #  # refinements.bin
+    write_refinements()
+    #  # assignments.bin
+    write_assignments()
+
+    # Validation suite
+    validate_refinements(blk_df, ref_df)
+
+    #  nblocks_0, prof_mat_0 = read_times(0)
+    #  nblocks_1, prof_mat_1 = read_times(1)
+
+    #  print("Validating nblocks from evt 0")
+    #  validate_nblocks(blk_df, nblocks_0)
+
+    #  print("Validating nblocks from evt 1")
+    #  validate_nblocks(blk_df, nblocks_1)
+
+    #  print("Computing stats on evt 0")
+    #  compute_prof_mat_stats(prof_mat_0)
+
+    #  print("Computing stats on evt 1")
+    #  compute_prof_mat_stats(prof_mat_1)
+
+    #  print("Computing stats on evt 0+1")
+    #  compute_prof_mat_stats(prof_mat_0 + prof_mat_1)
+
     #  Only when a run had to be restarted
     #  run_join_two_traces()
 
 
+def convert_pickled_mat_to_bin(trace_dir: str) -> None:
+    n = 3
+    m = 4
+    mat = np.random.rand(n, m)
+    mat_out = f"{trace_dir}/tmp.bin"
+
+    skip_map = { 0: 0, 1: 3 }
+
+    #  for evt_code in [0, 1, 5, 6]:
+    for evt_code in [0, 1]:
+        mat_in = f"{trace_dir}/evt{evt_code}.mat.pickle"
+        mat_out = f"{trace_dir}/evt{evt_code}.mat.bin"
+        #  mat_out = f"{trace_dir}/tmp.bin"
+
+        print(f"Converting {mat_in} -> {mat_out}")
+
+        mat = pickle.loads(open(mat_in, "rb").read())
+        n = len(mat)
+        ntoskip = skip_map[evt_code]
+        print(f"\t - {n} timesteps found ({ntoskip} skipped) ...")
+
+        with open(mat_out, "wb") as f:
+            f.write(n.to_bytes(4, "little"))
+            for ts in range(0, n):
+                row = np.array(mat[ts], dtype=np.int32)
+                m = len(row)
+                tstoemit = ts + ntoskip
+                f.write(tstoemit.to_bytes(4, "little"))
+                f.write(m.to_bytes(4, "little"))
+                f.write(row.tobytes())
+
+def convert_npy_mat_to_bin(trace_dir: str) -> None:
+    skip_map = { 0: 0, 1: 3}
+
+    for evt_code in [0, 1]:
+        mat_in = f"{trace_dir}/evt{evt_code}.mat.npy"
+        mat_out = f"{trace_dir}/evt{evt_code}.mat.bin"
+        #  mat_out = f"{trace_dir}/tmp.bin"
+
+        print(f"Converting {mat_in} -> {mat_out}")
+        mat = np.load(mat_in)
+        mat = mat.astype(np.int32)
+        print(mat)
+
+        n = len(mat)
+        #  n = 8
+        ntoskip = skip_map[evt_code]
+        print(f"\t - {n} timesteps found ({ntoskip} skipped) ...")
+
+        with open(mat_out, "wb") as f:
+            f.write(n.to_bytes(4, "little"))
+            for ts in range(0, n):
+                row = mat[ts]
+                row = row[row != -2147483648]
+                #  row = np.array(mat[ts], dtype=np.int32)
+                m = len(row)
+                #  print(f"Row len: {m}")
+                tstoemit = ts + ntoskip
+                #  print(row)
+                #  return
+                f.write(tstoemit.to_bytes(4, "little"))
+                f.write(m.to_bytes(4, "little"))
+                f.write(row.tobytes())
+
+
+        return
+    pass
+
+def tmp():
+    x = blk_df['block_id'].map(len)
+    y = list(map(len, mat))
+    len(x)
+    len(y)
+    sum(x) - sum(y) - sum(x[0:3])
+    evt_code = 1
+
+
 def plot():
     plot_init()
     #  evts = list(range(2))
@@ -706,15 +1380,40 @@ def plot():
     #  plot_rankhours(evts)
 
     # plots 0, 1, 0 + 1
-    plot_timegrid_all()
+    #  plot_timegrid_all()
+    plot_nblocks_from_assigndf()
 
 
 def run():
-    analyze()
+    nranks = 2048
+    analyze(nranks)
+    #  run_analyze_prof_times()
+    #  run_analyze_compare_prof()
     #  plot()
 
 
 if __name__ == "__main__":
+    global trace_dir_fmt
     global trace_dir
-    trace_dir = "/mnt/ltio/parthenon-topo/profile22"
+
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/{}"
+    trace_dir = trace_dir_fmt.format("profile44")
+    trace_dir = trace_dir_fmt.format("athenapk14")
+    trace_dir = trace_dir_fmt.format("athenapk4")
+    trace_dir = trace_dir_fmt.format("stochsg6")
+    trace_dir = trace_dir_fmt.format("stochsg25")
+    trace_dir = trace_dir_fmt.format("stochsg44")
+    #  trace_dir = trace_dir_fmt.format("sparse1")
+    trace_dir
+
+    #  plot_init()
+
     run()
+    #  convert_pickled_mat_to_bin(trace_dir)
+    #  convert_npy_mat_to_bin(trace_dir)
+    #  for trace in [38, 39, 41, 42]:
+    #  trace_dir = trace_dir_fmt.format(trace)
+    #  run()
+
+    #  trace_dir = "/mnt/ltio/parthenon-topo/burgers2"
+    #  run()
diff --git a/scripts/tau_analysis/analyze_prof_hist.py b/scripts/tau_analysis/analyze_prof_hist.py
new file mode 100644
index 0000000..07b7ef8
--- /dev/null
+++ b/scripts/tau_analysis/analyze_prof_hist.py
@@ -0,0 +1,44 @@
+import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+import matplotlib.colors as colors
+
+from common import plot_init, PlotSaver, prof_evt_map
+from matplotlib.ticker import MultipleLocator
+
+def plot_hist(trace_idx, evt_id):
+    global trace_dir_fmt
+    trace_dir = trace_dir_fmt.format(trace_idx)
+    evt_df_path = f"{trace_dir}/prof.aggr.evt{evt_id}.csv"
+    evt_df = pd.read_csv(evt_df_path)
+
+    bins = np.arange(0, 25, 0.25)
+    data_hist = evt_df["time_us"] / 1e3
+    hist, bin_edges = np.histogram(data_hist, bins=bins)
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+    ax.hist(data_hist, bins, zorder=2)
+    ax.set_title(f"Evt Time Distrib - Trace {trace_idx}, Evt {evt_id}")
+    ax.set_xlabel("Time (ms)")
+    ax.set_ylabel("Counts")
+
+    ax.yaxis.set_major_locator(MultipleLocator(10 * 1e6))
+    ax.yaxis.set_minor_locator(MultipleLocator(2 * 2e6))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.1f}M".format(x/1e6))
+
+    fig.tight_layout()
+    plot_fname = f"hist.evt{evt_id}.profile{trace_idx}"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+def run():
+    plot_hist(31, 0)
+    plot_hist(31, 1)
+
+if __name__ == "__main__":
+    global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/profile{}"
+    run()
diff --git a/scripts/tau_analysis/analyze_recurrence.py b/scripts/tau_analysis/analyze_recurrence.py
new file mode 100644
index 0000000..4f56588
--- /dev/null
+++ b/scripts/tau_analysis/analyze_recurrence.py
@@ -0,0 +1,161 @@
+import glob
+import matplotlib.pyplot as plt
+import multiprocessing
+import numpy as np
+import pandas as pd
+import subprocess
+import sys
+import ray
+import traceback
+from typing import Tuple, Dict
+
+from common import label_map
+from trace_reader import TraceOps
+
+
+def row_to_idx_ktiles(row, k):
+    r_widx = enumerate(row)
+    r_sorted_idx = list(map(lambda x: x[0], sorted(r_widx, key=lambda x: x[1])))
+    r_ksplits = np.array_split(r_sorted_idx, k)
+    r_ksplit_set = [set(r) for r in r_ksplits]
+    return r_ksplit_set
+
+
+def compute_olap_pct(s1, s2):
+    try:
+        assert len(s1) == len(s2)
+    except AssertionError as e:
+        print("ERROR: {} != {}".format(len(s1), len(s2)))
+        return
+
+    scomm = s1.intersection(s2)
+    return len(scomm) * 1.0 / len(s1)
+
+
+def compute_olap_series(s):
+    olap_scores = []
+    for idx in range(1, len(s)):
+        prev = s[idx - 1]
+        cur = s[idx]
+        olappct = compute_olap_pct(prev, cur)
+        olap_scores.append(olappct)
+
+    return olap_scores
+
+
+def compute_recurrence_mat(mat, k):
+    matl = mat.tolist()
+    matl_rsplits = map(lambda x: row_to_idx_ktiles(x, k), matl)
+
+    # [ts][ktile] = {set} to
+    # [ktile][ts] = {set}
+    ktile_data = list(zip(*matl_rsplits))
+    ktile_olaps = list(map(compute_olap_series, ktile_data))
+
+    return ktile_olaps
+
+
+def plot_ktile_all(k, all_ktiles, mat_label, plotdir, save=False):
+    print(k, mat_label, plotdir)
+
+    mat_label_fs = mat_label.replace(":", "_")
+    plot_name = "{}.k{}.allk.pdf".format(mat_label_fs, k)
+    plot_fullpath = "{}/{}".format(plotdir, plot_name)
+    print("Generating {}".format(plot_fullpath))
+
+    fig, ax = plt.subplots(1, 1)
+
+    data_x = range(len(all_ktiles[0]))
+
+    for ktidx, ktile in enumerate(all_ktiles):
+        label = "K-Tile {}".format(ktidx)
+        print(label)
+        ax.plot(data_x, ktile, label=label, alpha=0.8)
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Sim-Pct Vs Prev TS")
+    ax.set_title("All K-Tiles (Evt {}, K={})".format(label_map[mat_label], k))
+
+    ax.set_ylim([0, 1])
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f}%".format(x * 100))
+
+    if (k <= 8)
+        ax.legend()
+
+    if save:
+        fig.savefig(plot_fullpath, dpi=300)
+    else:
+        fig.show()
+
+
+def plot_ktile_top(all_ks, all_top_ktiles, mat_label, plotdir, save=False):
+    mat_label_fs = mat_label.replace(":", "_")
+    num_ks = len(all_ks)
+    plot_name = "{}.topktiles.n{}.pdf".format(mat_label_fs, num_ks)
+    plot_fullpath = "{}/{}".format(plotdir, plot_name)
+    print("Generating {}".format(plot_fullpath))
+
+    fig, ax = plt.subplots(1, 1)
+
+    data_x = range(len(all_top_ktiles[0]))
+
+    for k, top_ktile in zip(all_ks, all_top_ktiles):
+        label = "Top {}%".format(100.0/k)
+        print(label)
+        ax.plot(data_x, top_ktile, label=label, alpha=0.8)
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Sim-Pct Vs Prev TS")
+    ax.set_title("Top K-Tiles For Evt {}".format(label_map[mat_label]))
+
+    ax.set_ylim([0, 1])
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f}%".format(x * 100))
+
+    ax.legend()
+
+    if save:
+        fig.savefig(plot_fullpath, dpi=300)
+    else:
+        fig.show()
+    pass
+
+
+def plot_recurrence_suite(mat, mat_label, plotdir):
+    all_k = [4, 8, 16, 32]
+    all_k_olaps = list(map(lambda k: compute_recurrence_mat(mat, k), all_k))
+    all_top_olaps = list(map(lambda x: x[-1], all_k_olaps))
+
+
+    #  for k, k_olap in zip(all_k, all_k_olaps):
+        #  plot_ktile_all(k, k_olap, mat_label, plotdir, save=True)
+
+    for num_k in [2, 3, 4]:
+        all_k_toplot = all_k[:num_k]
+        all_top_olaps_toplot = all_top_olaps[:num_k]
+        plot_ktile_top(all_k_toplot, all_top_olaps_toplot, mat_label, plotdir, save=True)
+
+
+def run_recurrence(tracedir: str, plotdir: str) -> None:
+    print(tracedir)
+    tr = TraceOps(tracedir)
+
+    mat_labels = ["tau:AR1", "tau:SR", "tau:AR2", "tau:AR3-AR3_UMBT"]
+    mats = [tr.multimat(l) for l in mat_labels]
+
+    for mat_label, mat in zip(mat_labels, mats):
+        plot_recurrence_suite(mat, mat_label, plotdir)
+    #  ar1 = tr.multimat("tau:AR1")
+    #  sr = tr.multimat("tau:SR")
+    #  ar2 = tr.multimat("tau:AR2")
+    #  ar3 = tr.multimat("tau:AR3-AR3_UMBT")
+    pass
+
+
+def run():
+    tracedir = "/mnt/ltio/parthenon-topo/profile8"
+    plotdir = "figures/20220830"
+    run_recurrence(tracedir, plotdir)
+
+
+if __name__ == "__main__":
+    run()
diff --git a/scripts/tau_analysis/analyze_taskflow.py b/scripts/tau_analysis/analyze_taskflow.py
index 0432a22..df68595 100644
--- a/scripts/tau_analysis/analyze_taskflow.py
+++ b/scripts/tau_analysis/analyze_taskflow.py
@@ -532,9 +532,10 @@ if __name__ == "__main__":
     In case an analysis is failing at some intermediate timestep,
     this should be re-enabled."""
 
-    trace_dir = "/mnt/ltio/parthenon-topo/profile17"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile22"
+    trace_dir = "/mnt/ltio/parthenon-topo/stochsg1"
     #  run_classify(trace_dir)
     # XXX: run_parse_log probably not necessary
     # generates logstats.csv, but log.txt.csv already generated by phoebus_runs.sh
-    run_parse_log(trace_dir)
-    #  run_aggregate(trace_dir)
+    #  run_parse_log(trace_dir)
+    run_aggregate(trace_dir)
diff --git a/scripts/tau_analysis/analyze_taskflow_2.py b/scripts/tau_analysis/analyze_taskflow_2.py
new file mode 100644
index 0000000..6536da0
--- /dev/null
+++ b/scripts/tau_analysis/analyze_taskflow_2.py
@@ -0,0 +1,117 @@
+import matplotlib.pyplot as plt
+import numpy as np
+import pandas as pd
+import ray
+
+from analyze_taskflow import find_func
+
+from trace_reader import TraceOps
+from task import Task
+
+ray.init(address="h0:6379")
+
+
+def missing_gaps(df):
+    df_mo = df[(df["func"] == "MakeOutputs") & (df["enter_or_exit"] == 0)]
+
+    diff_total = 0
+
+    for row_idx, row in df_mo.iterrows():
+        prev_row = df.loc[row_idx - 1]
+        prev_ts = prev_row["timestamp"]
+        cur_ts = row["timestamp"]
+        diff_ts = cur_ts - prev_ts
+        diff_total += diff_ts
+
+    return diff_total
+
+
+def compute_fdtime_inlb(df):
+    df_lb = df[df["func"] == "LoadBalancingAndAdaptiveMeshRefinement"]
+    df_lb = df_lb.astype({"enter_or_exit": int}).copy()
+
+    df_lb_op = df_lb[df_lb["enter_or_exit"] == 0]
+    df_lb_cl = df_lb[df_lb["enter_or_exit"] == 1]
+    idx_op = df_lb_op.index.tolist()
+    idx_cl = df_lb_cl.index.tolist()
+    lb_pair = list(zip(idx_op, idx_cl))
+
+    fd_time_tot = 0
+
+    for lb in lb_pair:
+        df_lb = df.iloc[lb[0] : lb[1] + 1]
+        fd_time = find_func(df_lb, "Task_FillDerived")
+        for fd_pair in fd_time:
+            pair_diff = fd_pair[1] - fd_pair[0]
+            fd_time_tot += pair_diff
+
+    return fd_time_tot
+
+
+@ray.remote
+def analyze_fd(args):
+    rank = args["rank"]
+
+    tr = TraceOps(trace_dir)
+    df = tr.trace.read_rank_trace(rank).dropna().astype({"enter_or_exit": int})
+
+    time_gap = missing_gaps(df)
+    time_fd = compute_fdtime_inlb(df)
+
+    return time_gap, time_fd
+
+
+class MissingGapCalc(Task):
+    def __init__(self, trace_dir):
+        super().__init__(trace_dir)
+
+    @staticmethod
+    def worker(fn_args):
+        return find_missing_gaps(fn_args)
+
+    """ Gap between the function Task_EstimateTimestep and MakeOutputs
+    in AR3_LB region. Presumably hides a barrier that some ranks spend
+    time waiting at. This gap is inversely proportional to the time
+    spent in FillDerived calls in AR3_LB. This plot visually confirms that.
+    """
+
+    @staticmethod
+    def plot_gap_vs_fd(data, plot_dir):
+        fig, ax = plt.subplots(1, 1)
+
+        data = list(zip(*data))
+        data_gap = np.array(data[0])
+        data_fd = np.array(data[1])
+        data_x = range(len(data_gap))
+
+        ax.plot(data_x, data_gap, label="Gap:ETS-MO")
+        ax.plot(data_x, data_fd, label="Time:T_FD")
+
+        ax.plot(data_x, data_gap + data_fd, label="Sum Of Both")
+
+        ax.legend()
+
+        ax.set_xlabel("Rank")
+        ax.set_ylabel("Time")
+        ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+
+        plot_path = "{}/gap_vs_fd.pdf".format(plot_dir)
+        fig.savefig(plot_path)
+
+
+def run_missing_gaps():
+    trace_dir = "/mnt/ltio/parthenon-topo/profile8"
+    mgc = MissingGapCalc(trace_dir)
+    #  ret = mgc.run_rankwise(0, 512)
+    data = mgc.run_rank(0)
+    data = mgc.run_func_with_ray(analyze_fd)
+    plot_dir = "figures/other"
+    mgc.plot_gap_vs_fd(data, plot_dir)
+
+
+def run():
+    run_missing_gaps()
+
+
+if __name__ == "__main__":
+    run()
diff --git a/scripts/tau_analysis/common.py b/scripts/tau_analysis/common.py
new file mode 100644
index 0000000..dae7a4e
--- /dev/null
+++ b/scripts/tau_analysis/common.py
@@ -0,0 +1,158 @@
+import glob as glob
+import matplotlib.figure as pltfig
+import matplotlib.pyplot as plt
+import re
+
+from typing import Union
+
+label_map = {
+    "tau:AR1": "$FC_{CN}$",
+    "tau:SR": "$BC_{NO}$",
+    "tau:AR2": "$FD_{CO}$",
+    "tau:AR3_UMBT": "$AG_{NO}$",
+    "tau:AR3-AR3_UMBT": "$LB_{NO}$",
+    "tau:AR3_LB": "$LB_{NO}$",
+    "tau:AR3": "$AG+LB_{NO}$",
+    "msgcnt:BoundaryComm": "MsgCount (BC)",
+    "msgsz:BoundaryComm": "MsgSize (BC)",
+    "npeer:BoundaryComm": "NumPeers (BC)",
+    "rcnt:": "Load",
+}
+
+profile_label_map = {
+    "profile10": "parth-baseline-1",
+    "profile22": "parth-baseline-2",
+    "profile23": "lpt-buggy-noderef",
+    "profile24": "lpt-fixed-derefs",
+    "profile25": "lpt-fixed-derefs",
+    "profile26": "lpt-fixed-derefs",
+    "profile28": "lpt-fixed-costs",
+    "profile29": "lpt-lb-onlyB",
+    "profile30": "contig-improved",
+    "profile31": "parth-baseline-nomsglog",
+    "profile32": "lpt-fixed-costs-nomsglog",
+    "profile33": "contig-improved-nomsglog",
+    "profile34": "parth-baseline-nomsglog-2",
+    "profile35": "lpt-fixed-costs-nomsglog-2",
+    "profile36": "contig-improved-nomsglog-2",
+    "profile37": "parth-upd-noout-baseline",
+    "profile38": "parth-upd-noout-lpt",
+    "profile39": "parth-upd-noout-contig++",
+    "profile40": "parth-upd-outnderef-baseline",
+    "profile41": "parth-upd-outnderef-lpt",
+    "profile42": "parth-upd-outnderef-contig++",
+    "profile43": "parth-upd-outnderef-cppiter",
+    "profile44": "parth-upd-outnderef-cppiter-lb1k",
+    "burgers1": "parth-vibe-baseline",
+    "burgers2": "parth-vibe-bl-packsz1",
+    "athenapk1": "glxcool-baseline",
+    "athenapk2": "glxcool-pack1",
+    "athenapk4": "glxcool-p1-100k",
+    "athenapk5": "glxcool-p1-thrott0",
+    "athenapk7": "glxcool-postopt",
+    "athenapk10": "glxcool-postopt-slot64",
+    "athenapk11": "glxcool-postopt-slot64-2",
+    "athenapk12": "glxcool-postopt-slot64-3",
+    "athenapk13": "glxcool-postopt-slot64-4",
+    "athenapk14": "glxcool-postopt-baseline",
+    "athenapk15": "glxcool-postopt-lpt",
+    "athenapk16": "glxcool-postopt-cpp",
+    "stochsg2": "StochSG - Test"
+}
+
+prof_evt_map = {"0": "FluxDivergence", "1": "CalculateFluxes", "0+1": "FD+CF"}
+
+
+def get_label(key: str):
+    if key in label_map:
+        return label_map[key]
+
+    key = f"tau:{key}"
+    return label_map[key]
+
+
+def plot_init():
+    SMALL_SIZE = 12
+    MEDIUM_SIZE = 14
+    BIGGER_SIZE = 26
+
+    plt.rc("font", size=SMALL_SIZE)  # controls default text sizes
+    plt.rc("axes", titlesize=SMALL_SIZE)  # fontsize of the axes title
+    plt.rc("axes", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels
+    plt.rc("xtick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("ytick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("legend", fontsize=SMALL_SIZE)  # legend fontsize
+    plt.rc("figure", titlesize=BIGGER_SIZE)  # fontsize of the figure title
+
+
+def plot_init_big():
+    SMALL_SIZE = 15
+    MEDIUM_SIZE = 16
+    BIGGER_SIZE = 22
+
+    plt.rc("font", size=SMALL_SIZE)  # controls default text sizes
+    plt.rc("axes", titlesize=BIGGER_SIZE)  # fontsize of the axes title
+    plt.rc("axes", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels
+    plt.rc("xtick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("ytick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("legend", fontsize=SMALL_SIZE)  # legend fontsize
+    plt.rc("legend", fontsize=14)  # legend fontsize
+    plt.rc("figure", titlesize=BIGGER_SIZE)  # fontsize of the figure title
+
+
+def plot_init_print():
+    SMALL_SIZE = 16
+    MEDIUM_SIZE = 18
+    BIGGER_SIZE = 20
+
+    plt.rc(
+        "font", size=SMALL_SIZE
+    )  # controls default text sizes plt.rc("axes", titlesize=SMALL_SIZE)  # fontsize of the axes title
+    plt.rc("axes", labelsize=MEDIUM_SIZE)  # fontsize of the x and y label
+    plt.rc("xtick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("ytick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("legend", fontsize=SMALL_SIZE)  # legend fontsize
+    plt.rc("figure", titlesize=BIGGER_SIZE)  # fontsize of the figure title
+
+
+def plot_dir_latest() -> str:
+    all_dirs = glob.glob("figures/202*")
+    def get_key(x: str) -> int:
+        mobj = re.search(r"202[0-9]+", x)
+        if mobj:
+            return int(mobj.group(0))
+        else:
+            return -1
+
+    dir_latest = max(all_dirs, key=get_key)
+    return dir_latest
+
+
+class PlotSaver:
+    @staticmethod
+    def save(fig: pltfig.Figure, trpath: Union[str, None], fpath: Union[str, None], fname: str):
+        PlotSaver._save_to_fpath(
+            fig, trpath, fpath, fname, ext="png", show=False)
+
+    @staticmethod
+    def _save_to_fpath(
+        fig: pltfig.Figure, trpath: Union[str, None], fpath: Union[str, None], fname: str, ext="png", show=True
+    ):
+        trpref = ""
+        if trpath is not None:
+            if "/" in trpath:
+                trpref = trpath.split("/")[-1] + "_"
+            elif len(trpath) > 0:
+                trpref = f"{trpath}_"
+
+        if fpath is None:
+            fpath = plot_dir_latest()
+
+        full_path = f"{fpath}/{trpref}{fname}.{ext}"
+
+        if show:
+            print(f"[PlotSaver] Displaying figure\n")
+            fig.show()
+        else:
+            print(f"[PlotSaver] Writing to {full_path}\n")
+            fig.savefig(full_path, dpi=300)
diff --git a/scripts/tau_analysis/plot_analyses_v2.py b/scripts/tau_analysis/plot_analyses_v2.py
new file mode 100644
index 0000000..879b575
--- /dev/null
+++ b/scripts/tau_analysis/plot_analyses_v2.py
@@ -0,0 +1,109 @@
+import ipdb
+import matplotlib.pyplot as plt
+import numpy as np
+import pandas as pd
+import pickle
+
+from common import plot_init, PlotSaver
+from matplotlib.ticker import AutoMinorLocator, FuncFormatter, MultipleLocator
+from trace_reader import TraceReader, TraceOps
+
+
+def compute_straggler_savings(trace_dir):
+    tr = TraceOps(trace_dir)
+    ag_mat = tr.multimat("tau:AR3_UMBT")
+    ag_mat.sort(axis=1)
+    # all_time_spent has 512 rows. each row has 30k cols
+    # each row is the time spent by the ith most straggling rank on UMBT
+    # this time is negative
+    # this is time saved by preventing that rank from straggling
+
+    #  mean_time_spent = np.mean(ag_mat, axis=1)
+    #  all_time_spent = ag_mat.T - mean_time_spent
+
+    ts_rank_savings = np.diff(ag_mat)
+    rank_ts_savings = ts_rank_savings.T
+    rank_ts_savings = np.cumsum(rank_ts_savings, axis=1)
+    rank_savings = np.cumsum(rank_ts_savings[:, -1])
+
+    return rank_ts_savings, rank_savings
+
+
+def plot_straggler_savings_aggr(rank_savings, trace_dir):
+    nitems = 100
+    data_x = list(range(100))
+    data_y = rank_savings[:100]
+
+    fig, ax = plt.subplots(1, 1)
+
+    ax.plot(data_x, data_y)
+    ax.set_xlabel("Stragglers fixed (out of 512)")
+    ax.set_ylabel("Time saved (s)")
+    trpref = trace_dir.split("/")[-1]
+    ax.set_title(f"Impact of Straggler Mitigation on Total Runtime ({trpref})")
+    ax.set_ylim(bottom=0)
+    ax.yaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    )
+    ax.yaxis.set_minor_locator(AutoMinorLocator(n=4))
+
+    ax.yaxis.grid(visible=True, which='major', color='#aaa')
+    ax.yaxis.grid(visible=True, which='minor', color='#ddd')
+    ax.xaxis.grid(visible=True, which='major', color='#aaa')
+
+    fig.tight_layout()
+
+    PlotSaver.save(fig, trace_dir, None, "straggler_impact_aggr")
+
+
+def plot_straggler_savings_tswise(rank_ts_savings, trace_dir):
+    ranks_to_plot = [0, 1, 2, 5, 10, 20, 50]
+
+    data_x = list(range(rank_ts_savings.shape[1]))
+
+    fig, ax = plt.subplots(1, 1)
+    for rank in ranks_to_plot:
+        label = f"Rank {rank}"
+        data_y = rank_ts_savings[rank]
+        ax.plot(data_x, data_y, label=label)
+
+    ax.legend()
+    ax.set_xlabel("Simulation Timestep")
+    ax.set_ylabel("Cumulative Time Saved (s)")
+    trpref = trace_dir.split("/")[-1]
+    ax.set_title(f"Impact of Straggler Mitigation on Total Runtime ({trpref})")
+
+    ax.set_ylim(bottom=0)
+    ax.yaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f} s".format(x / 1e6))
+    )
+    ax.yaxis.set_minor_locator(AutoMinorLocator(n=4))
+
+    ax.yaxis.grid(visible=True, which='major', color='#aaa')
+    ax.yaxis.grid(visible=True, which='minor', color='#ddd')
+    ax.xaxis.grid(visible=True, which='major', color='#aaa')
+
+    fig.tight_layout()
+
+    PlotSaver.save(fig, trace_dir, None, "straggler_impact_tswise")
+
+
+def plot_straggler_savings(trace_dir):
+    rank_ts_savings, rank_savings = compute_straggler_savings(trace_dir)
+    plot_straggler_savings_aggr(rank_savings, trace_dir)
+    plot_straggler_savings_tswise(rank_ts_savings, trace_dir)
+    pass
+
+
+def run():
+    plot_init()
+
+    trace_dir = "/mnt/ltio/parthenon-topo/profile10"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile8"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile11"
+    print(f"Plotting trace: {trace_dir}")
+    plot_straggler_savings(trace_dir)
+
+
+if __name__ == "__main__":
+    run()
diff --git a/scripts/tau_analysis/plot_policy_eval.py b/scripts/tau_analysis/plot_policy_eval.py
new file mode 100644
index 0000000..b0d3a1f
--- /dev/null
+++ b/scripts/tau_analysis/plot_policy_eval.py
@@ -0,0 +1,851 @@
+import matplotlib.pyplot as plt
+import matplotlib.colors as colors
+from matplotlib.ticker import FuncFormatter, MultipleLocator, LogLocator
+import multiprocessing
+import numpy as np
+import pandas as pd
+import glob
+import os
+import re
+import struct
+
+from common import plot_init_big as plot_init, PlotSaver, label_map, get_label
+from scipy.ndimage import gaussian_filter1d
+from trace_reader import TraceReader, TraceOps
+from typing import Dict, List
+
+
+def plot_bar():
+    excess_cost = [859.96, 689.58, 663.81, 540.00, 335.86]
+    avg_cost = [2089.75, 2088.49, 2088.49, 2088.49, 2088.49]
+    max_cost = [2949.72, 2778.07, 2752.30, 2628.49, 2424.35]
+    loc_cost = [33.41, 33.28, 100.26, 295.26, 294.97]
+
+    names = ["Contiguous/UC", "Contiguous/AC",
+             "RoundRobin/AC", "SPT/AC", "LPT/AC"]
+
+    del excess_cost[3]
+    del avg_cost[3]
+    del max_cost[3]
+    del loc_cost[3]
+    del names[3]
+
+    data_x = np.arange(len(names))
+
+    fig, ax = plt.subplots(1, 1)
+    width = 0.25
+
+    ax.bar(data_x + width * 0, excess_cost,
+           width, label="Excess Cost", zorder=2)
+    ax.bar(data_x + width * 1, avg_cost, width, label="Avg Cost", zorder=2)
+    ax.bar(data_x + width * 2, max_cost, width, label="Max Cost", zorder=2)
+
+    ax.set_xticks(data_x + width, names, rotation=10)
+    ax.set_xlabel("Policy")
+    ax.set_ylabel("Time (seconds)")
+
+    ax.set_ylim(bottom=0)
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x))
+    ax.legend(ncol=3)
+
+    ax.yaxis.set_major_locator(MultipleLocator(500))
+    ax.yaxis.set_minor_locator(MultipleLocator(125))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    fig.tight_layout()
+
+    plot_fname = "policy.sim.barplot"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+    pass
+
+
+def plot_bar_2():
+    excess_cost = [859.96, 689.58, 663.81, 540.00, 335.86]
+    avg_cost = [2089.75, 2088.49, 2088.49, 2088.49, 2088.49]
+    max_cost = [2949.72, 2778.07, 2752.30, 2628.49, 2424.35]
+    loc_cost = [33.41, 33.28, 100.26, 295.26, 294.97]
+
+    names = ["Contiguous/UC", "Contiguous/AC",
+             "RoundRobin/AC", "SPT/AC", "LPT/AC"]
+
+    del excess_cost[3]
+    del avg_cost[3]
+    del max_cost[3]
+    del loc_cost[3]
+    del names[3]
+
+    data_x = np.arange(len(names))
+
+    fig, ax = plt.subplots(1, 1)
+    width = 0.5
+
+    #  ax.bar(data_x + width*0, excess_cost, width, label="Excess Cost", zorder=2)
+    #  ax.bar(data_x + width*1, avg_cost, width, label="Avg Cost", zorder=2)
+    #  ax.bar(data_x + width*2, max_cost, width, label="Max Cost", zorder=2)
+
+    ax.bar(data_x + width * 0, loc_cost, width,
+           label="Locality Cost", zorder=2)
+
+    ax.set_xticks(data_x + width * 0, names, rotation=10)
+    ax.set_xlabel("Policy")
+    ax.set_ylabel("Locality Cost (Lower = Better)")
+
+    ax.set_ylim(bottom=0)
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} %".format(x))
+    #  ax.legend()
+
+    ax.yaxis.set_major_locator(MultipleLocator(50))
+    ax.yaxis.set_minor_locator(MultipleLocator(12.5))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    fig.tight_layout()
+
+    plot_fname = "policy.sim.barplot2"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+    pass
+
+
+def concat_lbsim_ilp(trace_dir: str, num_shards: int):
+    files = glob.glob(trace_dir + "/lb_sim/*.csv")
+
+    def shard_match(x): return re.match(
+        r"^ilp_actual_cost_shard_(\d+)_" +
+        str(num_shards) + "\.csv", x.split("/")[-1]
+    )
+
+    matching_files = [f for f in files if shard_match(f)]
+    matching_files = sorted(
+        matching_files, key=lambda x: int(shard_match(x).group(1)))
+
+    all_dfs = [pd.read_csv(f, index_col=None) for f in matching_files]
+
+    prev_df_ts_end = -1
+    for df in all_dfs:
+        df["ts"] += prev_df_ts_end + 1
+        prev_df_ts_end = max(df["ts"])
+        print(df)
+    df_concat = pd.concat(all_dfs)
+    print(df_concat)
+
+    df_concat.to_csv(f"{trace_dir}/lb_sim/ilp_actual_cost.csv", index=None)
+
+
+def get_loc_score_ref(line_str):
+    line_arr = line_str.strip("\n,").split(",")
+    line_arr = np.array([int(i) for i in line_arr])
+
+    loc_score = 0
+
+    for bidx in range(0, len(line_arr) - 1):
+        p = line_arr[bidx]
+        q = line_arr[bidx + 1]
+
+        pn = p / 16
+        qn = q / 16
+
+        if p == q:
+            pass
+        elif abs(p - q) == 1:
+            loc_score += 1
+        elif pn == qn:
+            loc_score += 2
+        else:
+            loc_score += 3
+
+    disorder = loc_score
+    arr_len = len(line_arr)
+    norm_disorder = disorder / arr_len
+    return norm_disorder
+
+
+def get_loc_score(line_str):
+    line_arr = line_str.strip("\n,").split(",")
+    line_arr = np.array([int(i) for i in line_arr])
+    disorder = np.sum(np.abs(np.diff(line_arr)))
+    arr_len = len(line_arr)
+    norm_disorder = disorder / arr_len
+    return norm_disorder
+
+
+def get_loc_scores(fpath: str):
+    f = open(fpath, "r").readlines()
+    assignments = f[1::2]
+    loc_scores = np.array(list(map(get_loc_score_ref, assignments)))
+    return loc_scores
+
+
+def read_lbsim_loc(trace_dir: str):
+    files = glob.glob(trace_dir + "/block_sim/*.det")
+    files = [f for f in files if "shard" not in f]
+
+    fdata = None
+    with multiprocessing.Pool(8) as p:
+        fdata = list(p.map(get_loc_scores, files))
+
+    data = {}
+    for f, loc_scores in zip(files, fdata):
+        print(f"Reading Loc Scores: {f}")
+        policy_name = get_policy_abbrev(f)
+        data[policy_name] = loc_scores
+
+    return data
+
+
+def get_policy_abbrev(fpath: str):
+    fname = os.path.basename(fpath)
+    fname = fname.replace(".csv", "").split("_")
+
+    abbrev_1 = fname[0]
+    abbrev_2 = "".join([x[0] for x in fname[1:]])
+
+    return f"{abbrev_1}_{abbrev_2}"
+
+
+def read_lbsim(trace_dir: str):
+    files = glob.glob(trace_dir + "/block_sim/*.summ")
+    files = [f for f in files if "shard" not in f]
+
+    data = {}
+    for f in files:
+        try:
+            df = pd.read_csv(f)
+            df["max_us"] /= 1000
+            df["avg_us"] /= 1000
+
+            policy_name = get_policy_abbrev(f)
+            print(f"Read policy: {policy_name}")
+
+            data[policy_name] = df
+        except Exception as e:
+            print(f"FAILED: lbsim {f}")
+
+    return data
+
+
+def aggr_lbsim(lbsim_data, loc_data, keys):
+    avg_cost = {}
+    max_cost = {}
+    loc_cost = {}
+
+    for k in lbsim_data:
+        df = lbsim_data[k]
+        avg_cost[k] = df["avg_us"].sum()
+        max_cost[k] = df["max_us"].sum()
+        if loc_data:
+            loc_cost[k] = np.mean(loc_data[k])
+
+    avg_array = order_dict_vals(avg_cost, keys)
+    max_array = order_dict_vals(max_cost, keys)
+    loc_array = order_dict_vals(loc_cost, keys)
+
+    return avg_array, max_array, loc_array
+
+
+def order_dict_vals(d, keys):
+    return [d[k] for k in keys]
+
+
+def get_policy_names():
+    names = [
+        "actual_ac",
+        "contiguous_ec",
+        "lpt_ec",
+        "kcontigimproved_ec",
+        "cppiter_ec",
+        "ilp_1ac",
+    ]
+
+    names = [
+        "actual_ac",
+        "contiguous_ec",
+        "lpt_ec",
+        "kcontigimproved_ec",
+        "cppiter_ec",
+    ]
+
+    return names
+
+
+def plot_lbsim_excess(lbsim_data):
+    #  fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+    fig, ax = plt.subplots(1, 1)
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (ms)")
+    ax.set_ylim([0, 200])
+
+    ax.set_title("(Max-Avg) compute time for different policies")
+
+    ax.yaxis.set_major_locator(MultipleLocator(20))
+    ax.yaxis.set_minor_locator(MultipleLocator(5))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} ms".format(x))
+
+    plot_fname = "policy.sim.ts.sq"
+
+    idx = 0
+    for pol_name in get_policy_names():
+        df = lbsim_data[pol_name]
+        data_x = df.index
+        data_y = df["max_us"] - df["avg_us"]
+        ax.plot(data_x, data_y, label=pol_name, alpha=0.8)
+
+        frame_fname = plot_fname + f"_frame{idx}"
+        ax.legend(ncol=1)
+        if idx == 0:
+            fig.tight_layout()
+        PlotSaver.save(fig, "", None, frame_fname)
+
+        idx += 1
+
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def plot_lbsim_time_saved_cumul(lbsim_data):
+    x = lbsim_data["actual_ac"]["max_us"] - lbsim_data["lpt_ec"]["max_us"]
+    y = lbsim_data["actual_ac"]["max_us"] - \
+        lbsim_data["kcontigimproved_ec"]["max_us"]
+
+    lpt_cum = (x.cumsum() / 1e3).astype(int)
+    cpp_cum = (y.cumsum() / 1e3).astype(int)
+
+    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
+    ax.plot(range(len(lpt_cum)), lpt_cum, label="LPT", zorder=2)
+    ax.plot(range(len(cpp_cum)), cpp_cum, label="Contig++", zorder=2)
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Cumulative Savings (s)")
+    ax.set_title("Cumulative Impact of Policy on Compute (simulated)")
+
+    ax.xaxis.set_major_formatter(lambda x, pos: "{:.0f}K".format(x / 1e3))
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f}K s".format(x / 1e3))
+
+    ylim, ymaj, ymin = 10000, 1000, 200
+    xmaj = 10000
+    ax.xaxis.set_major_locator(MultipleLocator(xmaj))
+    ax.set_ylim([0, ylim])
+    ax.yaxis.set_major_locator(MultipleLocator(ymaj))
+    ax.yaxis.set_minor_locator(MultipleLocator(ymin))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    ax.legend()
+
+    fig.tight_layout()
+
+    plot_fname = "policy.sim.ts.cumsave"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+    pass
+
+
+def plot_lbsim_actual(lbsim_data: dict[str, pd.DataFrame], plot_fname: str):
+    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
+    #  fig, ax = plt.subplots(1, 1)
+
+    cidx = 0
+    for pol_name, df in lbsim_data.items():
+        data_x = df.index
+        data_y = df["avg_us"]
+        ax.plot(data_x, data_y, linestyle="--", color=f"C{cidx}", alpha=0.3)
+        cidx += 1
+        break
+
+    ylim, ymaj, ymin = 1000, 200, 40
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (ms)")
+    #  ax.set_ylim([0, 80])
+    ax.set_ylim([0, ylim])
+
+    ax.set_title("Max & Avg compute time for different policies")
+
+    ax.yaxis.set_major_locator(MultipleLocator(ymaj))
+    ax.yaxis.set_minor_locator(MultipleLocator(ymin))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} ms".format(x))
+
+    # plot_fname = "policy.sim.ts.wavg.sq"
+
+    idx = 0
+    for pol_name in get_policy_names():
+        df = lbsim_data[pol_name]
+        data_x = df.index
+        data_y = df["max_us"]
+        ax.plot(data_x, data_y, label=pol_name, alpha=0.8)
+
+        frame_fname = plot_fname + f"_frame{idx}"
+        ax.legend(ncol=1)
+        if idx == 0:
+            fig.tight_layout()
+        #  PlotSaver.save(fig, "", None, frame_fname)
+
+        idx += 1
+
+    global trace_dir
+    PlotSaver.save(fig, trace_dir, None, plot_fname)
+
+
+def plot_lbsim_barplot(lbsim_data):
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    names = get_policy_names()
+    avg_cost = []
+    max_cost = []
+    excess_cost = []
+    loc_cost = []
+
+    for name in names:
+        df = lbsim_data[name]
+        df_avg = df["avg_us"].sum() / 1e3
+        avg_cost.append(df_avg)
+
+        df_max = df["max_us"].sum() / 1e3
+        max_cost.append(df_max)
+
+        df_excess = df_max - df_avg
+        excess_cost.append(df_excess)
+
+    print(f"Avg cost: {avg_cost}")
+    print(f"Max cost: {max_cost}")
+    print(f"Excess cost: {excess_cost}")
+
+    width = 0.25
+    data_x = np.arange(len(names))
+    ax.bar(data_x + width * 0, avg_cost, width, label="Avg Cost", zorder=2)
+    ax.bar(data_x + width * 1, max_cost, width, label="Max Cost", zorder=2)
+    ax.bar(data_x + width * 2, excess_cost,
+           width, label="Excess Cost", zorder=2)
+
+    ax.set_xticks(data_x + width, names, rotation=10)
+    ax.set_xlabel("Policy")
+    ax.set_ylabel("Time (seconds)")
+
+    ax.set_ylim(bottom=0)
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} s".format(x))
+    ax.legend(ncol=3)
+
+    ax.yaxis.set_major_locator(MultipleLocator(500))
+    ax.yaxis.set_minor_locator(MultipleLocator(125))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    fig.tight_layout()
+
+    plot_fname = "policy.sim.barplot"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def plot_lbsim_barplot_2(traces: List[int]):
+    names = get_policy_names()
+
+    global trace_dir_fmt
+    trace_dirs = [trace_dir_fmt.format(t) for t in traces]
+    all_data = [read_lbsim(t) for t in trace_dirs]
+    aggr_data = [aggr_lbsim(d, names) for d in all_data]
+
+    all_avg = np.array([x[0] for x in aggr_data])
+    all_max = np.array([x[1] for x in aggr_data])
+
+    all_avg /= 1000
+    all_max /= 1000
+
+    all_excess = all_max - all_avg
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    nbins = len(all_avg[0])
+    data_x = np.array([0, 1])
+    labels_x = ["Parth-Old", "Parth-New"]
+    labels_policy = ["Baseline", "LPT", "Contig++"]
+
+    total_width = 0.7
+    bin_width = total_width / nbins
+
+    for bidx in range(nbins):
+        ax.bar(
+            data_x + bidx * bin_width,
+            all_avg.T[bidx],
+            bin_width * 0.95,
+            zorder=2,
+            color=f"C0",
+        )
+
+        p = ax.bar(
+            data_x + bidx * bin_width,
+            all_excess.T[bidx],
+            bin_width * 0.95,
+            label=labels_policy[bidx],
+            bottom=all_avg.T[bidx],
+            zorder=2,
+            color=f"C{bidx+1}",
+        )
+        bar_data = all_max.T[bidx]
+        bar_labels = ["{:.0f} s".format(x) for x in bar_data]
+
+        ax.bar_label(p, bar_labels, fontsize=12, padding=2)
+
+    ax.set_xlabel("Policy")
+    ax.set_ylabel("Simulated Compute Phase Time (s)")
+    ax.set_title("Simulated Time Comparison: Parth-Old vs Parth-New")
+
+    ax.set_xticks(data_x, labels_x)
+    ax.yaxis.set_major_locator(MultipleLocator(1000))
+    ax.yaxis.set_minor_locator(MultipleLocator(200))
+    ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
+    ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
+    ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 8000])
+
+    ax.legend()
+
+    fig.tight_layout()
+
+    fname = "policy_sim_comp_old_vs_new"
+    PlotSaver.save(fig, "", None, fname)
+
+
+def plot_cluster_sim_mean(trace_dir: str):
+    df = pd.read_csv(
+        f"{trace_dir}/cluster_sim/cluster_sim_mean.csv", header=1, index_col=None
+    )
+    df.columns = ["ts", "n", "k", "mean_rel_error", "max_rel_error"]
+
+    df = df.dropna()
+
+    fig, ax = plt.subplots(1, 1)
+    ax.plot(df["ts"], df["k"], label="Number of clusters (left)",
+            color="C0", alpha=0.7)
+
+    ax2 = ax.twinx()
+    ax2.plot(
+        df["ts"],
+        df["k"] / df["n"],
+        label="Pct of blocks (right)",
+        color="C1",
+        alpha=0.7,
+    )
+
+    fig.legend()
+    ax2.yaxis.set_major_formatter(lambda x, pos: "{:.1f}%".format(x * 100))
+
+    ax.yaxis.set_major_locator(MultipleLocator(20))
+    ax.yaxis.set_minor_locator(MultipleLocator(5))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    ax.set_title("k for mean_error < 1%")
+    ax.set_xlabel("timestep")
+    ax.set_ylabel("Number of clusters")
+    ax2.set_ylabel("Pct of total")
+
+    ax.set_ylim([0, 100])
+    ax2.set_ylim([0, 0.2])
+
+    fig.tight_layout()
+    plot_fname = "cluster.sim.mean"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def get_policy_mat(fname: str) -> np.ndarray:
+    fdata = open(fname, "rb").read()
+
+    nranks = struct.unpack("@i", fdata[0:4])
+    nitems = int((len(fdata) - 4) / 8)
+
+    print(f"Mat len: {nitems}, ranks: {nranks}")
+
+    unpack_fmt = "@{}d".format(nitems)
+    vals = struct.unpack(unpack_fmt, fdata[4:])
+
+    mat = np.reshape(vals, (-1, nranks[0]))
+    print(f"Mat shape: {mat.shape}")
+
+    return mat
+
+
+def norm_mat(mat):
+    mins = np.min(mat, axis=1)
+    mins = np.vstack(mins)
+    mat2 = ((mat - mins) / 1000).astype(int)
+    print("99 percentile: ", np.percentile(mat2, 99))
+    print("95 percentile: ", np.percentile(mat2, 95))
+    return mat2
+
+
+def plot_policy_mat(mat, policy_name):
+    # make log safe, shouldn't be a big deal
+    mat = mat + 1
+    bounds = np.linspace(1, 200, 100)
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="max")
+    print(f"Mat: {mat.min()}, {mat.max()}")
+    norm = colors.LogNorm(vmin=1, vmax=200)
+
+    pretty_name = policy_name.split("_")
+    abbrev = "".join([x[0] for x in pretty_name[1:]]).upper()
+    pretty_name = f"{pretty_name[0].capitalize()} ({abbrev})"
+
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    im = ax.imshow(mat, norm=norm, aspect="auto", cmap="plasma")
+    ax.set_title(f"Excess Time (ms) for Policy: {pretty_name}")
+    ax.set_xlabel("Rank")
+    ax.set_ylabel("Timestep (top to bottom)")
+
+    fig.tight_layout()
+
+    fig.subplots_adjust(left=0.15, right=0.78)
+    cax = fig.add_axes([0.81, 0.12, 0.08, 0.8])
+
+    def cax_fmt(x, pos): return "{:.0f}ms".format(x)
+    fig.colorbar(im, cax=cax, format=FuncFormatter(cax_fmt))
+
+    #  tname = trace_dir.split('/')[-1]
+    global trace_dir
+    fname = f"policymat.{policy_name}"
+    PlotSaver.save(fig, trace_dir, None, fname)
+
+
+def plot_mats_ts_cumul(mats, names):
+    maxes = list(
+        map(lambda m: (np.max(m, axis=1) / 1e6).cumsum().astype(int), mats))
+    maxes = list(map(lambda m: (np.max(m, axis=1) / 1e3).astype(int), mats))
+
+    fig = plt.Figure()
+    ax = fig.subplots(1, 1)
+
+    data_x = np.arange(len(maxes[0]))
+    ax.plot(data_x, maxes[0] - maxes[1], label="LPT vs Baseline")
+    ax.plot(data_x, maxes[0] - maxes[2], label="Contig++ vs Baseline")
+    ax.plot(data_x, maxes[0] - maxes[3], label="CppIter vs Baseline")
+
+    #  for policy, data_y in zip(names, maxes):
+    #  data_x = np.arange(len(data_y))
+    #  label = policy.split('/')[-1].split('_')[0]
+    #  ax.plot(data_x, data_y, label=label)
+
+    ax.set_title("Ts-wise Time Saved vs Policy")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (ms)")
+
+    ax.set_ylim([0, 100])
+
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} ms".format(x))
+
+    ax.legend()
+    fig.tight_layout()
+
+    fname = "policy.cumultime"
+    PlotSaver.save(fig, trace_dir, None, fname)
+    pass
+
+
+def run_plot_policy_mats():
+    global trace_dir
+    policy_glob = f"{trace_dir}/block_sim/*.csv"
+    policy_files = glob.glob(policy_glob)
+    policy_files = [
+        "contiguous_unit_cost",
+        "lpt_extrapolated_cost",
+        "kcontigimproved_extrapolated_cost",
+        "cppiter_extrapolated_cost",
+    ]
+
+    policy_fnames = [
+        f"{trace_dir}/block_sim/{x}.ranksum.csv" for x in policy_files]
+    policy_mats = list(map(get_policy_mat, policy_fnames))
+    norm_mats = list(map(norm_mat, policy_mats))
+
+    for mat, policy_name in zip(norm_mats, policy_files):
+        plot_policy_mat(mat, policy_name)
+
+    plot_mats_ts_cumul(policy_mats, policy_fnames)
+
+
+def humrdstr(x):
+    if x < 1e3:
+        return str(x)
+        pass
+    if x < 1e6:
+        return "{:.3f}".format(x / 1e3).strip("0") + "K"
+        pass
+    if x < 1e9:
+        return "{:.3f}".format(x / 1e6).strip("0") + "M"
+        pass
+    if x < 1e12:
+        return "{:.3f}".format(x / 1e9).strip("0") + "B"
+        pass
+
+
+def plot_scalesim_log():
+    scale_df_path = "/users/ankushj/repos/amr/scripts/tau_analysis/figures/20230718/scalesim.log.csv"
+    df = pd.read_csv(scale_df_path)
+
+    policy_name_map = {
+        'contiguous_unit_cost': 'Baseline',
+        'lpt_actual_cost': 'LPT',
+        'cpp_actual_cost': 'Contiguous-DP',
+        'cpp_iter_actual_cost': 'Contiguous-DP++'
+    }
+
+    df["policy"] = df_policy.apply(policy_name_map)
+
+    df.columns
+
+    fig = plt.figure(figsize=(9, 6))
+    ax = fig.subplots(1, 1)
+
+    policies = df["policy"].unique()
+
+    for policy in policies:
+        df_policy = df[df["policy"] == policy]
+        data_x = df_policy["nblocks"]
+        data_y = df_policy["iter_time"] / 1e3
+
+        ax.plot(data_x, data_y, "-o", label=policy, zorder=2)
+
+    ax.set_xlabel("Number of Simulation Blocks")
+    ax.set_ylabel("Solution Time (ms)")
+    ax.set_title("LB Policy vs Solution Time")
+    ax.legend()
+
+    ax.set_xscale("log")
+    ax.set_yscale("log")
+
+    #  ax.set_ylim(bottom=1)
+    ax.yaxis.set_major_locator(LogLocator(base=10))
+    ax.yaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.4f}".format(x).strip("0") + " ms")
+    )
+    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: humrdstr(x)))
+
+    #  ax.yaxis.set_minor_locator(LogLocator(base=10, subs=(1.0, 10.0)))
+    #  ax.xaxis.set_major_locator(LogLocator(base=2, numticks=20))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    fig.tight_layout()
+
+    plot_fname = "scalesim.plot"
+    PlotSaver.save(fig, "", None, plot_fname)
+    pass
+
+
+def plot_lbsim_tradeoffs(trace_dir: str):
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    keys = get_policy_names()
+
+    lbsim_data = read_lbsim(trace_dir)
+    loc_data = read_lbsim_loc(trace_dir)
+
+    avg_arr, max_arr, loc_arr = aggr_lbsim(lbsim_data, loc_data, keys)
+    avg_arr = np.array(avg_arr)
+    max_arr = np.array(max_arr)
+    loc_arr = np.array(loc_arr) * 100
+    lb_ratios = max_arr * 100 / avg_arr
+
+    for x, y, lab in zip(loc_arr, lb_ratios, keys):
+        print(f"{x} {y} {lab}")
+        ax.plot(x, y, "o", label=lab, ms=13, zorder=2)
+
+    ax.legend()
+    ax.set_ylim([100, 200])
+    ax.set_xlim([0, 300])
+    #  ax.set_xlim(bottom=0)
+
+    ax.xaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f}".format(x)))
+    ax.yaxis.set_major_formatter(
+        FuncFormatter(lambda x, pos: "{:.0f} %".format(x))
+    )
+    ax.yaxis.set_major_locator(MultipleLocator(10))
+    ax.yaxis.set_minor_locator(MultipleLocator(2))
+
+    ax.set_title("Load Balance vs Locality Cost")
+    ax.set_xlabel("Loc Cost (lower the better)")
+    ax.set_ylabel("Load Balance (% of optimal, lower = better)")
+
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    fig.tight_layout()
+
+    plot_fname = "lbsim.tradeoffs"
+    PlotSaver.save(fig, trace_dir, None, plot_fname)
+
+
+def smoothen_series(series: pd.Series, sigma: int = 1):
+    return gaussian_filter1d(series, sigma=sigma)
+
+
+def smoothen_series_roll(series: pd.Series, window: int = 10):
+    return series.rolling(window=window).mean()
+
+
+def smoothen_data(data: Dict[str, pd.DataFrame], sigma: int = 1) -> Dict[str, pd.DataFrame]:
+    data_smooth = {}
+    for k, v in data.items():
+        vcopy = v.copy()
+        vcopy["avg_us"] = smoothen_series(vcopy["avg_us"], sigma)
+        vcopy["max_us"] = smoothen_series(vcopy["max_us"], sigma)
+        data_smooth[k] = vcopy
+
+    return data_smooth
+
+
+def smoothen_data_roll(data: Dict[str, pd.DataFrame], window: int = 10) -> Dict[str, pd.DataFrame]:
+    data_smooth = {}
+    for k, v in data.items():
+        vcopy = v.copy()
+        vcopy["avg_us"] = smoothen_series_roll(vcopy["avg_us"], window)
+        vcopy["max_us"] = smoothen_series_roll(vcopy["max_us"], window)
+        data_smooth[k] = vcopy
+
+    return data_smooth
+
+
+def run_plot():
+    plot_init()
+    #  plot_bar()
+    #  plot_bar_2()
+    global trace_dir
+    #  trace_dir = "/mnt/ltio/parthenon-topo/profile40"
+    #  run_plot_policy_mats()
+
+    #  plot_lbsim_tradeoffs(trace_dir)
+    #  return
+
+    trace_dir = "/mnt/ltio/parthenon-topo/athenapk4"
+    trace_dir = "/mnt/ltio/parthenon-topo/stochsg44"
+    data = read_lbsim(trace_dir)
+    data_smooth = smoothen_data(data, 4)
+    data_smooth = smoothen_data_roll(data, 50)
+    #  plot_lbsim(data)
+    #  plot_lbsim_excess(data)
+    plot_fname = "policy.sim.ts.smooth.roll"
+    plot_lbsim_actual(data_smooth, plot_fname)
+    plot_lbsim_time_saved_cumul(data)
+    #  plot_cluster_sim_mean(trace_dir)
+
+    return
+
+    global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/profile{}"
+    traces = [32, 37]
+    plot_lbsim_barplot_2(traces)
+    plot_scalesim_log()
+
+
+if __name__ == "__main__":
+    run_plot()
diff --git a/scripts/tau_analysis/plot_prof.py b/scripts/tau_analysis/plot_prof.py
new file mode 100644
index 0000000..2673c38
--- /dev/null
+++ b/scripts/tau_analysis/plot_prof.py
@@ -0,0 +1,93 @@
+import matplotlib.pyplot as plt
+from matplotlib.ticker import MultipleLocator
+import numpy as np
+import pandas as pd
+import glob
+import os
+import re
+
+from common import plot_init, PlotSaver, label_map, get_label, profile_label_map
+from trace_reader import TraceReader, TraceOps
+from typing import Dict, List
+
+
+def parse_array(arr):
+    arr = arr.strip("[]").split(",")
+    arr = [int(i) for i in arr]
+    return arr
+
+def read_df(trace_dir, evt_code):
+    df_path = "{trace_dir}/prof.aggr.evt{evt_code}.csv"
+    df = pd.read_csv(df_path)
+    pass
+
+
+def get_nblocks(trace_dir: str):
+    df_path = f"{trace_dir}/prof.aggrmore.evt3.csv"
+    df = pd.read_csv(df_path)
+    df["bl_cnt"] = df["block_id"].apply(lambda x: len(parse_array(x)))
+    nblocks = df["bl_cnt"].to_numpy()
+    #  aggr_df = df.groupby("sub_ts", as_index=False).agg({"block_id": "count"})
+    #  nblocks = aggr_df["block_id"].to_numpy()
+    return nblocks
+
+
+def plot_nblocks(trace_dirs: List[str]):
+    # set a primary trace dir, assumed to be idx 0
+    # used by PlotSaver to name things
+    global trace_dir
+    trace_dir = trace_dirs[0]
+
+    all_nblocks = [get_nblocks(t) for t in trace_dirs]
+    data_x = np.arange(len(all_nblocks[0]))
+
+    fig, ax = plt.subplots(1, 1, figsize=(9, 5))
+
+    for tidx, tdir in enumerate(trace_dirs):
+        data_y = all_nblocks[tidx]
+        label = profile_label_map[os.path.basename(tdir)]
+
+        if tidx == 0:
+            linewidth = 2
+            zorder = 3
+            alpha = 1
+        else:
+            linewidth = 1
+            zorder = 2
+            alpha = 0.6
+
+        ax.plot(
+            data_x, data_y, linewidth=linewidth, label=label, alpha=alpha, zorder=zorder
+        )
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Block Count")
+
+    ax.set_ylim(bottom=0)
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.1f}K".format(x / 1e3))
+
+    ax.yaxis.set_major_locator(MultipleLocator(500))
+    ax.yaxis.set_minor_locator(MultipleLocator(125))
+    plt.grid(visible=True, which="major", color="#999", zorder=0)
+    plt.grid(visible=True, which="minor", color="#ddd", zorder=0)
+
+    fig.tight_layout()
+
+    ax.legend()
+
+    plot_fname = "nblocks"
+    PlotSaver.save(fig, "", None, plot_fname)
+
+
+def run_plot():
+    trace_dir_pref = "/mnt/ltio/parthenon-topo/profile"
+    trace_nums = [23, 27]
+    trace_nums = [37, 38, 39]
+    trace_dirs = [f"{trace_dir_pref}{tnum}" for tnum in trace_nums]
+    plot_nblocks(trace_dirs)
+    pass
+
+
+if __name__ == "__main__":
+    plot_init()
+    run_plot()
diff --git a/scripts/tau_analysis/plot_prof_bid.py b/scripts/tau_analysis/plot_prof_bid.py
new file mode 100644
index 0000000..34b1cd3
--- /dev/null
+++ b/scripts/tau_analysis/plot_prof_bid.py
@@ -0,0 +1,143 @@
+import glob
+import multiprocessing
+import numpy as np
+import pandas as pd
+#  import ipdb
+import matplotlib.pyplot as plt
+import matplotlib.colors as colors
+import pickle
+import re
+import subprocess
+import struct
+import sys
+import time
+import os
+
+#  import ray
+import traceback
+
+from common import plot_init, PlotSaver, prof_evt_map
+from matplotlib.ticker import FuncFormatter, MultipleLocator
+from typing import List, Tuple
+from pathlib import Path
+#  from task import Task
+from trace_reader import TraceReader, TraceOps
+
+from analyze_prof import _fig_make_cax, get_evtmat_by_bid
+
+
+def plot_heatmap(mat, bounds, norm):
+    fig = plt.figure()
+    ax = fig.subplots(1, 1)
+
+    im = ax.imshow(mat, norm=norm, aspect="auto", cmap="plasma")
+    ax.set_title("ABCD")
+    ax.set_xlabel("Rank ID")
+    ax.set_ylabel("Timestep")
+
+    fig.tight_layout()
+
+    cax = _fig_make_cax(fig)
+    cax_fmt = lambda x, pos: "{:.0f} ms".format(x / 1e3)
+    fig.colorbar(im, cax=cax, format=FuncFormatter(cax_fmt))
+
+    return fig, ax
+
+
+def plot_compare_prof(
+    prof_a: int,
+    prof_b: int,
+    evt_code: int,
+    trace_dir_a: str,
+    prof_mat_a: np.ndarray,
+    trace_dir_b: str,
+    prof_mat_b: np.ndarray,
+    clip=0) -> None:
+    if clip:
+        prof_mat_a = prof_mat_a[:, :clip]
+        prof_mat_b = prof_mat_b[:, :clip]
+
+    range_beg = 0
+    mat_1d = np.concatenate([prof_mat_a.flatten(), prof_mat_b.flatten()])
+    range_end = np.percentile(mat_1d, 99)
+
+    bounds = np.linspace(range_beg, range_end, 10)  # 10 bins
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="both")
+
+    fig, ax = plot_heatmap(prof_mat_a, bounds, norm)
+    ax.set_title("Evt{} - Prof{}".format(evt_code, prof_a))
+    plot_fname = "heatmap-evt{}-prof{}".format(evt_code, prof_a)
+    plot_fname = f"{plot_fname}-clip{clip}"
+    PlotSaver.save(fig, trace_dir_a, None, plot_fname)
+
+    fig, ax = plot_heatmap(prof_mat_b, bounds, norm)
+    ax.set_title("Evt{} - Prof{}".format(evt_code, prof_b))
+    plot_fname = "heatmap-evt{}-prof{}".format(evt_code, prof_b)
+    plot_fname = f"{plot_fname}-clip{clip}"
+    PlotSaver.save(fig, trace_dir_b, None, plot_fname)
+
+    prof_mat_diff = prof_mat_a - prof_mat_b
+    prof_mat_diff_1d = np.abs(prof_mat_diff.flatten())
+    diff_max = np.percentile(prof_mat_diff_1d, 99)
+    bounds = np.linspace(-diff_max, diff_max, 10)
+    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend="both")
+
+    fig, ax = plot_heatmap(prof_mat_diff, bounds, norm)
+    ax.set_title("Evt{} - Prof{} minus Prof{}".format(evt_code, prof_a, prof_b))
+    plot_fname = "heatmap-evt{}-prof{}minus{}".format(evt_code, prof_a, prof_b)
+    plot_fname = f"{plot_fname}-clip{clip}"
+    PlotSaver.save(fig, trace_dir_a, None, plot_fname)
+
+
+def analyze_compare_prof(prof_a, prof_b, evt_code):
+    global trace_dir_fmt
+    #  prof_a = 22
+    #  prof_b = 24
+    #  evt_code = 0
+
+    trace_dir_a = trace_dir_fmt.format(prof_a)
+    _, prof_mat_a = get_evtmat_by_bid(trace_dir_a, evt_code)
+
+    trace_dir_b = trace_dir_fmt.format(prof_b)
+    _, prof_mat_b = get_evtmat_by_bid(trace_dir_b, evt_code)
+
+    plot_compare_prof(
+        prof_a,
+        prof_b,
+        evt_code,
+        trace_dir_a,
+        prof_mat_a,
+        trace_dir_b,
+        prof_mat_b,
+        clip=0,
+    )
+
+    pass
+
+def run_iter():
+    for self in [22, 24]:
+        for other in [24, 25, 26, 27]:
+            if self != other:
+                for evtcode in [0, 1]:
+                    yield (self, other, evtcode)
+
+def run_parallel():
+    with multiprocessing.Pool(8) as pool:
+        pool.starmap(analyze_compare_prof, run_iter())
+
+
+def run():
+    analyze_compare_prof(28, 30, 0)
+    analyze_compare_prof(28, 30, 1)
+    return
+
+    for job in run_iter():
+        print(job)
+        analyze_compare_prof(*job)
+
+
+if __name__ == "__main__":
+    global trace_dir_fmt
+    trace_dir_fmt = "/mnt/ltio/parthenon-topo/profile{}"
+    run()
+    #  run_parallel()
diff --git a/scripts/tau_analysis/plot_taskflow.py b/scripts/tau_analysis/plot_taskflow.py
index c378277..04a2048 100644
--- a/scripts/tau_analysis/plot_taskflow.py
+++ b/scripts/tau_analysis/plot_taskflow.py
@@ -1,11 +1,12 @@
 import matplotlib.pyplot as plt
+import matplotlib.colors as colors
 from matplotlib.ticker import MultipleLocator
 import numpy as np
 import pandas as pd
 import pickle
 import ipdb
 
-from common import plot_init, PlotSaver, label_map, get_label
+from common import plot_init, PlotSaver, label_map, get_label, profile_label_map
 from trace_reader import TraceReader, TraceOps
 from typing import Dict, List
 
@@ -829,36 +830,49 @@ def get_rankhours(trace_dir: str, phases: List[str]) -> Dict[str, int]:
     return rh
 
 
-def plot_rankhour_comparison(trace_a: str, trace_b: str):
+def plot_rankhour_comparison(traces: List[str]):
     phases = ["AR1", "SR", "AR2", "AR3_UMBT", "AR3-AR3_UMBT"]
-    phase_query_strs = list(map(lambda x: "aggr:rw:" + x, phases))
-
-    times_a = get_rankhours(trace_a, phases)
-    label_a = trace_a.split("/")[-1]
-
-    times_b = get_rankhours(trace_b, phases)
-    label_b = trace_b.split("/")[-1]
 
     fig, ax = plt.subplots(1, 1)
-
     data_x = np.arange(len(phases))
     labels_x = [get_label(p) for p in phases]
 
-    data_x1 = times_a.keys()
-    data_x2 = times_b.keys()
-    for x1, x2 in zip(data_x1, data_x2):
-        assert x1 == x2
+    all_data_x = []
+    all_prof_labels = []
+    all_labels = []
+
+    total_width = 0.7
+    nbins = len(traces)
+    bin_width = total_width / nbins
+
+    for bidx, trace in enumerate(traces):
+        times = get_rankhours(trace, phases)
 
-    data_y1 = [times_a[p] for p in phases]
-    data_y2 = [times_b[p] for p in phases]
+        label = trace.split("/")[-1]
+        all_labels.append(label)
 
-    width = 0.35
-    ax.bar(data_x - width / 2, data_y1, width, label=label_a, zorder=2)
-    ax.bar(data_x + width / 2, data_y2, width, label=label_b, zorder=2)
+        prof_label = profile_label_map[label]
+        all_prof_labels.append(prof_label)
+
+        # assert all keys appear in same order
+        data_xt = times.keys()
+        all_data_x.append(data_xt)
+        for xi in list(zip(*all_data_x)):
+            assert len(set(xi)) == 1
+
+        data_y = [times[p] for p in phases]
+        data_x_bin = data_x + bidx * bin_width
+
+        p = ax.bar(data_x_bin, data_y, bin_width, label=prof_label, zorder=2)
+        ax.bar_label(
+            p, fmt=lambda x: int(x * 7.03), rotation="vertical", fontsize=10, padding=4
+        )
 
     ax.set_xlabel("Phase Name")
     ax.set_ylabel("Rank-Hours (512 ranks * hrs/rank)")
-    ax.set_title(f"Phase-wise Time Comparison: {label_a} vs {label_b}")
+
+    prof_title = " vs ".join(all_prof_labels)
+    ax.set_title(f"Phase-wise Time Comparison: {prof_title}")
 
     ax.set_xticks(data_x, labels_x)
 
@@ -866,11 +880,14 @@ def plot_rankhour_comparison(trace_a: str, trace_b: str):
     ax.yaxis.grid(which="major", visible=True, color="#bbb", zorder=0)
     ax.yaxis.grid(which="minor", visible=True, color="#ddd", zorder=0)
     ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 500])
 
     ax.legend()
 
     fig.tight_layout()
-    fname = f"phase_rh_{label_a}_{label_b}"
+
+    label_fname = "_".join(all_labels)
+    fname = f"phase_rh_{label_fname}"
     PlotSaver.save(fig, "", None, fname)
 
 
@@ -883,6 +900,10 @@ def plot_times_by_rank(tr: TraceOps):
     nranks = 512
     data_x = list(range(nranks))
     fig, ax = plt.subplots(1, 1)
+    tr_name = tr._trace.get_trace_name()
+    tr_label = profile_label_map[tr_name]
+
+    print(f"[PlotTimesByRank] Trace: {tr_name}/{tr_label}")
 
     phases = ["AR1", "SR", "AR2", "AR3_UMBT", "AR3-AR3_UMBT"]
     for phase in phases:
@@ -892,7 +913,9 @@ def plot_times_by_rank(tr: TraceOps):
 
     ax.set_xlabel("Rank ID")
     ax.set_ylabel("Total Time (s)")
-    ax.set_title("Total Time For Each Phase/Rank")
+    ax.set_title(f"Total Time For Each Phase/Rank ({tr_label})")
+
+    ax.set_ylim([0, 4000 * 1e6])
 
     ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f}s".format(x / 1e6))
     ax.legend(bbox_to_anchor=(-0.18, 1.08), loc="lower left", ncol=5)
@@ -905,6 +928,10 @@ def plot_times_by_rank(tr: TraceOps):
 def plot_times_by_ts(tr: TraceOps):
     data_x = None
     fig, ax = plt.subplots(1, 1)
+    tr_name = tr._trace.get_trace_name()
+    tr_label = profile_label_map[tr_name]
+
+    print(f"[PlotTimesByTs] Trace: {tr_name}/{tr_label}")
 
     phases = ["AR1", "SR", "AR2", "AR3_UMBT", "AR3-AR3_UMBT"]
     #  phases = ["AR1"]
@@ -934,9 +961,10 @@ def plot_times_by_ts(tr: TraceOps):
 
     ax.set_xlabel("Timestep")
     ax.set_ylabel("Total Time")
-    ax.set_title("Time For Each Phase/Timestep")
+    ax.set_title(f"Time For Each Phase/Timestep ({tr_label})")
 
     ax.set_ylim(bottom=0)
+    ax.set_ylim([0, 250 * 1e3])
 
     ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f} ms".format(x / 1e3))
     ax.legend(bbox_to_anchor=(-0.18, 1.08), loc="lower left", ncol=5)
@@ -1044,7 +1072,7 @@ def run_plot():
     """
 
     global trace_dir
-    trace_dir = "/mnt/ltio/parthenon-topo/profile17"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile30"
     tr = TraceOps(trace_dir)
     #  trace_dir = "/mnt/ltio/parthenon-topo/profile10"
     # aggr_fpath = '/Users/schwifty/repos/amr-data/20220517-phase-analysis/aggregate.csv'
@@ -1064,20 +1092,27 @@ def run_plot():
     #  run_analyze(trace_dir)
 
     #  run_plot_timestep(trace_dir)
-    plot_times_by_rank(tr)
+    #  plot_times_by_rank(tr)
     #  plot_times_by_ts(tr)
-    return
+    #  return
 
     events = ["AR1", "AR2", "AR3", "SR", "AR3_UMBT"]
     events = ["SR", "AR3_UMBT"]
-    for event in events:
-        plot_phasetimegrid(tr, event)
+    #  for event in events:
+    #  plot_phasetimegrid(tr, event)
     #  plot_lbvsmsgcnt(tr)
     #  plot_lb_someranks(tr)
 
+    #  return
+
     # This takes two traces and plots a comparison
-    other_trace_dir = "/mnt/ltio/parthenon-topo/profile10"
-    plot_rankhour_comparison(other_trace_dir, trace_dir)
+    traces = [
+        "/mnt/ltio/parthenon-topo/profile10",
+        "/mnt/ltio/parthenon-topo/profile22",
+        "/mnt/ltio/parthenon-topo/profile28",
+        "/mnt/ltio/parthenon-topo/profile30",
+    ]
+    plot_rankhour_comparison(traces)
 
 
 if __name__ == "__main__":
diff --git a/scripts/tau_analysis/plot_taskflow_printable.py b/scripts/tau_analysis/plot_taskflow_printable.py
new file mode 100644
index 0000000..3ddaff4
--- /dev/null
+++ b/scripts/tau_analysis/plot_taskflow_printable.py
@@ -0,0 +1,1020 @@
+import matplotlib.pyplot as plt
+from matplotlib.ticker import MultipleLocator
+import numpy as np
+import pandas as pd
+import pickle
+import ipdb
+
+from trace_reader import TraceReader, TraceOps
+
+
+def plot_init():
+    SMALL_SIZE = 16
+    MEDIUM_SIZE = 18
+    BIGGER_SIZE = 20
+
+    plt.rc("font", size=SMALL_SIZE)  # controls default text sizes
+    plt.rc("axes", titlesize=SMALL_SIZE)  # fontsize of the axes title
+    plt.rc("axes", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels
+    plt.rc("xtick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("ytick", labelsize=SMALL_SIZE)  # fontsize of the tick labels
+    plt.rc("legend", fontsize=SMALL_SIZE)  # legend fontsize
+    plt.rc("figure", titlesize=BIGGER_SIZE)  # fontsize of the figure title
+
+
+def plot_neighbors(df, plot_dir):
+    fig, ax = plt.subplots(1, 1)
+    print(df.describe())
+    print(df.columns)
+
+    df = df.groupby("ts", as_index=False).agg({"evtval_count": ["mean"]})
+
+    data_x = df["ts"]
+    data_y = df["evtval_count"]["mean"]
+
+    ax.plot(data_x, data_y)
+    ax.set_title("Datapoints Salvaged (Out of 512) For Each AMR TS")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Number Of Datapoints (= Ranks) Parseable")
+    # fig.show()
+    fig.savefig("{}/taskflow_nbrcnt.pdf".format(plot_dir), dpi=300)
+
+
+def get_data(df, evt, col):
+    df = df[df["evtname"] == evt]
+    data_x = df["ts"]
+    data_y = df[col]
+    return data_x, data_y
+
+
+def plot_event(event_name, df, plot_dir, plot_tail=False, save=False):
+    fig, ax = plt.subplots(1, 1)
+    cm = plt.cm.get_cmap("tab20c")
+
+    dx, dy = get_data(df, event_name, "evtval_mean")
+    ax.plot(dx, dy, color=cm(0), label="Mean ({})".format(event_name))
+
+    dx, dy = get_data(df, event_name, "evtval_percentile_50")
+    ax.plot(dx, dy, "--", color=cm(4), label="50th %-ile ({})".format(event_name))
+
+    dx, dy = get_data(df, event_name, "evtval_percentile_75")
+    ax.plot(dx, dy, "--", color=cm(8), label="75th %-ile ({})".format(event_name))
+
+    if plot_tail:
+        dx, dy = get_data(df, event_name, "evtval_percentile_99")
+        ax.plot(dx, dy, "--", color=cm(12), label="99th %-ile ({})".format(event_name))
+
+    ax.set_title("Statistics for Event {}".format(event_name))
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (s)")
+
+    ax.legend()
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.1f}s".format(x / 1e6))
+
+    ax.set_xlim([4000, 6000])
+
+    event_key = event_name.lower()
+    event_key = "{}_zoomed".format(event_name.lower())
+
+    plot_fname = None
+    if plot_tail:
+        plot_fname = "taskflow_{}_w99.pdf".format(event_key)
+    else:
+        plot_fname = "taskflow_{}_wo99.pdf".format(event_key)
+
+    fig.tight_layout()
+    if save:
+        fig.savefig("{}/{}".format(plot_dir, plot_fname), dpi=300)
+    else:
+        fig.show()
+
+
+def plot_all_events(df, plot_dir):
+    for event in ["AR1", "AR2", "AR3", "SR"]:
+        plot_event(event, df, plot_dir, plot_tail=False, save=True)
+        plot_event(event, df, plot_dir, plot_tail=True, save=True)
+
+
+def plot_amr_log(log_df, plot_dir, save=False):
+    print(log_df)
+
+    fig, ax = plt.subplots(1, 1)
+
+    key_y = "wtime_step_other"
+    label_y = "Walltime (Non-AMR/LB)"
+    data_x = log_df["cycle"]
+    data_y = log_df[key_y]
+    ax.plot(data_x, data_y, label=label_y)
+
+    key_y = "wtime_step_amr"
+    label_y = "Walltime (AMR/LB)"
+    data_x = log_df["cycle"]
+    data_y = log_df[key_y]
+    ax.plot(data_x, data_y, label=label_y)
+
+    ax.set_title("Wall Time for AMR Run (512 Timesteps)")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Walltime (seconds)")
+
+    ax.legend()
+
+    plot_fname = "amr_steptimes.pdf"
+
+    # ax.set_xlim([3750, 4250])
+    # plot_fname = 'amr_steptimes_zoomed.pdf'
+
+    fig.tight_layout()
+
+    if save:
+        fig.savefig("{}/{}".format(plot_dir, plot_fname), dpi=300)
+    else:
+        fig.show()
+
+
+def calc_amr_log_stats(log_df):
+    def calc_key_stats(key):
+        print("Analyzing {}".format(key))
+        data_y = log_df[key]
+        med_val = np.median(data_y)
+        sum_val = data_y.sum()
+        print("Median: {:.2f}, Sum: {:.2f}".format(med_val, sum_val))
+
+        first_half = sum([i for i in data_y if i < med_val])
+        second_half = sum([i for i in data_y if i > med_val])
+
+        print(
+            "Sums: {:.1f}/{:.1f} (First 50%, Last 50%)".format(first_half, second_half)
+        )
+
+    data_y = log_df["wtime_step_other"]
+    calc_key_stats("wtime_step_other")
+    calc_key_stats("wtime_step_amr")
+
+
+def plot_amr_log_distrib(log_df, plot_dir, save=False):
+    fig, ax = plt.subplots(1, 1)
+
+    data_y = log_df["wtime_step_other"]
+    plt.hist(
+        data_y,
+        bins=100,
+        density=0,
+        histtype="step",
+        cumulative=True,
+        label="Non-AMR/LB (Cumul.)",
+    )
+    # plt.hist(data_y, bins=100, density=0, histtype='step', cumulative=False, label='Non-AMR/LB')
+    data_y = log_df["wtime_step_amr"]
+    plt.hist(
+        data_y,
+        bins=100,
+        density=0,
+        histtype="step",
+        cumulative=True,
+        label="AMR/LB (Cumul)",
+    )
+    # plt.hist(data_y, bins=100, density=0, histtype='step', cumulative=False, label='AMR/LB')
+
+    ax.legend()
+
+    noncum_profile = True
+    zoomed_profile = False
+    save = True
+
+    if noncum_profile:
+        ax.set_xlim([0, 3])
+        ax.set_title("Wall Time for AMR Run (512 Timesteps)")
+        ax.set_xlabel("Walltime (seconds)")
+        ax.set_ylabel("Num Times")
+        plot_fname = "amr_steptimes_distrib_noncumul.pdf"
+    else:
+        ax.set_title("Wall Time for AMR Run (512 Timesteps)")
+        ax.set_xlabel("Walltime (seconds)")
+        ax.set_ylabel("Number Of Timesteps > X")
+
+        ax.yaxis.set_major_formatter(lambda x, pos: max(round(30000 * (1 - x)), 0))
+
+        ax.xaxis.set_major_locator(MultipleLocator(2))
+        ax.xaxis.set_minor_locator(MultipleLocator(1))
+
+        if zoomed_profile:
+            ax.set_ylim([0.99, 1.001])
+            plot_fname = "amr_steptimes_distrib.pdf"
+            ax.yaxis.set_major_locator(MultipleLocator(0.002))
+            ax.yaxis.set_minor_locator(MultipleLocator(0.001))
+        else:
+            plot_fname = "amr_steptimes_distrib.pdf"
+            ax.yaxis.set_major_locator(MultipleLocator(0.1))
+            ax.yaxis.set_minor_locator(MultipleLocator(0.05))
+
+    plt.grid(visible=True, which="major", color="#999")
+    plt.grid(visible=True, which="minor", color="#ddd")
+    fig.tight_layout()
+
+    if save:
+        fig.savefig("{}/{}".format(plot_dir, plot_fname), dpi=300)
+    else:
+        fig.show()
+
+
+def plot_amr_comp(all_dfs, plot_dir, save=False):
+    fig, ax = plt.subplots(1, 1)
+
+    cm = plt.cm.get_cmap("Set1")
+
+    for idx, df in enumerate(all_dfs):
+        data_x = df["cycle"]
+        data_y1 = df["wtime_step_other"]
+        data_y2 = df["wtime_step_amr"]
+
+        label_1 = "Run{}-Kernel".format(idx)
+        label_2 = "Run{}-AMR".format(idx)
+        ax.plot(data_x, data_y1.cumsum(), label=label_1, color=cm(idx))
+        ax.plot(data_x, data_y2.cumsum(), label=label_2, linestyle="--", color=cm(idx))
+
+    ax.set_title("AMR Runs (512 Ranks) Phasewise Cumul. Times")
+    ax.set_xlabel("Timestep")
+    ax.set_xlabel("Total Time (seconds)")
+
+    ax.legend()
+    plt.grid(visible=True, which="major", color="#999")
+    plt.grid(visible=True, which="minor", color="#ddd")
+    fig.tight_layout()
+
+    plot_fname = "amr_steptimes_comp.pdf"
+    plot_fname = "amr_steptimes_comp_zoomed.pdf"
+    ax.set_xlim([0000, 10000])
+
+    # save = True
+
+    if save:
+        fig.savefig("{}/{}".format(plot_dir, plot_fname), dpi=300)
+    else:
+        fig.show()
+    pass
+
+
+def run_plot_amr_comp():
+    plot_dir = "figures_bigrun"
+    log_dirs = [
+        "/Users/schwifty/Repos/amr-data/20220524-phase-analysis/phoebus.log.times.csv",
+        "/Users/schwifty/Repos/amr-data/20220524-phase-analysis/phoebus.log2.csv",
+        "/Users/schwifty/Repos/amr-data/20220524-phase-analysis/phoebus.log3.csv",
+        "/Users/schwifty/Repos/amr-data/20220524-phase-analysis/phoebus.log4.csv",
+    ]
+
+    log_dfs = map(pd.read_csv, log_dirs)
+    plot_amr_comp(log_dfs, plot_dir, save=False)
+
+
+def plot_profile():
+    pass
+
+
+def run_profile():
+    df_path = "/Users/schwifty/Repos/amr-data/20220524-phase-analysis/profile.log.csv"
+    df = pd.read_csv(df_path)
+    df = df.astype({"rank": "int32", "event": str, "timepct": float})
+    events = df["event"].unique()
+
+    fig, ax = plt.subplots(1, 1)
+
+    for event in events:
+        dfe = df[df["event"] == event]
+        data_x = dfe["rank"]
+        data_y = dfe["timepct"]
+        print(data_x)
+        print(data_y)
+        ax.plot(dfe["rank"], dfe["timepct"], label=event)
+
+    ax.set_title("Function-Wise Times (Pct Of Process Time)")
+    ax.set_xlabel("Rank Index")
+    ax.set_ylabel("Time Taken (%)")
+    ax.legend()
+    # fig.tight_layout()
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.0f}%".format(x))
+    fig.show()
+    plot_dir = "figures_bigrun"
+    plot_fname = "amr_profile_phases.pdf"
+    fig.savefig("{}/{}".format(plot_dir, plot_fname), dpi=300)
+
+
+def strtols(list_str):
+    ls = list_str.strip("[]").split(",")
+    ls = np.array([float(i) for i in ls])
+    ls /= 1e6
+    return ls
+
+
+def plot_timestep(df_ts, df_log, fpath):
+    get_data_y = lambda x: strtols(df_ts[df_ts["evtname"] == x]["evtval"].iloc[0])
+
+    timestep = df_ts["ts"].unique()[0]
+    print(timestep)
+    nranks = 512
+
+    cmap = plt.cm.get_cmap("tab20b")
+
+    fig, ax = plt.subplots(1, 1)
+    data_x = range(nranks)
+
+    data_pt = df_log["wtime_step_other"] + df_log["wtime_step_amr"]
+    ax.plot(
+        [0, nranks - 1],
+        [data_pt, data_pt],
+        linestyle="--",
+        label="Internal (Total)",
+        color=cmap(0),
+    )
+
+    data_y = get_data_y("TIME_CLASSIFIEDPHASES") - get_data_y("AR3_UMBT")
+    ax.plot(data_x, data_y, label="TauClassified", color=cmap(1))
+
+    data_y = get_data_y("AR1")
+    ax.plot(data_x, data_y, label="Tau AR1", color=cmap(8))
+    data_y = get_data_y("AR2")
+    ax.plot(data_x, data_y, label="Tau AR2", color=cmap(9))
+
+    data_pt = df_log["wtime_step_amr"]
+    ax.plot(
+        [0, nranks - 1],
+        [data_pt, data_pt],
+        linestyle="--",
+        label="Internal (AMR)",
+        color=cmap(17),
+    )
+    data_y = get_data_y("AR3")
+    ax.plot(data_x, data_y, linestyle=":", label="Tau AR3", color=cmap(13))
+    data_y = get_data_y("AR3_UMBT")
+    ax.plot(data_x, data_y, label="Tau AR3_AllGather", color=cmap(12))
+
+    #  data_y = get_data_y('TIME_FROMPREVEND')
+    #  ax.plot(data_x, data_y, label='Tau StepTimeFromPrevEnd')
+
+    ax.legend(ncol=3, prop={"size": 10})
+
+    ax.set_xlabel("Rank #")
+    ax.set_ylabel("Time (s)")
+    ax.set_title("Rankwise Time Breakdown For Step {}".format(timestep))
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.2f}".format(x))
+    fig.tight_layout()
+    fig.savefig("{}/plot_step/plot_step{}.pdf".format(fpath, timestep), dpi=300)
+
+
+def plot_logstats(df_logstats, plot_dir: str) -> None:
+    fig, ax = plt.subplots(1, 1)
+
+    data_x = range(len(df_logstats))
+
+    dy_compute = np.array(df_logstats["wtime_step_other"])
+    dy_amr = np.array(df_logstats["wtime_step_amr"])
+
+    print("Total Phases: {:.0f}/{:.0f}".format(sum(dy_compute), sum(dy_amr)))
+
+    ax.plot(data_x, dy_compute, label="Internal (Compute+BC)")
+    #  ax.plot(data_x, dy_amr, label='Internal (AMR)')
+    #  ax.plot(data_x, dy_compute + dy_amr, label='Internal (Total)')
+
+    ax.set_xlim([9500, 10500])
+
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (s)")
+
+    ax.set_title("Time Taken For Two Phases As Per Internal Timers")
+    ax.legend()
+    fig.savefig("{}/timeline_a_zoomed.pdf".format(plot_dir), dpi=300)
+
+
+def get_all_and_aggr(df, key, aggr_f):
+    df = df[df["evtname"] == key]
+    mapobj = map(lambda x: aggr_f(strtols(x)), df["evtval"])
+    mapobj = list(mapobj)
+    print(len(mapobj))
+    return np.array(mapobj)
+
+
+def plot_umbt_stats(df_phases, df_log, plot_dir) -> None:
+    fig, ax = plt.subplots(1, 1)
+
+    get_data_y = lambda x: strtols(
+        df_phases[df_phases["evtname"] == x]["evtval"].iloc[0]
+    )
+
+    min5 = lambda x: sorted(x)[5]
+
+    data_y1 = get_all_and_aggr(df_phases, "AR3", max)
+    data_y1a = get_all_and_aggr(df_phases, "AR3_UMBT", max)
+    data_y1b = get_all_and_aggr(df_phases, "AR3_UMBT", min)
+    data_y1c = get_all_and_aggr(df_phases, "AR3_UMBT", min5)
+    data_y1d = get_all_and_aggr(df_phases, "AR3_UMBT", np.median)
+    data_y2 = df_log["wtime_step_amr"]
+    data_x = range(len(data_y2))
+
+    common_len = min(
+        [len(i) for i in [data_y1, data_y1a, data_y1b, data_y1c, data_y1d, data_y2]]
+    )
+    data_y1 = data_y1[:common_len]
+    data_y1a = data_y1a[:common_len]
+    data_y1b = data_y1b[:common_len]
+    data_y1c = data_y1c[:common_len]
+    data_y1d = data_y1d[:common_len]
+    data_y2 = data_y2[:common_len]
+
+    print(data_y1)
+    print(data_y2)
+    assert len(data_y1) == len(data_y2)
+    assert len(data_y1a) == len(data_y2)
+
+    ax.plot(data_x, data_y1, label="TAU (AR3)")
+    ax.plot(data_x, data_y1a, label="TAU (AR3_UMBT)")
+    ax.plot(data_x, data_y2, label="Internal (AMR)")
+
+    ax.legend()
+    ax.set_title("Times For Different AMR Phases")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (s)")
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.2f}s".format(x))
+
+    fig.tight_layout()
+    fig.savefig("{}/amr_stats.pdf".format(plot_dir), dpi=300)
+
+    fig, ax = plt.subplots(1, 1)
+    ax.plot(data_x, data_y1a / data_y1, label="TAU_UMBT/TAU_AR3")
+    ax.plot(data_x, data_y1a / data_y2, label="TAU_UMBT/INT_AMR")
+
+    ax.legend()
+    ax.set_title("Ratio Of Collective Time To Total AMR-LB Time")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time Ratio")
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.1f}%".format(x * 100))
+
+    fig.tight_layout()
+    fig.savefig("{}/amr_timerat.pdf".format(plot_dir), dpi=300)
+
+    fig, ax = plt.subplots(1, 1)
+    ax.plot(data_x, data_y1a / data_y1, label="TAU_UMBT/TAU_AR3")
+    ax.plot(data_x, data_y1a / data_y2, label="TAU_UMBT/INT_AMR")
+
+    ax.legend()
+    ax.set_title("Ratio Of Collective Time To Total AMR-LB Time")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time Ratio")
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.1f}%".format(x * 100))
+
+    ax.set_ylim([0, 1.5])
+
+    fig.tight_layout()
+    fig.savefig("{}/amr_timerat_clipped.pdf".format(plot_dir), dpi=300)
+
+    fig, ax = plt.subplots(1, 1)
+    #  ax.plot(data_x, data_y1a, label="$AG_{NO}$:Max", alpha=0.7)
+    #  ax.plot(data_x, data_y1d, label="$AG_{NO}$:Median", alpha=0.7)
+    #  ax.plot(data_x, data_y1c, label="$AG_{NO}$:Min5", alpha=0.7)
+    #  ax.plot(data_x, data_y1b, label="$AG_{NO}$:Min", alpha=0.7)
+
+    ax.plot(data_x, data_y1a, label="$Time_{barrier}$, max", alpha=1)
+    ax.plot(data_x, data_y1b, label="$Time_{barrier}$, min", alpha=1)
+
+    ax.legend(fontsize=14)
+    ax.set_title("Barrier Wait Times Across All Ranks Per Timestep")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (s)")
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.2f}s".format(x))
+    ax.xaxis.set_major_formatter(lambda x, pos: "{:.0f}K".format(x/1000.0))
+
+    fig.tight_layout()
+    fig.savefig("{}/amr_umbt_min_max.pdf".format(plot_dir), dpi=300)
+
+    fig, ax = plt.subplots(1, 1)
+    data_y1amb = data_y1a - data_y1b
+    data_y1amc = data_y1a - data_y1c
+    data_y1amd = data_y1a - data_y1d
+
+    ax.plot(data_x, data_y1amb, label="TAU_UMBT_MAX-MIN")
+    ax.plot(data_x, data_y1amc, label="TAU_UMBT_MAX-MIN5")
+    ax.plot(data_x, data_y1amd, label="TAU_UMBT_MAX-MED")
+
+    ax.legend()
+    ax.set_title("Min And Max Collective Times Across All Ranks")
+    ax.set_xlabel("Timestep")
+    ax.set_ylabel("Time (s)")
+    ax.yaxis.set_major_formatter(lambda x, pos: "{:.2f}s".format(x))
+
+    fig.tight_layout()
+    fig.savefig("{}/amr_umbt_minmax_delta.pdf".format(plot_dir), dpi=300)
+
+    print("Sum UMBT_MAX: {:.0f}".format(sum(data_y1a)))
+    print("Sum UMBT_MAX-MIN: {:.0f}".format(sum(data_y1amb)))
+    print("Sum UMBT_MAX-MIN5: {:.0f}".format(sum(data_y1amc)))
+    print("Sum UMBT_MAX-MED: {:.0f}".format(sum(data_y1amd)))
+
+
+def plot_umbt_rankgrid(df_phases, imevent, plot_dir, cached=False):
+    def sort_xargs(ls):
+        ls_widx = [(ls[i], i) for i in range(len(ls))]
+        ls_widx = sorted(ls_widx)
+        ls_idx = [i[1] for i in ls_widx]
+        #  ls_idx = np.array(ls_idx)
+        return ls_idx
+
+    CACHE_FNAME = ".rankgrid.{}".format(imevent)
+    data_ranks = None
+
+    if not cached:
+        data_ranks = get_all_and_aggr(df_phases, imevent, sort_xargs)
+        with open(CACHE_FNAME, "wb+") as f:
+            f.write(pickle.dumps(data_ranks))
+    else:
+        with open(CACHE_FNAME, "rb") as f:
+            data_ranks = pickle.loads(f.read())
+
+    data_ranks = list(data_ranks[:-1])
+    data_ranks = np.vstack(data_ranks)
+    #  print(data_ranks.shape)
+
+    fig, ax = plt.subplots(1, 1)
+    im = ax.imshow(data_ranks, aspect="auto", cmap="plasma")
+
+    ax.set_title("Rank Order For Event {}".format(imevent))
+    ax.set_ylabel("Timestep")
+    ax.xaxis.set_ticks([])
+    ax.set_xlabel("Ranks In Increasing Order Of Phase Time")
+
+    plt.subplots_adjust(left=0.15, right=0.8)
+    cax = plt.axes([0.85, 0.1, 0.075, 0.8])
+    fig.colorbar(im, cax=cax)
+
+    fig.savefig("{}/umbt_rankgrid_{}.pdf".format(plot_dir, imevent.lower()), dpi=600)
+
+
+def plot_umbt_rankgrid_wcompare(df_phases, df_log, imevent, plot_dir, cached=False):
+    def sort_xargs(ls):
+        ls_widx = [(ls[i], i) for i in range(len(ls))]
+        ls_widx = sorted(ls_widx)
+        ls_idx = [i[1] for i in ls_widx]
+        #  ls_idx = np.array(ls_idx)
+        return ls_idx
+
+    CACHE_FNAME = ".phasetimegrid.{}".format(imevent)
+    data_ranks = None
+
+    if not cached:
+        data_ranks = get_all_and_aggr(df_phases, imevent, lambda x: x)
+        with open(CACHE_FNAME, "wb+") as f:
+            f.write(pickle.dumps(data_ranks))
+    else:
+        with open(CACHE_FNAME, "rb") as f:
+            data_ranks = pickle.loads(f.read())
+
+    data_times = list(data_ranks[:-1])
+    data_times = np.vstack(data_times)
+    #  print(data_ranks.shape)
+
+    def smoothen(data):
+        kernel_size = 20
+        kernel = np.ones(kernel_size) / kernel_size
+        data_convolved_20 = np.convolve(data, kernel, mode="same")
+        return data_convolved_20
+
+    data_time_med = np.median(data_times, axis=1)
+    data_time_max = np.max(data_times, axis=1)
+    data_time_log = df_log["wtime_step_other"]
+
+    print(data_time_med.shape)
+
+    #  fig, axes = plt.subplots(1, 4, gridspec_kw={"width_ratios": [4, 1, 1, 1]})
+    fig, axes = plt.subplots(1, 3, gridspec_kw={"width_ratios": [4, 1, 1]})
+
+    ax_im = axes[0]
+    im = ax_im.imshow(data_times, aspect="auto", cmap="plasma")
+
+    num_ts = data_times.shape[0]
+    data_y = range(num_ts)
+
+    axes[1].plot(data_time_med, data_y)
+    axes[2].plot(data_time_max, data_y)
+    #  axes[3].plot(data_time_log[:num_ts], data_y)
+
+    # ax.set_title('Rank Order For Event {}'.format(imevent))
+    # ax.set_ylabel('Timestep')
+    # ax.xaxis.set_ticks([])
+    # ax.set_xlabel('Ranks In Increasing Order Of Phase Time')
+    axes[0].xaxis.set_ticks([])
+    axes[0].set_title("Rankwise Phase Times")
+
+    #  axes[1].xaxis.set_ticks([])
+    axes[1].yaxis.set_ticks([])
+    axes[1].set_title("TauMed")
+    axes[1].set_ylim([num_ts, 0])
+
+    #  axes[2].xaxis.set_ticks([])
+    axes[2].yaxis.set_ticks([])
+    axes[2].set_title("TauMax")
+    axes[2].set_ylim([num_ts, 0])
+
+    #  axes[3].yaxis.set_ticks([])
+    #  axes[3].set_ylim([num_ts, 0])
+
+    fig.suptitle("Time Distributions For Event {}".format(imevent))
+    # fig.supxlabel('Something')
+    fig.supylabel("Timesteps")
+
+    # plt.subplots_adjust(left=0.15, right=0.8)
+    plt.subplots_adjust(wspace=0.03, left=0.15)
+    # cax = plt.axes([0.85, 0.1, 0.075, 0.8])
+    # fig.colorbar(im, cax=cax)
+    fig.colorbar(im, ax=axes[-1])
+
+    save = True
+    if save:
+        fig.savefig(
+            "{}/umbt_phasetimegrid_{}.pdf".format(plot_dir, imevent.lower()), dpi=600
+        )
+    else:
+        fig.show()
+
+
+def plot_umbt_rankgrid_wcompare_amr(df_phases, df_log, plot_dir, cached=False):
+    def sort_xargs(ls):
+        ls_widx = [(ls[i], i) for i in range(len(ls))]
+        ls_widx = sorted(ls_widx)
+        ls_idx = [i[1] for i in ls_widx]
+        #  ls_idx = np.array(ls_idx)
+        return ls_idx
+
+    CACHE_FNAME = ".phasetimegrid.amr"
+    data_ranks = None
+
+    if not cached:
+        data_ranks = get_all_and_aggr(df_phases, "AR3", lambda x: x)
+        with open(CACHE_FNAME, "wb+") as f:
+            f.write(pickle.dumps(data_ranks))
+    else:
+        with open(CACHE_FNAME, "rb") as f:
+            data_ranks = pickle.loads(f.read())
+
+    data_times = list(data_ranks[:-1])
+    data_times = np.vstack(data_times)
+    #  print(data_ranks.shape)
+
+    def smoothen(data):
+        kernel_size = 20
+        kernel = np.ones(kernel_size) / kernel_size
+        data_convolved_20 = np.convolve(data, kernel, mode="same")
+        return data_convolved_20
+
+    data_time_med = np.median(data_times, axis=1)
+    data_time_max = np.max(data_times, axis=1)
+    data_time_log_comp = df_log["wtime_step_other"]
+    data_time_log_amr = df_log["wtime_step_amr"]
+
+    print(data_time_med.shape)
+
+    fig, axes = plt.subplots(1, 5, gridspec_kw={"width_ratios": [4, 1, 1, 1, 1]})
+
+    ax_im = axes[0]
+    im = ax_im.imshow(data_times, aspect="auto", cmap="plasma")
+
+    num_ts = data_times.shape[0]
+    data_y = range(num_ts)
+
+    axes[1].plot(data_time_med, data_y)
+    axes[2].plot(data_time_max, data_y)
+    axes[3].plot(data_time_log_comp[:num_ts], data_y)
+    axes[4].plot(data_time_log_amr[:num_ts], data_y)
+
+    # ax.set_title('Rank Order For Event {}'.format(imevent))
+    # ax.set_ylabel('Timestep')
+    # ax.xaxis.set_ticks([])
+    # ax.set_xlabel('Ranks In Increasing Order Of Phase Time')
+    axes[0].xaxis.set_ticks([])
+    axes[0].set_title("Rankwise Phase Times")
+
+    #  axes[1].xaxis.set_ticks([])
+    axes[1].yaxis.set_ticks([])
+    axes[1].set_title("T_Med")
+    axes[1].set_ylim([num_ts, 0])
+
+    #  axes[2].xaxis.set_ticks([])
+    axes[2].yaxis.set_ticks([])
+    axes[2].set_title("T_Max")
+    axes[2].set_ylim([num_ts, 0])
+
+    axes[3].yaxis.set_ticks([])
+    axes[3].set_title("I_Comp")
+    axes[3].set_ylim([num_ts, 0])
+
+    axes[4].yaxis.set_ticks([])
+    axes[4].set_title("I_Amr")
+    axes[4].set_ylim([num_ts, 0])
+
+    fig.suptitle("Time Distributions For AR3=AMR")
+    # fig.supxlabel('Something')
+    fig.supylabel("Timesteps")
+
+    # plt.subplots_adjust(left=0.15, right=0.8)
+    plt.subplots_adjust(wspace=0.03, left=0.15)
+    # cax = plt.axes([0.85, 0.1, 0.075, 0.8])
+    # fig.colorbar(im, cax=cax)
+    fig.colorbar(im, ax=axes[-1])
+
+    save = True
+    if save:
+        fig.savefig("{}/umbt_phasetimegrid_amr.pdf".format(plot_dir), dpi=600)
+    else:
+        fig.show()
+
+
+def plot_umbt_rankgrid_wcompare_nonamr(df_phases, df_log, plot_dir, cached=False):
+    def sort_xargs(ls):
+        ls_widx = [(ls[i], i) for i in range(len(ls))]
+        ls_widx = sorted(ls_widx)
+        ls_idx = [i[1] for i in ls_widx]
+        #  ls_idx = np.array(ls_idx)
+        return ls_idx
+
+    CACHE_FNAME = ".phasetimegrid.nonamr"
+    data_ranks = None
+
+    if not cached:
+        data = map(
+            lambda key: get_all_and_aggr(df_phases, key, lambda x: x),
+            ["AR1", "AR2", "SR"],
+        )
+        #  IPython.embed()
+        data = list(data)
+        dims = [i.shape for i in data]
+        dim_min = min([i[0] for i in dims]) - 1
+        print("Dims: ", dims)
+        print("Dim Min: ", dim_min)
+
+        data_minned = [i[:dim_min] for i in data]
+        data_ranks = np.sum(data_minned, axis=0)
+
+        print("Data Shape: ", data_ranks.shape)
+
+        with open(CACHE_FNAME, "wb+") as f:
+            f.write(pickle.dumps(data_ranks))
+    else:
+        with open(CACHE_FNAME, "rb") as f:
+            data_ranks = pickle.loads(f.read())
+
+    data_times = list(data_ranks[:-1])
+    data_times = np.vstack(data_times)
+    print(data_ranks.shape)
+
+    def smoothen(data):
+        kernel_size = 20
+        kernel = np.ones(kernel_size) / kernel_size
+        data_convolved_20 = np.convolve(data, kernel, mode="same")
+        return data_convolved_20
+
+    data_time_med = np.median(data_times, axis=1)
+    data_time_max = np.max(data_times, axis=1)
+    data_time_log_comp = df_log["wtime_step_other"]
+    data_time_log_amr = df_log["wtime_step_amr"]
+
+    print(data_time_med.shape)
+
+    fig, axes = plt.subplots(1, 5, gridspec_kw={"width_ratios": [4, 1, 1, 1, 1]})
+
+    ax_im = axes[0]
+    im = ax_im.imshow(data_times, aspect="auto", cmap="plasma")
+
+    num_ts = data_times.shape[0]
+    data_y = range(num_ts)
+
+    axes[1].plot(data_time_med, data_y)
+    axes[2].plot(data_time_max, data_y)
+    axes[3].plot(data_time_log_comp[:num_ts], data_y)
+    axes[4].plot(data_time_log_amr[:num_ts], data_y)
+
+    axes[0].xaxis.set_ticks([])
+    axes[0].set_title("Rankwise Phase Times")
+
+    axes[1].yaxis.set_ticks([])
+    axes[1].set_title("T_Med")
+    axes[1].set_ylim([num_ts, 0])
+
+    axes[2].yaxis.set_ticks([])
+    axes[2].set_title("T_Max")
+    axes[2].set_ylim([num_ts, 0])
+
+    axes[3].yaxis.set_ticks([])
+    axes[3].set_title("I_Comp")
+    axes[3].set_ylim([num_ts, 0])
+
+    axes[4].yaxis.set_ticks([])
+    axes[4].set_title("I_AMR")
+    axes[4].set_ylim([num_ts, 0])
+
+    fig.suptitle("Time Distributions For AR1+AR2+SR=NonAMR")
+    fig.supylabel("Timesteps")
+
+    plt.subplots_adjust(wspace=0.03, left=0.15)
+    fig.colorbar(im, ax=axes[-1])
+
+    save = True
+    if save:
+        fig.savefig("{}/umbt_phasetimegrid_nonamr.pdf".format(plot_dir), dpi=600)
+    else:
+        fig.show()
+
+
+def run_plot_timestep():
+    trace_dir = "/mnt/ltio/parthenon-topo/profile6.wtau"
+    trace_dir = "/mnt/ltio/parthenon-topo/profile8"
+    plot_dir = "/users/ankushj/repos/amr/scripts/tau_analysis/figures"
+    plot_dir = "figures/20221103"
+    ts_to_plot = 1
+
+    cached = False
+
+    df_phases = None
+
+    if not cached:
+        df_phases = pd.read_csv("{}/aggregate.csv".format(trace_dir))
+
+    df_log = pd.read_csv("{}/run/log.txt.csv".format(trace_dir)).astype({"cycle": int})
+
+    #  plot_umbt_rankgrid(df_phases, "AR1", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid_wcompare(df_phases, df_log, "AR1", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid_wcompare(df_phases, df_log, "AR2", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid_wcompare(df_phases, df_log, "SR", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid_wcompare(df_phases, df_log, "AR3", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid_wcompare(df_phases, df_log, "AR3_UMBT", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid_wcompare_nonamr(df_phases, df_log, plot_dir, cached=cached)
+    #  plot_umbt_rankgrid_wcompare_amr(df_phases, df_log, plot_dir, cached=cached)
+    #  plot_umbt_rankgrid(df_phases, "AR2", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid(df_phases, "AR3", plot_dir, cached=cached)
+    #  plot_umbt_rankgrid(df_phases, "AR3_UMBT", plot_dir, cached=cached)
+    plot_umbt_stats(df_phases, df_log, plot_dir)
+    return
+
+    ts_selected = df_log[df_log["wtime_step_other"] > 0.6]["cycle"]
+    ts_to_plot = []
+    for ts in ts_selected:
+        ts_to_plot.append(ts - 1)
+        ts_to_plot.append(ts)
+        ts_to_plot.append(ts + 1)
+    print(ts_to_plot)
+
+    for ts in ts_to_plot:
+        print(ts)
+        df_ts = df_phases[df_phases["ts"] == ts]
+        df_logts = df_log[df_log["cycle"] == ts]
+        print(df_ts)
+        print(df_logts)
+        plot_timestep(df_ts, df_logts, plot_dir)
+
+    #  plot_logstats(df_log, plot_dir)
+    return
+
+
+""" Input: trace/taskaggr.csv 
+Output: XX
+"""
+
+
+def run_plot_aggr(trace_dir: str, plot_dir):
+    tr = TraceOps(trace_dir)
+    aggr_df_path = "{}/trace/taskaggr.csv".format(trace_dir)
+    aggr_df = pd.read_csv(aggr_df_path)
+    print(aggr_df)
+
+    def get_data(key):
+        row = aggr_df[aggr_df["evtname"] == key]["evtval"].iloc[0]
+        row = np.array([int(i) for i in row.split(",")], dtype=np.int64)
+        return row
+
+    def plot_total_phasetimes():
+        data_ar1 = get_data("AR1")
+        data_ar2 = get_data("AR2")
+        data_sr = get_data("SR")
+        data_ar3 = get_data("AR3")
+        data_ar3u = get_data("AR3_UMBT")
+
+        nranks = 512
+        data_x = list(range(nranks))
+        fig, ax = plt.subplots(1, 1)
+
+        #  ipdb.set_trace()
+
+        ax.plot(data_x, data_ar1, label="$FC_{CN}$")
+        ax.plot(data_x, data_sr, label="$BC_{NO}$")
+        ax.plot(data_x, data_ar2, label="$FD_{CO}$")
+        ax.plot(data_x, data_ar3u, label="$AG_{NO}$")
+        ax.plot(data_x, data_ar3 - data_ar3u, label="$LB_{NO}$")
+
+        ax.set_xlabel("Rank ID")
+        ax.set_ylabel("Total Time (s)")
+        ax.set_title("Total Time For Each Phase/Rank")
+
+        ax.yaxis.set_major_formatter(lambda x, pos: '{:.0f}s'.format(x/1e6))
+        ax.legend(bbox_to_anchor=(-0.15, 1.08), loc="lower left", ncol=5)
+        fig.tight_layout()
+
+        plot_dest = "{}/phases.aggr.pdf".format(plot_dir)
+        print("Saving plot: {}".format(plot_dest))
+        fig.savefig(plot_dest, dpi=300)
+
+    def plot_lbvsmsgcnt():
+        msg_mat_lb = tr.multimat("msgcnt:LoadBalancing")
+        msg_mat_bc = tr.multimat("msgcnt:BoundaryComm")
+        lbmsg_rwtotals = np.sum(msg_mat_lb, axis=0)
+        bcmsg_rwtotals = np.sum(msg_mat_bc, axis=0)
+
+        data_lb = get_data("AR3") - get_data("AR3_UMBT")
+
+        nranks = 512
+        data_x = list(range(nranks))
+
+        fig, ax = plt.subplots(1, 1)
+        ax2 = ax.twinx()
+
+        ax.plot(data_x, lbmsg_rwtotals, label='Message Count - LB')
+        ax.plot(data_x, bcmsg_rwtotals, label='Message Count - BC')
+        ax2.plot(data_x, data_lb, label='Time', color='orange')
+
+        ax.set_xlabel("Rank ID")
+        ax.set_ylabel("Message Count")
+        ax2.set_ylabel("Time AR3_LB (s)")
+
+        ax.yaxis.set_major_formatter(lambda x, pos: '{:.0f}K'.format(x/1e3))
+        ax2.yaxis.set_major_formatter(lambda x, pos: '{:.0f} s'.format(x/1e6))
+
+        ax.set_ylim([0, ax.get_ylim()[1]])
+        ax2.set_ylim([0, ax2.get_ylim()[1]])
+
+        ax.legend()
+
+        plot_dest = "{}/ar3_vs_msgcnt.pdf".format(plot_dir)
+        fig.tight_layout()
+        fig.savefig(plot_dest, dpi=300)
+
+    def plot_lb_someranks():
+        ranks_to_plot = [367, 368]
+
+        lb_mat = tr.multimat("tau:AR3-AR3_UMBT")
+        data_x = list(range(lb_mat.shape[0]))
+
+        fig, ax = plt.subplots(1, 1)
+
+        for r in ranks_to_plot:
+            data_ry = lb_mat[:, r]
+            ax.plot(data_x, data_ry, label='Rank {}'.format(r))
+
+        ax.legend()
+        ax.set_xlabel('Timestep')
+        ax.set_ylabel('Time ms')
+
+        ax.set_ylim([0, ax.get_ylim()[1]])
+        ax.xaxis.set_major_formatter(lambda x, pos: '{:0.0f} ms'.format(x/1e3))
+
+        plot_dest = "{}/lb_rankwise.pdf".format(plot_dir)
+        fig.tight_layout()
+        fig.savefig(plot_dest, dpi=300)
+
+
+    plot_total_phasetimes()
+    #  plot_lbvsmsgcnt()
+    #  plot_lb_someranks()
+
+
+def run_analyze(trace_dir: str):
+    tr = TraceReader(trace_dir)
+    all_events = ["AR1", "AR2", "SR", "AR3", "AR3_UMBT"]
+    for evt in all_events:
+        mat = tr.get_tau_event(evt)
+        mat_ts_mean = np.mean(mat, axis=1)
+        print(mat_ts_mean.shape)
+        mat_ts_sum = np.sum(mat_ts_mean)
+        print("{}: {:.1f}s".format(evt, mat_ts_sum / 1e6))
+
+
+def run_plot():
+    trace_dir = "/mnt/ltio/parthenon-topo/profile9"
+    # aggr_fpath = '/Users/schwifty/repos/amr-data/20220517-phase-analysis/aggregate.csv'
+    # df = pd.read_csv(aggr_fpath)
+    plot_init()
+    plot_dir = "figures/20220811-profile9"
+    # # plot_neighbors(df, plot_dir)
+    # plot_all_events(df, plot_dir)
+
+    # phoebus_log = '/Users/schwifty/Repos/amr-data/20220524-phase-analysis/phoebus.log.times.csv'
+    # phoebus_log2 = '/Users/schwifty/Repos/amr-data/20220524-phase-analysis/phoebus.log2.csv'
+    phoebus_log = "{}/run/log.txt.csv".format(trace_dir)
+    #  log_df = pd.read_csv(phoebus_log)
+    # log_df2 = pd.read_csv(phoebus_log2)
+    #  plot_amr_log(log_df, plot_dir, save=True)
+    #  plot_amr_log_distrib(log_df, plot_dir, save=True)
+    #  calc_amr_log_stats(log_df)
+    #  run_plot_amr_comp()
+    #  run_profile()
+    run_plot_timestep()
+    #  run_analyze(trace_dir)
+    #  run_plot_aggr(trace_dir, plot_dir)
+
+
+if __name__ == "__main__":
+    run_plot()
-- 
2.25.1

